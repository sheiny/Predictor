{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb56f37",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7846cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import shutil\n",
    "from keras.callbacks import CSVLogger\n",
    "import os\n",
    "from enum import Enum\n",
    "import imblearn\n",
    "import time\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea80aed",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8209cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_positive_ratio(train_labels):\n",
    "  neg, pos = np.bincount(train_labels)\n",
    "  total = neg + pos\n",
    "  print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "\n",
    "# Claculate weight for classes\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "def calculate_class_weights(train_labels):\n",
    "  neg, pos = np.bincount(train_labels)\n",
    "  total = neg + pos\n",
    "  weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "  weight_for_1 = (1 / pos)*(total)/2.0\n",
    "  class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "  print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "  print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "  return class_weight, neg, pos\n",
    "\n",
    "def oversample(train_array, train_labels):\n",
    "  oversample = RandomOverSampler()\n",
    "  train_array, train_labels = oversample.fit_resample(train_array, train_labels)\n",
    "  return train_array, train_labels\n",
    "\n",
    "def undersample(train_array, train_labels):\n",
    "  undersample = RandomUnderSampler()\n",
    "  train_array, train_labels = undersample.fit_resample(train_array, train_labels)\n",
    "  return train_array, train_labels\n",
    "\n",
    "########## Learning Model ##########\n",
    "def make_model(evalMetrics, dropOut, learningRate, inputSize, numNodes, numLayers):\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Input(shape=inputSize))\n",
    "  for x in range(numLayers):\n",
    "    model.add(tf.keras.layers.Dense(numNodes, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropOut))\n",
    "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=evalMetrics)\n",
    "  return model\n",
    "\n",
    "########## Test and check Performance ##########\n",
    "def calculate_test_metrics(model, results):\n",
    "  m = {}\n",
    "  for name, value in zip(model.metrics_names, results):\n",
    "    m[name] = value\n",
    "  if m['precision'] + m['recall'] != 0:\n",
    "    f_score = 2 * ((m['precision'] * m['recall'])/(m['precision'] + m['recall']))\n",
    "    m['F-score'] = f_score\n",
    "  sqrt = math.sqrt((m['tp']+m['fp'])*(m['tp']+m['fn'])*(m['tn']+m['fp'])*(m['tn']+m['fn']))\n",
    "  if sqrt != 0:\n",
    "    mcc = ((m['tp'] * m['tn']) - (m['fp'] * m['fn']))/sqrt\n",
    "    m['MCC'] = mcc\n",
    "  return m\n",
    "\n",
    "class timecallback(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, filePath):\n",
    "    self.start = None\n",
    "    self.filePath = filePath\n",
    "    pd.DataFrame(columns=['Runtime']).to_csv(self.filePath, mode='a', index=False, header=True)\n",
    "  def on_epoch_begin(self,epoch,logs = {}):\n",
    "    self.start = time.time()\n",
    "  def on_epoch_end(self,epoch,logs = {}):\n",
    "    duration = time.time() - self.start\n",
    "    pd.DataFrame([duration], columns = ['runtime']).to_csv(self.filePath, mode='a', index=False, header=False)\n",
    "\n",
    "def numLines(file):\n",
    "  lines = 0\n",
    "  with open(file) as fp:\n",
    "    for _ in fp:\n",
    "      lines += 1\n",
    "  return lines\n",
    "\n",
    "\n",
    "class BalanceStrategy(Enum):\n",
    "  NONE = 0\n",
    "  WEIGHTS = 1\n",
    "  OVERSAMPLE = 2\n",
    "  UNDERSAMPLE = 3\n",
    "\n",
    "class ValidationReader():\n",
    "  def __init__(self, filePath, validationChunkSize):\n",
    "    self.filePath = filePath\n",
    "    self.validationChunkSize = validationChunkSize\n",
    "    self.currentItr = 0\n",
    "\n",
    "    valSizeRows = numLines(self.filePath) - 1 #skip header counting\n",
    "    self.valChunkIterations = int(valSizeRows/validationChunkSize) #disregard last lines\n",
    "    self.reader = pd.read_csv(self.filePath, dtype=np.float32, iterator=True)\n",
    "\n",
    "  def getChunk(self):\n",
    "    if self.currentItr == self.valChunkIterations:\n",
    "      self.reader = pd.read_csv(self.filePath, dtype=np.float32, iterator=True)\n",
    "      self.currentItr = 0\n",
    "    self.currentItr += 1\n",
    "    return self.reader.get_chunk(self.validationChunkSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e54192",
   "metadata": {},
   "source": [
    "# Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ac196",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = BalanceStrategy.WEIGHTS\n",
    "batch_size = 32 # is important to ensure that each batch has a decent chance of containing a few positive samples\n",
    "numEpochs = 10\n",
    "learningRate = 0.001 #Eh?Predictor=0.05, default=0.001\n",
    "dropOut = 0.05 #Eh?Predictor=0.05\n",
    "evalMetrics = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "               tf.keras.metrics.FalsePositives(name='fp'),\n",
    "               tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "               tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "               tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "               tf.keras.metrics.Precision(name='precision'),\n",
    "               tf.keras.metrics.Recall(name='recall'),\n",
    "               tf.keras.metrics.AUC(name='auc')]\n",
    "scaler = pickle.load(open('data/scaler.pkl','rb'))\n",
    "testCSVFile = 'data/test.csv'\n",
    "validationCSVFile = 'data/validation.csv'\n",
    "testChunkSize = 1e6 # rows\n",
    "validationChunkSize = int(testChunkSize * 0.2)\n",
    "valReader = ValidationReader(validationCSVFile, validationChunkSize)\n",
    "\n",
    "\n",
    "for numNodes in [50, 100]:\n",
    "  for numLayers in range(1,3):\n",
    "    model_name = 'savedModels/DNN_'+str(numLayers)+'L_'+str(numNodes)+'N'\n",
    "    if os.path.exists(model_name):\n",
    "      shutil.rmtree(model_name)\n",
    "    os.mkdir(model_name)\n",
    "    checkpoint_path = model_name+'/model.ckpt'\n",
    "\n",
    "    inputSize = len(pd.read_csv(testCSVFile, nrows=1).columns)-2 # -2 to remove NodeID and label\n",
    "    model = make_model(evalMetrics, dropOut, learningRate, inputSize, numNodes, numLayers)\n",
    "\n",
    "    finalResults = {}\n",
    "    for epoch in range(numEpochs):\n",
    "      epochResults = {}\n",
    "      for train_df in pd.read_csv(testCSVFile, chunksize=testChunkSize):\n",
    "        train_df = train_df.drop(columns=['NodeID'])\n",
    "        train_df = train_df.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "\n",
    "        val_df = valReader.getChunk()\n",
    "        val_df = val_df.drop(columns=['NodeID'])\n",
    "        val_df = val_df.sample(frac=1).reset_index(drop=True)#shuffle\n",
    "\n",
    "\n",
    "        # Build np arrays of labels and features.\n",
    "        train_labels = np.array(train_df.pop('HasDetailedRoutingViolation'))\n",
    "        val_labels = np.array(val_df.pop('HasDetailedRoutingViolation'))\n",
    "\n",
    "        train_df = scaler.transform(train_df)\n",
    "        val_df = scaler.transform(val_df)\n",
    "\n",
    "        train_array = np.array(train_df)\n",
    "        val_array = np.array(val_df)\n",
    "\n",
    "        # print_positive_ratio(train_labels)\n",
    "        # Apply the selected strategy to handle umbalanced data.\n",
    "        weight = None\n",
    "        if strategy == BalanceStrategy.OVERSAMPLE:\n",
    "          train_array, train_labels = oversample(train_array, train_labels)\n",
    "        elif strategy == BalanceStrategy.UNDERSAMPLE:\n",
    "          train_array, train_labels = undersample(train_array, train_labels)\n",
    "        elif strategy == BalanceStrategy.WEIGHTS:\n",
    "          weight = calculate_class_weights(train_labels)\n",
    "          weight = weight[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Create a callback that saves the model's weights at the end of each epoch\n",
    "#         cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True)\n",
    "        # Create a callback that saves model history at the end of each epoch\n",
    "#         csv_logger = CSVLogger(model_name+'/model_history_log.csv', append=True)\n",
    "        # Create a callback that saves runtimes of each epoch\n",
    "#         runtime_callback = timecallback(model_name+'/runtime.csv')\n",
    "        train_history = model.fit(x=train_array,\n",
    "                                 y=train_labels,\n",
    "                                 batch_size=batch_size,\n",
    "                                 validation_data=(val_array, val_labels),\n",
    "                                 class_weight=weight)\n",
    "        if len(epochResult) == 0:\n",
    "          epochResults = {key:[] for key, value in train_history.history.items()}\n",
    "        for key, value in train_history.history.items():\n",
    "          epochResults[key].append(value[0])\n",
    "        break\n",
    "      #TODO concatenate epochResults in finalResults\n",
    "      #avg properly\n",
    "      #saveFile finalResults\n",
    "      #save model?\n",
    "      break\n",
    "    break\n",
    "  break\n",
    "#                                callbacks=[cp_callback, csv_logger, runtime_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21711b28",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSVs\n",
    "# Drop node IDS\n",
    "# Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c950647",
   "metadata": {},
   "source": [
    "# Generate Train and Test CSVs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba0a3f26",
   "metadata": {},
   "source": [
    "circuitsDir = '/home/sheiny/workspace/RoutedDesigns/'\n",
    "neighborDistance = 1\n",
    "testSize = 0.2 #20%\n",
    "useRandomOversample = False\n",
    "\n",
    "if os.path.exists('data/'):\n",
    "  shutil.rmtree('data/')\n",
    "  os.mkdir('data/')\n",
    "else:\n",
    "  os.mkdir('data/')\n",
    "\n",
    "typesOfDRVs = [\"AdjacentCutSpacing\", \"SameLayerCutSpacing\", \"EndOfLine\", \"FloatingPatch\", \"MinArea\", \"MinWidth\",\n",
    "  \"NonSuficientMetalOverlap\", \"CutShort\", \"MetalShort\", \"OutOfDieShort\", \"CornerSpacing\", \"ParallelRunLength\"]\n",
    "SelectedDRVTypes = [\"CutShort\", \"MetalShort\"]\n",
    "label_name = \"HasDetailedRoutingViolation\"\n",
    "\n",
    "circuits = []\n",
    "for circuit in os.listdir(circuitsDir):\n",
    "  if os.path.isdir(circuitsDir+'/'+circuit) and circuit != 'nangate45':\n",
    "    for file in os.listdir(circuitsDir+'/'+circuit+'/base/'):\n",
    "      if '_viol.csv' in file:\n",
    "        circuits.append(circuitsDir+circuit+'/base/'+file.split('_')[0])\n",
    "circuits.sort()\n",
    "\n",
    "testIdx = set(random.sample(list(range(len(circuits))), int(testSize*len(circuits))))\n",
    "testCircuits = [circuits[x] for x in testIdx]\n",
    "circuits = [n for i, n in enumerate(circuits) if i not in testIdx]\n",
    "circuits.sort()\n",
    "testCircuits.sort()\n",
    "\n",
    "with open('data/CSVInfo.txt', 'w') as fp:\n",
    "  fp.write('Train Circuits:\\n')\n",
    "  for circuit in circuits:\n",
    "    fp.write(circuit+'\\n')\n",
    "  fp.write('\\nTest Circuits:\\n')\n",
    "  for circuit in testCircuits:\n",
    "    fp.write(circuit+'\\n')\n",
    "  fp.close()\n",
    "\n",
    "def processCSV(csvPath):\n",
    "  typesOfDRVs = [\"AdjacentCutSpacing\", \"SameLayerCutSpacing\", \"EndOfLine\", \"FloatingPatch\", \"MinArea\", \"MinWidth\",\n",
    "  \"NonSuficientMetalOverlap\", \"CutShort\", \"MetalShort\", \"OutOfDieShort\", \"CornerSpacing\", \"ParallelRunLength\"]\n",
    "  SelectedDRVTypes = [\"CutShort\", \"MetalShort\"]\n",
    "\n",
    "  df = pd.read_csv(csvPath, dtype=np.float32)\n",
    "  df[\"HasDetailedRoutingViolation\"] = False\n",
    "  for drv in SelectedDRVTypes:\n",
    "    df[\"HasDetailedRoutingViolation\"] = df[label_name] | df[drv]\n",
    "  df = df.drop(columns=typesOfDRVs)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51f21911",
   "metadata": {},
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "saveHeader = True\n",
    "for i in range(len(circuits)):\n",
    "  if i % 10 == 0:\n",
    "    print('reading: ',i,' of ', len(circuits), ' circuits.')\n",
    "  circuit = circuits[i]\n",
    "\n",
    "  tempDF = processCSV(circuit+'_1_nonViol.csv')\n",
    "  if useRandomOversample:\n",
    "    tempDF = tempDF.sample(frac=0.1, replace=False)\n",
    "  if len(tempDF) != 0:\n",
    "    scaler.partial_fit(tempDF.drop(columns=['NodeID', 'HasDetailedRoutingViolation']))\n",
    "  valDF = tempDF.sample(frac=0.2)\n",
    "  tempDF = tempDF.drop(valDF.index)\n",
    "  valDF.to_csv('data/validation.csv', mode='a', index=False, header=saveHeader) \n",
    "  tempDF.to_csv('data/train.csv', mode='a', index=False, header=saveHeader)\n",
    "  saveHeader = False\n",
    "\n",
    "  tempDF = processCSV(circuit+'_1_surround.csv')\n",
    "  if len(tempDF) != 0:\n",
    "    scaler.partial_fit(tempDF.drop(columns=['NodeID', 'HasDetailedRoutingViolation']))\n",
    "  valDF = tempDF.sample(frac=0.2)\n",
    "  tempDF = tempDF.drop(valDF.index)\n",
    "  valDF.to_csv('data/validation.csv', mode='a', index=False, header=False) \n",
    "\n",
    "  tempDF = processCSV(circuit+'_1_viol.csv')\n",
    "  if len(tempDF) != 0:\n",
    "    scaler.partial_fit(tempDF.drop(columns=['NodeID', 'HasDetailedRoutingViolation']))\n",
    "  valDF = tempDF.sample(frac=0.2)\n",
    "  tempDF = tempDF.drop(valDF.index)\n",
    "  valDF.to_csv('data/validation.csv', mode='a', index=False, header=False) \n",
    "  tempDF.to_csv('data/train.csv', mode='a', index=False, header=False)\n",
    "\n",
    "pickle.dump(scaler, open('data/scaler.pkl','wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b000ca2a",
   "metadata": {},
   "source": [
    "saveHeader = True\n",
    "for i in range(len(testCircuits)):\n",
    "  if i % 10 == 0:\n",
    "    print('reading: ',i,' of ', len(testCircuits), ' circuits.')\n",
    "  circuit = testCircuits[i]\n",
    "  processCSV(circuit+'_1_nonViol.csv').to_csv('data/test.csv', mode='a', index=False, header=saveHeader)\n",
    "  saveHeader = False\n",
    "  processCSV(circuit+'_1_surround.csv').to_csv('data/test.csv', mode='a', index=False, header=False)\n",
    "  processCSV(circuit+'_1_viol.csv').to_csv('data/test.csv', mode='a', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
