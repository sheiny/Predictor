{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb56f37",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7846cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import shutil\n",
    "from keras.callbacks import CSVLogger\n",
    "import os\n",
    "from enum import Enum\n",
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c950647",
   "metadata": {},
   "source": [
    "# Generate Train and Test CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuitsDir = '/home/sheiny/workspace/RoutedDesigns/'\n",
    "neighborDistance = 1\n",
    "testSize = 0.2 #20%\n",
    "useRandomOversample = False\n",
    "\n",
    "if os.path.exists('data/'):\n",
    "  shutil.rmtree('data/')\n",
    "  os.mkdir('data/')\n",
    "else:\n",
    "  os.mkdir('data/')\n",
    "\n",
    "typesOfDRVs = [\"AdjacentCutSpacing\", \"SameLayerCutSpacing\", \"EndOfLine\", \"FloatingPatch\", \"MinArea\", \"MinWidth\",\n",
    "  \"NonSuficientMetalOverlap\", \"CutShort\", \"MetalShort\", \"OutOfDieShort\", \"CornerSpacing\", \"ParallelRunLength\"]\n",
    "SelectedDRVTypes = [\"CutShort\", \"MetalShort\"]\n",
    "label_name = \"HasDetailedRoutingViolation\"\n",
    "\n",
    "circuits = []\n",
    "for circuit in os.listdir(circuitsDir):\n",
    "  if os.path.isdir(circuitsDir+'/'+circuit) and circuit != 'nangate45':\n",
    "    for file in os.listdir(circuitsDir+'/'+circuit+'/base/'):\n",
    "      if '_viol.csv' in file:\n",
    "        circuits.append(circuitsDir+circuit+'/base/'+file.split('_')[0])\n",
    "\n",
    "testIdx = set(random.sample(list(range(len(circuits))), int(testSize*len(circuits))))\n",
    "testCircuits = [circuits[x] for x in testIdx]\n",
    "circuits = [n for i, n in enumerate(circuits) if i not in testIdx]\n",
    "\n",
    "with open('data/CSVInfo.txt', 'w') as fp:\n",
    "  fp.write('Train Circuits:\\n')\n",
    "  for circuit in circuits:\n",
    "    fp.write(circuit+'\\n')\n",
    "  fp.write('\\nTest Circuits:\\n')\n",
    "  for circuit in testCircuits:\n",
    "    fp.write(circuit+'\\n')\n",
    "  fp.close()\n",
    "\n",
    "def processCSV(csvPath):\n",
    "  typesOfDRVs = [\"AdjacentCutSpacing\", \"SameLayerCutSpacing\", \"EndOfLine\", \"FloatingPatch\", \"MinArea\", \"MinWidth\",\n",
    "  \"NonSuficientMetalOverlap\", \"CutShort\", \"MetalShort\", \"OutOfDieShort\", \"CornerSpacing\", \"ParallelRunLength\"]\n",
    "  SelectedDRVTypes = [\"CutShort\", \"MetalShort\"]\n",
    "\n",
    "  df = pd.read_csv(csvPath, dtype=np.float32)\n",
    "  df[\"HasDetailedRoutingViolation\"] = False\n",
    "  for drv in SelectedDRVTypes:\n",
    "    df[\"HasDetailedRoutingViolation\"] = df[label_name] | df[drv]\n",
    "  df = df.drop(columns=typesOfDRVs)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1340da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "saveHeader = True\n",
    "for i in range(len(circuits)):\n",
    "  if i % 10 == 0:\n",
    "    print('reading: ',i,' of ', len(circuits), ' circuits.')\n",
    "  circuit = circuits[i]\n",
    "\n",
    "  tempDF = processCSV(circuit+'_1_nonViol.csv')\n",
    "  if useRandomOversample:\n",
    "    tempDF = tempDF.sample(frac=0.1, replace=False)\n",
    "  if len(tempDF) != 0:\n",
    "    scaler.partial_fit(tempDF.drop(columns=['NodeID', 'HasDetailedRoutingViolation']))\n",
    "  tempDF.to_csv('data/train.csv', mode='a', index=False, header=saveHeader)\n",
    "  saveHeader = False\n",
    "\n",
    "  tempDF = processCSV(circuit+'_1_surround.csv')\n",
    "  if len(tempDF) != 0:\n",
    "    scaler.partial_fit(tempDF.drop(columns=['NodeID', 'HasDetailedRoutingViolation']))\n",
    "  tempDF.to_csv('data/train.csv', mode='a', index=False, header=False)\n",
    "\n",
    "  tempDF = processCSV(circuit+'_1_viol.csv')\n",
    "  if len(tempDF) != 0:\n",
    "    scaler.partial_fit(tempDF.drop(columns=['NodeID', 'HasDetailedRoutingViolation']))\n",
    "  tempDF.to_csv('data/train.csv', mode='a', index=False, header=False)\n",
    "\n",
    "pickle.dump(scaler, open('data/scaler.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveHeader = True\n",
    "for i in range(len(testCircuits)):\n",
    "  if i % 10 == 0:\n",
    "    print('reading: ',i,' of ', len(testCircuits), ' circuits.')\n",
    "  circuit = testCircuits[i]\n",
    "  processCSV(circuit+'_1_nonViol.csv').to_csv('data/test.csv', mode='a', index=False, header=saveHeader)\n",
    "  saveHeader = False\n",
    "  processCSV(circuit+'_1_surround.csv').to_csv('data/test.csv', mode='a', index=False, header=False)\n",
    "  processCSV(circuit+'_1_viol.csv').to_csv('data/test.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea80aed",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8209cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_positive_ratio(train_labels):\n",
    "    neg, pos = np.bincount(train_labels)\n",
    "    total = neg + pos\n",
    "    print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "\n",
    "# Claculate weight for classes\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "def calculate_class_weights(train_labels):\n",
    "    neg, pos = np.bincount(train_labels)\n",
    "    total = neg + pos\n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    return class_weight, neg, pos\n",
    "\n",
    "def oversample(train_array, train_labels):\n",
    "    oversample = RandomOverSampler()\n",
    "    train_array, train_labels = oversample.fit_resample(train_array, train_labels)\n",
    "    return train_array, train_labels\n",
    "\n",
    "def undersample(train_array, train_labels):\n",
    "    undersample = RandomUnderSampler()\n",
    "    train_array, train_labels = undersample.fit_resample(train_array, train_labels)\n",
    "    return train_array, train_labels\n",
    "\n",
    "########## Learning Model ##########\n",
    "def make_model(evalMetrics, dropOut, learningRate, inputSize, numNodes, numLayers, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=inputSize))\n",
    "    for x in range(numLayers):\n",
    "        model.add(tf.keras.layers.Dense(numNodes, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(dropOut))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=evalMetrics)\n",
    "    return model\n",
    "\n",
    "########## Test and check Performance ##########\n",
    "def calculate_test_metrics(model, results):\n",
    "    m = {}\n",
    "    for name, value in zip(model.metrics_names, results):\n",
    "        m[name] = value\n",
    "    if m['precision'] + m['recall'] != 0:\n",
    "        f_score = 2 * ((m['precision'] * m['recall'])/(m['precision'] + m['recall']))\n",
    "        m['F-score'] = f_score\n",
    "    sqrt = math.sqrt((m['tp']+m['fp'])*(m['tp']+m['fn'])*(m['tn']+m['fp'])*(m['tn']+m['fn']))\n",
    "    if sqrt != 0:\n",
    "        mcc = ((m['tp'] * m['tn']) - (m['fp'] * m['fn']))/sqrt\n",
    "        m['MCC'] = mcc\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bfa8cf",
   "metadata": {},
   "source": [
    "# Learning Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e54192",
   "metadata": {},
   "source": [
    "# Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test.csv')\n",
    "# df = pd.read_csv('data/train.csv')\n",
    "# Remove NodeIDs (debug info)\n",
    "df = df.drop(columns=[\"NodeID\"])\n",
    "\n",
    "batch_size = 32 # is important to ensure that each batch has a decent chance of containing a few positive samples\n",
    "epochs = 10\n",
    "learningRate = 0.001 #Eh?Predictor=0.05, default=0.001\n",
    "dropOut = 0.05 #Eh?Predictor=0.05\n",
    "evalMetrics = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "               tf.keras.metrics.FalsePositives(name='fp'),\n",
    "               tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "               tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "               tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "               tf.keras.metrics.Precision(name='precision'),\n",
    "               tf.keras.metrics.Recall(name='recall'),\n",
    "               tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split 80/20 (train 80% test 20%)\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Build np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop(\"HasDetailedRoutingViolation\"))\n",
    "val_labels = np.array(val_df.pop(\"HasDetailedRoutingViolation\"))\n",
    "\n",
    "# READ scaler\n",
    "# Transform\n",
    "\n",
    "print_positive_ratio(train_labels)\n",
    "\n",
    "train_array = np.array(train_df)\n",
    "val_array = np.array(val_df)\n",
    "\n",
    "# Save some memory\n",
    "del train_df\n",
    "del val_df\n",
    "\n",
    "scaler = pickle.load(open('data/scaler.pkl','rb'))\n",
    "train_array = scaler.transform(train_array)\n",
    "val_array = scaler.transform(val_array)\n",
    "\n",
    "# Create Model\n",
    "\n",
    "\n",
    "# Train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21711b28",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccdcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSVs\n",
    "# Drop node IDS\n",
    "# Scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
