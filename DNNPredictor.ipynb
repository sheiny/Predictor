{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb56f37",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7846cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 11:13:33.417530: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 11:13:34.004890: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-18 11:13:35.065209: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 11:13:35.065397: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 11:13:35.065406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import shutil\n",
    "from keras.callbacks import CSVLogger\n",
    "import os\n",
    "from enum import Enum\n",
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c950647",
   "metadata": {},
   "source": [
    "# Generate Train and Test CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se nao funcionar agora deve ter a ver com o dtype nao ser o floar bla bla bla\n",
    "# uma solucao de contorno pode ser usar float para salvar e abrir, porem castar as labels para inteiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c29ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuitsDir = '/home/sheiny/workspace/RoutedDesigns/'\n",
    "neighborDistance = 1\n",
    "testSize = 0.2 #20%\n",
    "useRandomOversample = False\n",
    "memoryTarget=\"128MB\" #256Mb because it might use a little bit more then this\n",
    "\n",
    "if os.path.exists('data/'):\n",
    "  shutil.rmtree('data/')\n",
    "  os.mkdir('data/')\n",
    "else:\n",
    "  os.mkdir('data/')\n",
    "\n",
    "typesOfDRVs = [\"AdjacentCutSpacing\", \"SameLayerCutSpacing\", \"EndOfLine\", \"FloatingPatch\", \"MinArea\", \"MinWidth\",\n",
    "  \"NonSuficientMetalOverlap\", \"CutShort\", \"MetalShort\", \"OutOfDieShort\", \"CornerSpacing\", \"ParallelRunLength\"]\n",
    "SelectedDRVTypes = [\"CutShort\", \"MetalShort\"]\n",
    "label_name = \"HasDetailedRoutingViolation\"\n",
    "\n",
    "circuits = []\n",
    "for circuit in os.listdir(circuitsDir):\n",
    "  if os.path.isdir(circuitsDir+'/'+circuit) and circuit != 'nangate45':\n",
    "    for file in os.listdir(circuitsDir+'/'+circuit+'/base/'):\n",
    "      if '_viol.csv' in file:\n",
    "        circuits.append(circuitsDir+circuit+'/base/'+file.split('_')[0])\n",
    "\n",
    "testIdx = set(random.sample(list(range(len(circuits))), int(testSize*len(circuits))))\n",
    "testCircuits = [circuits[x] for x in testIdx]\n",
    "circuits = [n for i, n in enumerate(circuits) if i not in testIdx]\n",
    "\n",
    "with open('data/CSVInfo.txt', 'w') as fp:\n",
    "  fp.write('Train Circuits:\\n')\n",
    "  for circuit in circuits:\n",
    "    fp.write(circuit+'\\n')\n",
    "  fp.write('\\nTest Circuits:\\n')\n",
    "  for circuit in testCircuits:\n",
    "    fp.write(circuit+'\\n')\n",
    "  fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1340da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading:  0  of  178  circuits.\n",
      "reading:  10  of  178  circuits.\n",
      "reading:  20  of  178  circuits.\n",
      "reading:  30  of  178  circuits.\n",
      "reading:  40  of  178  circuits.\n",
      "reading:  50  of  178  circuits.\n",
      "reading:  60  of  178  circuits.\n",
      "reading:  70  of  178  circuits.\n",
      "reading:  80  of  178  circuits.\n",
      "reading:  90  of  178  circuits.\n",
      "reading:  100  of  178  circuits.\n",
      "reading:  110  of  178  circuits.\n",
      "reading:  120  of  178  circuits.\n",
      "reading:  130  of  178  circuits.\n",
      "reading:  140  of  178  circuits.\n",
      "reading:  150  of  178  circuits.\n",
      "reading:  160  of  178  circuits.\n",
      "reading:  170  of  178  circuits.\n",
      "Concatenating data frames.\n",
      "Writing CSVs.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NDFrame.to_csv() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# del trainDFs #save some memory\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWriting CSVs.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tensorflow/lib/python3.10/site-packages/dask/dataframe/core.py:1705\u001b[0m, in \u001b[0;36m_Frame.to_csv\u001b[0;34m(self, filename, **kwargs)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;124;03m\"\"\"See dd.to_csv docstring for more information\"\"\"\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_csv\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tensorflow/lib/python3.10/site-packages/dask/dataframe/io/csv.py:988\u001b[0m, in \u001b[0;36mto_csv\u001b[0;34m(df, filename, single_file, encoding, mode, name_function, compression, compute, scheduler, storage_options, header_first_partition_only, compute_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m         compute_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m scheduler\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/Tensorflow/lib/python3.10/site-packages/dask/base.py:600\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    598\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 600\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/Tensorflow/lib/python3.10/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/Tensorflow/lib/python3.10/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/Tensorflow/lib/python3.10/site-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Tensorflow/lib/python3.10/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[0;32m~/Tensorflow/lib/python3.10/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/Tensorflow/lib/python3.10/site-packages/dask/utils.py:71\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(func, args, kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"Apply a function given its positional and keyword arguments.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mEquivalent to ``func(*args, **kwargs)``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m>>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Tensorflow/lib/python3.10/site-packages/dask/dataframe/io/csv.py:787\u001b[0m, in \u001b[0;36m_write_csv\u001b[0;34m(df, fil, depend_on, **kwargs)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write_csv\u001b[39m(df, fil, \u001b[38;5;241m*\u001b[39m, depend_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m fil \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 787\u001b[0m         \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(fil\u001b[38;5;241m.\u001b[39mpath)\n",
      "File \u001b[0;32m~/Tensorflow/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: NDFrame.to_csv() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "#IF CORE ABOVE FAILS\n",
    "trainDFs = []\n",
    "for i in range(len(circuits)):\n",
    "  if i % 10 == 0:\n",
    "    print('reading: ',i,' of ', len(circuits), ' circuits.')\n",
    "  circuit = circuits[i]\n",
    "\n",
    "  tempDF = dd.read_csv(circuit+'_1_nonViol.csv', dtype=np.float32, blocksize=memoryTarget)\n",
    "  if useRandomOversample:\n",
    "    tempDF = tempDF.sample(frac=0.1, replace=False)\n",
    "  for col in typesOfDRVs:\n",
    "    tempDF[col] = tempDF[col].astype(bool)\n",
    "  tempDF[label_name] = tempDF[label_name].astype(bool)\n",
    "  trainDFs.append(tempDF)\n",
    "\n",
    "  tempDF = dd.read_csv(circuit+'_1_surround.csv', dtype=np.float32, blocksize=memoryTarget)\n",
    "  for col in typesOfDRVs:\n",
    "    tempDF[col] = tempDF[col].astype(bool)\n",
    "  tempDF[label_name] = tempDF[label_name].astype(bool)\n",
    "  trainDFs.append(tempDF)\n",
    "\n",
    "  tempDF = dd.read_csv(circuit+'_1_viol.csv', dtype=np.float32, blocksize=memoryTarget)\n",
    "  for col in typesOfDRVs:\n",
    "    tempDF[col] = tempDF[col].astype(bool)\n",
    "  tempDF[label_name] = tempDF[label_name].astype(bool)\n",
    "  trainDFs.append(tempDF)\n",
    "\n",
    "print('Concatenating data frames.')\n",
    "df = dd.concat(trainDFs, ignore_index=True)\n",
    "# del trainDFs #save some memory\n",
    "\n",
    "print('Writing CSVs.')\n",
    "df.to_csv('data/train.csv', index=False, single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334564f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Scaler\n",
    "df = dd.read_csv('data/train.csv', blocksize=memoryTarget)\n",
    "df = df.drop(columns=[\"NodeID\"])\n",
    "\n",
    "# Make sure to clear all DRV columns\n",
    "print('Applying DRV filter.')\n",
    "df[label_name] = False\n",
    "# Apply filter for selected DRVs\n",
    "for drv in SelectedDRVTypes:\n",
    "  df[label_name] = df[label_name] | df[drv]\n",
    "\n",
    "\n",
    "# Drop all drv collumns because they are no longer necessary\n",
    "print('Dropping unnecessary DRV collumns.')\n",
    "df = df.drop(columns=typesOfDRVs)\n",
    "\n",
    "print('Writing CSVs.')\n",
    "df.to_csv('data/train2.csv', index=False)\n",
    "\n",
    "print('Calculating mean and stdv.')\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "train_array = np.array(df)\n",
    "del df #save some memory\n",
    "train_array = scaler.fit(train_array)\n",
    "pickle.dump(scaler, open('data/scaler.pkl','wb'))\n",
    "del train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDFs = []\n",
    "for i in range(len(testCircuits)):\n",
    "  if i % 10 == 0:\n",
    "    print('reading: ',i,' of ', len(testCircuits), ' circuits.')\n",
    "  circuit = testCircuits[i]\n",
    "  testDFs.append(dd.read_csv(circuit+'_1_nonViol.csv', blocksize=memoryTarget))\n",
    "  testDFs.append(dd.read_csv(circuit+'_1_surround.csv', blocksize=memoryTarget))\n",
    "  testDFs.append(dd.read_csv(circuit+'_1_viol.csv', blocksize=memoryTarget))\n",
    "print('Concatenating data frames.')\n",
    "test_df = dd.concat(testDFs, ignore_index=True)\n",
    "del testDFs #save some memory\n",
    "\n",
    "print('Applying DRV filter.')\n",
    "# Make sure to clear all DRV columns\n",
    "test_df[label_name] = False\n",
    "# Apply filter for selected DRVs\n",
    "for drv in SelectedDRVTypes:\n",
    "  test_df[label_name] = test_df[label_name] | test_df[drv]\n",
    "\n",
    "print('Dropping unnecessary DRV collumns.')\n",
    "# Drop all drv collumns because they are no longer necessary\n",
    "test_df = test_df.drop(columns=typesOfDRVs)\n",
    "\n",
    "print('Writing CSVs.')\n",
    "test_df.to_csv('data/test.csv', index=False, single_file=True)\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3755454",
   "metadata": {},
   "source": [
    "trainDFs = []\n",
    "for i in range(len(circuits)):\n",
    "  if i % 10 == 0:\n",
    "    print('reading: ',i,' of ', len(circuits), ' circuits.')\n",
    "  circuit = circuits[i]\n",
    "  nonViolDF = dd.read_csv(circuit+'_1_nonViol.csv', blocksize=memoryTarget)\n",
    "  if useRandomOversample:\n",
    "    nonViolDF = nonViolDF.sample(frac=0.1, replace=False)\n",
    "  trainDFs.append(nonViolDF)\n",
    "  trainDFs.append(dd.read_csv(circuit+'_1_surround.csv', blocksize=memoryTarget))\n",
    "  trainDFs.append(dd.read_csv(circuit+'_1_viol.csv', blocksize=memoryTarget))\n",
    "print('Concatenating data frames.')\n",
    "df = dd.concat(trainDFs, ignore_index=True)\n",
    "del trainDFs #save some memory\n",
    "\n",
    "# Make sure to clear all DRV columns\n",
    "print('Applying DRV filter.')\n",
    "df[label_name] = False\n",
    "# Apply filter for selected DRVs\n",
    "for drv in SelectedDRVTypes:\n",
    "  df[label_name] = df[label_name] | df[drv]\n",
    "\n",
    "# Drop all drv collumns because they are no longer necessary\n",
    "print('Dropping unnecessary DRV collumns.')\n",
    "df = df.drop(columns=typesOfDRVs)\n",
    "\n",
    "print('Writing CSVs.')\n",
    "df.to_csv('data/train.csv', index=False, single_file=True)\n",
    "del df #save some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea80aed",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8209cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_positive_ratio(train_labels):\n",
    "    neg, pos = np.bincount(train_labels)\n",
    "    total = neg + pos\n",
    "    print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "\n",
    "# Claculate weight for classes\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "def calculate_class_weights(train_labels):\n",
    "    neg, pos = np.bincount(train_labels)\n",
    "    total = neg + pos\n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    return class_weight, neg, pos\n",
    "\n",
    "def oversample(train_array, train_labels):\n",
    "    oversample = RandomOverSampler()\n",
    "    train_array, train_labels = oversample.fit_resample(train_array, train_labels)\n",
    "    return train_array, train_labels\n",
    "\n",
    "def undersample(train_array, train_labels):\n",
    "    undersample = RandomUnderSampler()\n",
    "    train_array, train_labels = undersample.fit_resample(train_array, train_labels)\n",
    "    return train_array, train_labels\n",
    "\n",
    "########## Learning Model ##########\n",
    "def make_model(evalMetrics, dropOut, learningRate, inputSize, numNodes, numLayers, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=inputSize))\n",
    "    for x in range(numLayers):\n",
    "        model.add(tf.keras.layers.Dense(numNodes, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(dropOut))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=evalMetrics)\n",
    "    return model\n",
    "\n",
    "########## Test and check Performance ##########\n",
    "def calculate_test_metrics(model, results):\n",
    "    m = {}\n",
    "    for name, value in zip(model.metrics_names, results):\n",
    "        m[name] = value\n",
    "    if m['precision'] + m['recall'] != 0:\n",
    "        f_score = 2 * ((m['precision'] * m['recall'])/(m['precision'] + m['recall']))\n",
    "        m['F-score'] = f_score\n",
    "    sqrt = math.sqrt((m['tp']+m['fp'])*(m['tp']+m['fn'])*(m['tn']+m['fp'])*(m['tn']+m['fn']))\n",
    "    if sqrt != 0:\n",
    "        mcc = ((m['tp'] * m['tn']) - (m['fp'] * m['fn']))/sqrt\n",
    "        m['MCC'] = mcc\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bfa8cf",
   "metadata": {},
   "source": [
    "# Learning Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e54192",
   "metadata": {},
   "source": [
    "# Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test.csv')\n",
    "# df = pd.read_csv('data/train.csv')\n",
    "# Remove NodeIDs (debug info)\n",
    "df = df.drop(columns=[\"NodeID\"])\n",
    "\n",
    "# READ scaler\n",
    "# Transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32 # is important to ensure that each batch has a decent chance of containing a few positive samples\n",
    "epochs = 10\n",
    "learningRate = 0.001 #Eh?Predictor=0.05, default=0.001\n",
    "dropOut = 0.05 #Eh?Predictor=0.05\n",
    "evalMetrics = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "               tf.keras.metrics.FalsePositives(name='fp'),\n",
    "               tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "               tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "               tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "               tf.keras.metrics.Precision(name='precision'),\n",
    "               tf.keras.metrics.Recall(name='recall'),\n",
    "               tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split 80/20 (train 80% test 20%)\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Build np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop(\"HasDetailedRoutingViolation\"))\n",
    "val_labels = np.array(val_df.pop(\"HasDetailedRoutingViolation\"))\n",
    "\n",
    "print_positive_ratio(train_labels)\n",
    "\n",
    "train_array = np.array(train_df)\n",
    "val_array = np.array(val_df)\n",
    "\n",
    "# Save some memory\n",
    "del train_df\n",
    "del val_df\n",
    "\n",
    "scaler = pickle.load(open('data/scaler.pkl','rb'))\n",
    "train_array = scaler.transform(train_array)\n",
    "val_array = scaler.transform(val_array)\n",
    "\n",
    "# Create Model\n",
    "\n",
    "\n",
    "# Train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21711b28",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccdcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSVs\n",
    "# Drop node IDS\n",
    "# Scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
