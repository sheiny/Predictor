{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e3a2b8",
   "metadata": {},
   "source": [
    "# Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f04caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\n",
    "https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right\n",
    "\n",
    "a figura com drvs justifica muito bem a escolha de 80% para teste\n",
    "\n",
    "\n",
    "fazer eval em todos dfs de 80%, adicionar epoch e indices etc para depois pegar caso necessario\n",
    "pegar melhor modelo olhando o loss e aplicar pra todos circuitos de teste fcn, ou fcn_large\n",
    "\n",
    "\n",
    "\n",
    "Figura:\n",
    "como alguem faria para usar o preditor\n",
    "como alguem faria para testar com outra tecnologia o preditor\n",
    "explicar que ele he independente de tecnologia\n",
    "fluxograma mostrando a parte do ORDF e a parte do INNOVUS para geracao dos benchamrks\n",
    "dps para geracao dos dados de treino e inferencia.\n",
    "Explicar o problema dos dados, não é só uma questão de armazenamento, mas a leitura deles\n",
    "tbm seria muito demorada inviabilizando o treino do modelo.\n",
    "justificar o porque de pegar soh short"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb56f37",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021c96c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e65aaa-d80a-426f-bfb3-ee365dadf140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN0 = [[('bp_multi', 82), ('swerv', 84), ('bp_multi', 87), ('bp_multi', 86)],\n",
    "# FCN1 = [('aes', 88), ('bp_be', 77), ('bp_be', 76), ('bp_multi', 85)],\n",
    "# FCN2 = [('aes', 76), ('aes', 80), ('aes', 73), ('bp_multi', 90)],\n",
    "# FCN3 = [('aes', 87), ('bp_multi', 84), ('ibex', 90), ('aes', 89)],\n",
    "# FCN4 = [('tinyRocket', 90), ('swerv', 82), ('bp_be', 80), ('bp_be', 75)]]\n",
    "\n",
    "\n",
    "def getMetrics(tp, tn, fp, fn):\n",
    "  specificity = tn/(fp+tn) # True Negative Rate (TNR), is also known as Specificity or Selectivity.\n",
    "  sensitivity = tp/(tp+fn) # True Positive Rate (TPR), also known as Sensitivity, Recall, or the Hit Rate\n",
    "  prevalence = ((tp+fn)/(tp+fn+fp+tn))\n",
    "  accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "  # npv = tn/(fn+tn) # Negative Predictive Value (NPV)\n",
    "  # fpr = fp/(tn+fp) # False Positive Rate (FPR), also known as the Fall-Out or Type I Error Rate or False Alarm Rate\n",
    "  # fnr = fn / (fn + tp)# False Negative Rate (FNR), It's also known as the Miss Rate or Type II Error Rate.\n",
    "\n",
    "  sqrt = math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "  mcc = 0\n",
    "  if sqrt != 0:\n",
    "    mcc = ((tp * tn) - (fp * fn))/sqrt\n",
    "\n",
    "  precision = tp/(tp + fp)# Positive Predictive Value (PPV), also known as Precision.\n",
    "  beta=10\n",
    "  fbeta = ((1 + pow(beta, 2)) * precision * sensitivity) / (pow(beta, 2) * precision + sensitivity)\n",
    "  f1score = (2 * (precision * sensitivity)) / (precision + sensitivity)\n",
    "  return {'tp':tp, 'tn':tn, 'fp':fp, 'fn':fn,\n",
    "          'prevalence':prevalence,\n",
    "          'specificity':specificity,\n",
    "          'sensitivity':sensitivity,\n",
    "          'precision':precision,\n",
    "          'gmean':math.sqrt(sensitivity * specificity),\n",
    "          'f1score':f1score,\n",
    "          'fscore':fbeta,\n",
    "          'accuracy':accuracy,\n",
    "          'mcc':mcc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9fc59e",
   "metadata": {},
   "source": [
    "# Cross Validation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98856eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossValPath = '/home/sheiny/workspace/Predictor/results/FCNCV/'\n",
    "# crossValPath = '/home/sheiny/workspace/Predictor/results/FCNModel10CV/'\n",
    "pkls = [crossValPath+x for x in os.listdir(crossValPath) if ('.pkl' in x and 'CV' in x)]\n",
    "pkls.sort(key=lambda x:int(re.search(r\"\\d+(\\.\\d+)?\", x[x.rfind('/'):]).group(0)))\n",
    "\n",
    "cvDicts = []\n",
    "for group in range(len(pkls)):\n",
    "  df = pickle.load(open(pkls[group], 'rb'))\n",
    "  runs = {(row['Design'], row['Density']) for index, row in df.iterrows()}\n",
    "  for run in runs:\n",
    "    tempDf = df.loc[(df['Design'] == run[0]) & (df['Density'] == run[1])]\n",
    "    metrics = getMetrics(sum(tempDf['tp']), sum(tempDf['tn']), sum(tempDf['fp']), sum(tempDf['fn']))\n",
    "    cvDicts.append({'Group':str(int(group)),\n",
    "                    'Design':run[0]+' '+str(run[1]),\n",
    "                    'TP':metrics['tp'],\n",
    "                    'TN':metrics['tn'],\n",
    "                    'FP':metrics['fp'],\n",
    "                    'FN':metrics['fn'],\n",
    "                    'Prevalence %':metrics['prevalence']*100,\n",
    "                    'Specificity %':metrics['specificity']*100,\n",
    "                    'Sensitivity %':metrics['sensitivity']*100,\n",
    "                    'G-Mean %':math.sqrt(metrics['sensitivity'] * metrics['specificity'])*100,\n",
    "                    # 'f1-score':metrics['f1score'],\n",
    "                    'F$_\\beta$-score*':metrics['fscore'],\n",
    "                    # 'precision':metrics['precision'],\n",
    "                    'Accuracy %':metrics['accuracy']*100,\n",
    "                    'MCC [-1:1]':metrics['mcc']})\n",
    "cvDf = pd.DataFrame.from_dict(cvDicts)\n",
    "# cvDf.loc['mean'] = cvDf.mean(numeric_only=True)\n",
    "cvDf\n",
    "# cvDf.to_csv('results/CrossValidation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d08714",
   "metadata": {},
   "source": [
    "# Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl = '/home/sheiny/workspace/Predictor/results/PredictionExperiment/predictRuns.pkl'\n",
    "df = pickle.load(open(pkl, 'rb'))\n",
    "\n",
    "circuitResultDict = []\n",
    "runs = {(row['Design'], row['Density']) for index, row in df.iterrows()}\n",
    "for run in runs:\n",
    "  tempDf = df.loc[(df['Design'] == run[0]) & (df['Density'] == run[1])]\n",
    "  metrics = getMetrics(sum(tempDf['tp']), sum(tempDf['tn']), sum(tempDf['fp']), sum(tempDf['fn']))\n",
    "  tpr = metrics['tp']/(metrics['tp']+metrics['fn'])\n",
    "\n",
    "  PosRatio = ((metrics['tp']+metrics['fn'])/(metrics['tp']+metrics['fn']+metrics['fp']+metrics['fn']))*100\n",
    "  circuitResultDict.append({'Design':run[0]+str(run[1]),\n",
    "                            'tp':str(metrics['tp'])+' ('+str(int(tpr*100))+'%)', 'tn':int(metrics['tn']),\n",
    "                            'fp':int(metrics['fp']), 'fn':int(metrics['fn']),\n",
    "                            'PosRatio%':PosRatio,\n",
    "                            'spc %':metrics['specificity']*100, 'acc %':metrics['accuracy']*100, 'MCC[-1:1]':metrics['mcc']})\n",
    "resultDf = pd.DataFrame.from_dict(circuitResultDict)\n",
    "resultDf.set_index('Design', inplace=True)\n",
    "resultDf.to_csv('results/predictionResult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e3747",
   "metadata": {},
   "source": [
    "# Plot DRVs and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e12dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "design = '/home/sheiny/workspace/data/aes80/'\n",
    "modelPath = 'results/Training/fcn2/model_14.pkl'\n",
    "outPath = 'predictions/aes80_'\n",
    "\n",
    "\n",
    "\n",
    "def writePredictionFile(outFile, nodesToWrite):\n",
    "  with open(outFile, 'w') as fp:\n",
    "    for node in nodesToWrite:\n",
    "      fp.write(str(node)+'\\n')\n",
    "  fp.close()\n",
    "\n",
    "def predict(model, pkl):\n",
    "  testDf = pd.read_pickle(pkl, compression='zip')\n",
    "  labels = testDf.pop(testDf.columns.values[-1])\n",
    "  nodeIDs = testDf.pop(testDf.columns.values[0])#drop first column which contains the nodeIds\n",
    "  testHyperImages = np.array(testDf).reshape(len(testDf),22,33,33)\n",
    "  predictions = model.predict(testHyperImages)\n",
    "  return nodeIDs, predictions, labels\n",
    "\n",
    "model = pickle.load(open(modelPath, 'rb'))\n",
    "pkls = [design+x for x in os.listdir(design) if '.pkl' in x]\n",
    "pkls.sort(key = lambda x : int(x[x.rfind('_')+1:x.find('.')]))\n",
    "results = []\n",
    "for pkl in pkls:\n",
    "  results.append(predict(model, pkl))\n",
    "\n",
    "tp = []\n",
    "tn = []\n",
    "fp = []\n",
    "fn = []\n",
    "for x, y, z in results:\n",
    "  nodeIDs = [int(n) for n in x]\n",
    "  predictions = y > 0.5\n",
    "  for id, prediction, actual in zip(nodeIDs, predictions, z):\n",
    "    if prediction == actual and actual == True:\n",
    "      tp.append(id)\n",
    "    elif prediction == actual and actual == False:\n",
    "      tn.append(id)\n",
    "    elif prediction != actual and actual == True:\n",
    "      fn.append(id)\n",
    "    else:\n",
    "      fp.append(id)\n",
    "\n",
    "writePredictionFile(outPath+'tp.txt', tp)\n",
    "writePredictionFile(outPath+'tn.txt', tn)\n",
    "writePredictionFile(outPath+'fp.txt', fp)\n",
    "writePredictionFile(outPath+'fn.txt', fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d418c",
   "metadata": {},
   "source": [
    "# Benchmark Information Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19541ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beware the number of components is wrong!!! It is considering filler cells\n",
    "df = pd.read_pickle('benchmarkInfo/ufscbenchmark.pkl', compression='zip')\n",
    "df = df.loc[df['Design'] != 'bp']\n",
    "df = df.loc[df['Design'] != 'gcd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Info\n",
    "df2 = df.loc[df['FDRVTotal'] == 0]\n",
    "circuits = set(df2['Design'])\n",
    "benchmarkInfo = []\n",
    "for c in circuits:\n",
    "  dfCircuit = df2.loc[df['Design'] == c]\n",
    "  designCount = len(dfCircuit)\n",
    "  avgComponent = dfCircuit['COMPONENTS'].mean()\n",
    "  avgNets = dfCircuit['NETS'].mean()\n",
    "  avgPins = dfCircuit['PINS'].mean()\n",
    "  avgBlkgs = dfCircuit['BLOCKAGES'].mean()\n",
    "  benchmarkInfo.append({'Design':c, 'Design Count':int(designCount),\n",
    "                        'Components':int(avgComponent), 'Nets':int(avgNets),\n",
    "                        'Pins':int(avgPins), 'Blockages':int(avgBlkgs)})\n",
    "\n",
    "benchmarkDfInfo = pd.DataFrame.from_dict(benchmarkInfo)\n",
    "benchmarkDfInfo.sort_values('Components', ascending=False, inplace=True)\n",
    "benchmarkDfInfo.loc['mean'] = benchmarkDfInfo.mean(numeric_only=True)\n",
    "# benchmarkDfInfo\n",
    "benchmarkDfInfo.to_csv('benchmarkInfo/opencores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling result\n",
    "df2 = df.loc[df['FDRVTotal'] == 0]\n",
    "circuits = set(df2['Design'])\n",
    "benchmarkInfo = []\n",
    "for c in circuits:\n",
    "  dfCircuit = df2.loc[df['Design'] == c]\n",
    "  avgTotalGridSize = dfCircuit['TotalSizeGrid'].mean()\n",
    "  reduction = (1.0-10000/avgTotalGridSize)*100\n",
    "  avgPosSamples = dfCircuit['Positives'].mean()\n",
    "  stdPos = dfCircuit['Positives'].std()\n",
    "  afterPosRatio = (avgPosSamples/10000)*100\n",
    "  benchmarkInfo.append({'Design':c, 'Original Grid Size':int(avgTotalGridSize),\n",
    "                        'Size Reduction \\%':reduction,\n",
    "                        'Positive Samples':str(int(avgPosSamples))+' (±'+str(int(stdPos))+')',\n",
    "                        'Undersampled Positive %':afterPosRatio})\n",
    "\n",
    "benchmarkDfInfo = pd.DataFrame.from_dict(benchmarkInfo)\n",
    "benchmarkDfInfo.sort_values('Original Grid Size', ascending=False, inplace=True)\n",
    "benchmarkDfInfo.to_csv('benchmarkInfo/undersample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a92600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot Routing Runtime Break Down\n",
    "dfRuntime = df.loc[df['Design'].str.contains('80')].sort_values(by=['COMPONENTS'], ascending=False)\n",
    "dfRuntime['Design'] = [x[0:x.rfind('_')] for x in dfRuntime['Design']]\n",
    "dfRuntime = dfRuntime.set_index('Design')\n",
    "numIDRShorts = dfRuntime['IDRShort']\n",
    "dfRuntime = dfRuntime[['GR', 'IDR', 'FDR']]\n",
    "ax = dfRuntime.plot.bar(stacked=True)\n",
    "\n",
    "numBars = int(len(ax.patches)/3)\n",
    "for rect, value in zip(ax.patches[numBars:numBars*2], numIDRShorts):\n",
    "  h = rect.get_height() /2.\n",
    "  w = rect.get_width() /2.\n",
    "  x, y = rect.get_xy()\n",
    "  ax.text(x+w, y+h, value, horizontalalignment='center',verticalalignment='center')\n",
    "\n",
    "ax.set_ylabel('Runtime in seconds')\n",
    "ax.set_xlabel('Designs (80% row utilization)')\n",
    "ax.legend([\"Global Routing\", \"Initial Detailed Routing\", \"Complete Detailed Routing\"])\n",
    "plt.title('Routing Runtime Break Down')\n",
    "# plt.savefig('routing_runtime.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg IDR short (considering full routed circuits only)\n",
    "idrShort = []\n",
    "for x in range(70, 91):\n",
    "  tempDf = df.loc[df['Design'].str.contains(str(x))]\n",
    "  tempDf = tempDf.loc[tempDf['FDRTotal'] == 0]\n",
    "  avgIDRShort = sum(tempDf['IDRShort']/len(tempDf))\n",
    "  idrShort.append(avgIDRShort)\n",
    "    \n",
    "plt.plot([y for y in range(70, 91)], idrShort, color = 'r')\n",
    "plt.xlabel(\"Design Density (Row Utilization %)\")\n",
    "plt.ylabel(\"Initial Detailed Routing Short Violations (IDRV)\")\n",
    "plt.title('Average IDR Short x Row Utilization (Only fully routable circuits)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0532a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg IDR short (considering full routed circuits only)\n",
    "fdrRuntime = []\n",
    "for x in range(70, 91):\n",
    "  tempDf = df.loc[df['Design'].str.contains(str(x))]\n",
    "  tempDf = tempDf.loc[tempDf['FDRTotal'] == 0]\n",
    "  fdr = sum(tempDf['FDR']/len(tempDf))\n",
    "  fdrRuntime.append(fdr)\n",
    "\n",
    "plt.plot([y for y in range(70, 91)], fdrRuntime, color = 'r')\n",
    "plt.xlabel(\"Design Density (Row Utilization %)\")\n",
    "plt.ylabel(\"Runtime (seconds)\")\n",
    "plt.title('Average Runtime to complete routing (Only fully routable circuits)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b8cde-0278-4554-b8d8-d581b2ddfb88",
   "metadata": {},
   "source": [
    "# Related Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95767987-dde6-47cf-82d3-5d3ad327ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "works = {#DRC Violation Prediction with Pre-global-routing Features Through Convolutional Neural Network\n",
    "         '\\cite{tabrizi2019eh}':{'tp':4343, 'tn':225688, 'fp':10990, 'fn':102},# Eh? predictor: A deep learning framework to identify detailed routing short violations from a placed netlist\n",
    "        }\n",
    "\n",
    "# Transforming Global Routing Report into DRC Violation Map with Convolutional Neural Network\n",
    "# DRC Violation Prediction with Pre-global-routing Features Through Convolutional Neural Network\n",
    "# DRC Violation Prediction After Global Route Through Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851592f-9aba-478f-9fa0-af6386b5854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "work = works['\\cite{tabrizi2019eh}']\n",
    "print(getMetrics(work['tp'], work['tn'], work['fp'], work['fn']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
