{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e3a2b8",
   "metadata": {},
   "source": [
    "# Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f04caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\n",
    "https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right\n",
    "\n",
    "a figura com drvs justifica muito bem a escolha de 80% para teste\n",
    "\n",
    "\n",
    "fazer eval em todos dfs de 80%, adicionar epoch e indices etc para depois pegar caso necessario\n",
    "pegar melhor modelo olhando o loss e aplicar pra todos circuitos de teste fcn, ou fcn_large\n",
    "\n",
    "\n",
    "\n",
    "Figura:\n",
    "como alguem faria para usar o preditor\n",
    "como alguem faria para testar com outra tecnologia o preditor\n",
    "explicar que ele he independente de tecnologia\n",
    "fluxograma mostrando a parte do ORDF e a parte do INNOVUS para geracao dos benchamrks\n",
    "dps para geracao dos dados de treino e inferencia.\n",
    "Explicar o problema dos dados, não é só uma questão de armazenamento, mas a leitura deles\n",
    "tbm seria muito demorada inviabilizando o treino do modelo.\n",
    "justificar o porque de pegar soh short"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb56f37",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021c96c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import subprocess\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e65aaa-d80a-426f-bfb3-ee365dadf140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN0 = [[('bp_multi', 82), ('swerv', 84), ('bp_multi', 87), ('bp_multi', 86)],\n",
    "# FCN1 = [('aes', 88), ('bp_be', 77), ('bp_be', 76), ('bp_multi', 85)],\n",
    "# FCN2 = [('aes', 76), ('aes', 80), ('aes', 73), ('bp_multi', 90)],\n",
    "# FCN3 = [('aes', 87), ('bp_multi', 84), ('ibex', 90), ('aes', 89)],\n",
    "# FCN4 = [('tinyRocket', 90), ('swerv', 82), ('bp_be', 80), ('bp_be', 75)]]\n",
    "\n",
    "\n",
    "def getMetrics(tp, tn, fp, fn):\n",
    "  specificity = tn/(fp+tn) # True Negative Rate (TNR), is also known as Specificity or Selectivity.\n",
    "  sensitivity = tp/(tp+fn) # True Positive Rate (TPR), also known as Sensitivity, Recall, or the Hit Rate\n",
    "  prevalence = ((tp+fn)/(tp+fn+fp+tn))\n",
    "  accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "  # npv = tn/(fn+tn) # Negative Predictive Value (NPV)\n",
    "  # fpr = fp/(tn+fp) # False Positive Rate (FPR), also known as the Fall-Out or Type I Error Rate or False Alarm Rate\n",
    "  # fnr = fn / (fn + tp)# False Negative Rate (FNR), It's also known as the Miss Rate or Type II Error Rate.\n",
    "\n",
    "  sqrt = math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "  mcc = 0\n",
    "  if sqrt != 0:\n",
    "    mcc = ((tp * tn) - (fp * fn))/sqrt\n",
    "\n",
    "  precision = tp/(tp + fp)# Positive Predictive Value (PPV), also known as Precision.\n",
    "  beta=10\n",
    "  fbeta = ((1 + pow(beta, 2)) * precision * sensitivity) / (pow(beta, 2) * precision + sensitivity)\n",
    "  f1score = (2 * (precision * sensitivity)) / (precision + sensitivity)\n",
    "  return {'tp':tp, 'tn':tn, 'fp':fp, 'fn':fn,\n",
    "          'prevalence':prevalence,\n",
    "          'specificity':specificity,\n",
    "          'sensitivity':sensitivity,\n",
    "          'precision':precision,\n",
    "          'gmean':math.sqrt(sensitivity * specificity),\n",
    "          'f1score':f1score,\n",
    "          'fscore':fbeta,\n",
    "          'accuracy':accuracy,\n",
    "          'mcc':mcc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9fc59e",
   "metadata": {},
   "source": [
    "# Cross Validation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98856eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossValPath = '/home/sheiny/workspace/Predictor/results/FCNCV/'\n",
    "# crossValPath = '/home/sheiny/workspace/Predictor/results/FCNModel10CV/'\n",
    "pkls = [crossValPath+x for x in os.listdir(crossValPath) if ('.pkl' in x and 'CV' in x)]\n",
    "pkls.sort(key=lambda x:int(re.search(r\"\\d+(\\.\\d+)?\", x[x.rfind('/'):]).group(0)))\n",
    "\n",
    "cvDicts = []\n",
    "for group in range(len(pkls)):\n",
    "  df = pickle.load(open(pkls[group], 'rb'))\n",
    "  runs = {(row['Design'], row['Density']) for index, row in df.iterrows()}\n",
    "  for run in runs:\n",
    "    tempDf = df.loc[(df['Design'] == run[0]) & (df['Density'] == run[1])]\n",
    "    metrics = getMetrics(sum(tempDf['tp']), sum(tempDf['tn']), sum(tempDf['fp']), sum(tempDf['fn']))\n",
    "    cvDicts.append({'Group':str(int(group)),\n",
    "                    'Design':run[0]+' '+str(run[1]),\n",
    "                    'TP':metrics['tp'],\n",
    "                    'TN':metrics['tn'],\n",
    "                    'FP':metrics['fp'],\n",
    "                    'FN':metrics['fn'],\n",
    "                    'Prevalence %':metrics['prevalence']*100,\n",
    "                    'Specificity %':metrics['specificity']*100,\n",
    "                    'Sensitivity %':metrics['sensitivity']*100,\n",
    "                    'G-Mean %':math.sqrt(metrics['sensitivity'] * metrics['specificity'])*100,\n",
    "                    # 'f1-score':metrics['f1score'],\n",
    "                    'F$_\\beta$-score*':metrics['fscore'],\n",
    "                    # 'precision':metrics['precision'],\n",
    "                    'Accuracy %':metrics['accuracy']*100,\n",
    "                    'MCC [-1:1]':metrics['mcc']})\n",
    "cvDf = pd.DataFrame.from_dict(cvDicts)\n",
    "# cvDf.loc['mean'] = cvDf.mean(numeric_only=True)\n",
    "cvDf\n",
    "# cvDf.to_csv('results/CrossValidation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dafcd09-7db9-480d-bcad-5fe9f96b49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV cross-validation Confusion Matrix\n",
    "tp = int(sum(cvDf['TP']))\n",
    "tn = int(sum(cvDf['TN']))\n",
    "fp = int(sum(cvDf['FP']))\n",
    "fn = int(sum(cvDf['FN']))\n",
    "\n",
    "cm = np.array([[tn, fp],\n",
    "               [fn, tp]])\n",
    "\n",
    "# Define class labels (binary classification)\n",
    "class_labels = ['Negative', 'Positive']\n",
    "\n",
    "# Create a figure and axis for the plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "# Show all ticks and label them with their respective class names\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=class_labels, yticklabels=class_labels,\n",
    "       title='5-Fold Cross-Validation Confusion Matrix',\n",
    "       ylabel='True Label',\n",
    "       xlabel='Predicted Label')\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(im)\n",
    "\n",
    "# Annotate the confusion matrix cells with the respective counts\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "# plt.savefig('ConfusionMatrix.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d08714",
   "metadata": {},
   "source": [
    "# Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl = '/home/sheiny/workspace/Predictor/results/PredictionExperiment/predictRuns.pkl'\n",
    "df = pickle.load(open(pkl, 'rb'))\n",
    "\n",
    "circuitResultDict = []\n",
    "runs = {(row['Design'], row['Density']) for index, row in df.iterrows()}\n",
    "for run in runs:\n",
    "  tempDf = df.loc[(df['Design'] == run[0]) & (df['Density'] == run[1])]\n",
    "  metrics = getMetrics(sum(tempDf['tp']), sum(tempDf['tn']), sum(tempDf['fp']), sum(tempDf['fn']))\n",
    "  tpr = metrics['tp']/(metrics['tp']+metrics['fn'])\n",
    "\n",
    "  PosRatio = ((metrics['tp']+metrics['fn'])/(metrics['tp']+metrics['fn']+metrics['fp']+metrics['fn']))*100\n",
    "  circuitResultDict.append({'Design':run[0]+str(run[1]),\n",
    "                            'tp':str(metrics['tp'])+' ('+str(int(tpr*100))+'%)', 'tn':int(metrics['tn']),\n",
    "                            'fp':int(metrics['fp']), 'fn':int(metrics['fn']),\n",
    "                            'PosRatio%':PosRatio,\n",
    "                            'spc %':metrics['specificity']*100, 'acc %':metrics['accuracy']*100, 'MCC[-1:1]':metrics['mcc']})\n",
    "resultDf = pd.DataFrame.from_dict(circuitResultDict)\n",
    "resultDf.set_index('Design', inplace=True)\n",
    "resultDf.to_csv('results/predictionResult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e3747",
   "metadata": {},
   "source": [
    "# Plot DRVs and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e12dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "design = '/home/sheiny/workspace/data/aes88/'\n",
    "modelPath = 'results/fcn1/model_14.pkl'\n",
    "outPath = 'predictions/aes88_'\n",
    "\n",
    "def writePredictionFile(outFile, nodesToWrite):\n",
    "  with open(outFile, 'w') as fp:\n",
    "    for node in nodesToWrite:\n",
    "      fp.write(str(node)+'\\n')\n",
    "  fp.close()\n",
    "\n",
    "def predict(model, pkl):\n",
    "  testDf = pd.read_pickle(pkl, compression='zip')\n",
    "  labels = testDf.pop(testDf.columns.values[-1])\n",
    "  nodeIDs = testDf.pop(testDf.columns.values[0])#drop first column which contains the nodeIds\n",
    "  testHyperImages = np.array(testDf).reshape(len(testDf),22,33,33)\n",
    "  predictions = model.predict(testHyperImages)\n",
    "  return nodeIDs, predictions, labels\n",
    "\n",
    "model = pickle.load(open(modelPath, 'rb'))\n",
    "pkls = [design+x for x in os.listdir(design) if '.pkl' in x]\n",
    "pkls.sort(key = lambda x : int(x[x.rfind('_')+1:x.find('.')]))\n",
    "results = []\n",
    "for pkl in pkls:\n",
    "  results.append(predict(model, pkl))\n",
    "\n",
    "tp = []\n",
    "tn = []\n",
    "fp = []\n",
    "fn = []\n",
    "for x, y, z in results:\n",
    "  nodeIDs = [int(n) for n in x]\n",
    "  predictions = y > 0.5\n",
    "  for id, prediction, actual in zip(nodeIDs, predictions, z):\n",
    "    if prediction == actual and actual == True:\n",
    "      tp.append(id)\n",
    "    elif prediction == actual and actual == False:\n",
    "      tn.append(id)\n",
    "    elif prediction != actual and actual == True:\n",
    "      fn.append(id)\n",
    "    else:\n",
    "      fp.append(id)\n",
    "\n",
    "writePredictionFile(outPath+'tp.txt', tp)\n",
    "writePredictionFile(outPath+'tn.txt', tn)\n",
    "writePredictionFile(outPath+'fp.txt', fp)\n",
    "writePredictionFile(outPath+'fn.txt', fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33213b62",
   "metadata": {},
   "source": [
    "# Roc Curve for Aes88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3964b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = np.concatenate((results[0][1], results[1][1]))\n",
    "y_true = np.concatenate((results[0][2], results[1][2]))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"AUC-ROC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7217fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic(ROC) Curve for Aes 88')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "# plt.savefig('ROCAes88.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d418c",
   "metadata": {},
   "source": [
    "# Load BenchmarkInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79266d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath = '/home/sheiny/workspace/data/CSVS/'\n",
    "df = pd.read_pickle('benchmarkInfo/ufscbenchmark.pkl', compression='zip')\n",
    "df = df.loc[df['Design'] != 'bp']\n",
    "df = df.loc[df['Design'] != 'gcd']\n",
    "\n",
    "df = df.loc[df['FDRVTotal'] == 0]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "inexistingFiles = []\n",
    "for index in range(len(df)):\n",
    "  design = df['Design'][index]\n",
    "  density = df['Density'][index]\n",
    "  if os.path.exists(csvPath+design+'/cts_'+design+'_'+str(density)+'.pkl') == False:\n",
    "    inexistingFiles.append(index)\n",
    "df.drop(inexistingFiles, inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.sort_values('IDRVShort', ascending=False, inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c73043",
   "metadata": {},
   "source": [
    "# Benchmark Info Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Info\n",
    "df2 = df.loc[df['FDRVTotal'] == 0].copy()\n",
    "circuits = set(df2['Design'])\n",
    "benchmarkInfo = []\n",
    "for c in circuits:\n",
    "  dfCircuit = df2.loc[df['Design'] == c]\n",
    "  designCount = len(dfCircuit)\n",
    "  avgComponent = dfCircuit['COMPONENTS'].mean()\n",
    "  avgNets = dfCircuit['NETS'].mean()\n",
    "  avgPins = dfCircuit['PINS'].mean()\n",
    "  avgBlkgs = dfCircuit['BLOCKAGES'].mean()\n",
    "  benchmarkInfo.append({'Design':c, 'Design Count':int(designCount),\n",
    "                        'Components':int(avgComponent), 'Nets':int(avgNets),\n",
    "                        'Pins':int(avgPins), 'Blockages':int(avgBlkgs)})\n",
    "\n",
    "benchmarkDfInfo = pd.DataFrame.from_dict(benchmarkInfo)\n",
    "benchmarkDfInfo.sort_values('Components', ascending=False, inplace=True)\n",
    "benchmarkDfInfo.loc['mean'] = benchmarkDfInfo.mean(numeric_only=True)\n",
    "benchmarkDfInfo\n",
    "# benchmarkDfInfo.to_csv('benchmarkInfo/OpenCores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling result\n",
    "df2 = df.loc[df['FDRVTotal'] == 0].copy()\n",
    "circuits = set(df2['Design'])\n",
    "benchmarkInfo = []\n",
    "for c in circuits:\n",
    "  dfCircuit = df2.loc[df2['Design'] == c]\n",
    "  avgTotalGridSize = dfCircuit['TotalSizeGrid'].mean()\n",
    "  reduction = 100-(1000000/avgTotalGridSize)\n",
    "  avgPosSamples = dfCircuit['Positives'].mean()\n",
    "  stdPos = dfCircuit['Positives'].std()\n",
    "  afterPosRatio = (avgPosSamples/10000)*100\n",
    "  benchmarkInfo.append({'Design':c,\n",
    "                        'Original Grid Size':int(avgTotalGridSize),\n",
    "                        'New Size':'10k',\n",
    "                        'Size Reduction \\%':reduction,\n",
    "                        'Positive Samples (Std Dev)':str(int(avgPosSamples))+' ('+str(int(stdPos))+')',\n",
    "                        'Undersample Positive \\%':afterPosRatio})\n",
    "\n",
    "benchmarkDfInfo = pd.DataFrame.from_dict(benchmarkInfo)\n",
    "benchmarkDfInfo.sort_values('Original Grid Size', ascending=False, inplace=True)\n",
    "benchmarkDfInfo\n",
    "# benchmarkDfInfo.to_csv('benchmarkInfo/Undersampling.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c01767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot Routing Runtime Break Down\n",
    "numCircuits = 10\n",
    "dfRuntime = df[0:numCircuits].copy()\n",
    "designs = dfRuntime['Design']\n",
    "idx = ['B'+str(x+1) for x in range(numCircuits)]\n",
    "dfRuntime['RuntimeDR'] = dfRuntime['RuntimeIDR'] + dfRuntime['RuntimeFDR']\n",
    "dfRuntime['Benchmark'] = idx\n",
    "dfRuntime = dfRuntime.set_index('Benchmark')\n",
    "\n",
    "# dfRuntime['RuntimeGR'] = dfRuntime['RuntimeGR']/dfRuntime['RuntimeGR']\n",
    "# dfRuntime['RuntimeIDR'] = dfRuntime['RuntimeIDR']/dfRuntime['RuntimeGR']\n",
    "# dfRuntime['RuntimeFDR'] = dfRuntime['RuntimeFDR']/dfRuntime['RuntimeGR']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "dfRuntime[['RuntimeGR']].plot.bar(width=0.25, position=1, ax=ax, color='#9673A6')\n",
    "dfRuntime[['RuntimeIDR', 'RuntimeFDR']].plot.bar(width=0.25, position=0, stacked=True, ax=ax, color=['#6C8EBF','#82B366'])\n",
    "ax.set_ylabel(\"Runtime (seconds)\")\n",
    "ax.set_xlabel(\"Benchmarks\")\n",
    "\n",
    "# bars = ax.patches\n",
    "# patterns =('/', '*', 'O')\n",
    "# hatches = [p for p in patterns for i in range(numCircuits)]\n",
    "# for bar, hatch in zip(bars, hatches):\n",
    "#   bar.set_hatch(hatch)\n",
    "\n",
    "ax.set_title('Routing Runtime Breakdown')\n",
    "ax.legend(['Global Routing', 'Detailed Routing (first iter.)', 'Detailed Routing (remainder iters.)']);\n",
    "plt.show()\n",
    "# plt.savefig('RoutingRuntime.pdf')\n",
    "# plt.savefig('RoutingRuntime.png', dpi=500, bbox_inches=\"tight\")\n",
    "# dfRuntime[['Design', 'Density', 'IDRVShort']].to_csv('benchmarkInfo/RoutingRuntimeBreakdown.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361457d6-b5b9-49bb-9b7d-e8d0f95f6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtimes = {'Global Routing (avg)':2.1,\n",
    "#             'Global Routing (max)':4.3,\n",
    "#             'Detailed Routing (avg)':28.5,\n",
    "#             'Detailed Routing (max)':96.4}\n",
    "\n",
    "runtimesStacked = {'Global Routing (avg)':2.1,\n",
    "                   'Global Routing (max)':2.2,\n",
    "                   'Detailed Routing (avg)':28.5,\n",
    "                   'Detailed Routing (max)':67.9}\n",
    "runtimes = runtimesStacked\n",
    "runtimeDF = pd.DataFrame(runtimes, index=[0, 1])\n",
    "runtimeDF.iloc[0] = [runtimes['Global Routing (avg)'], runtimes['Global Routing (max)'], 0, 0]\n",
    "runtimeDF.iloc[1] = [0, 0, runtimes['Detailed Routing (avg)'], runtimes['Detailed Routing (max)']]\n",
    "\n",
    "\n",
    "ax = runtimeDF.plot.bar(stacked=True, color=['#e06666ff', '#6C8EBF', '#82B366', '#9673A6'])\n",
    "# ax.set_yscale('log')\n",
    "ax.set_ylabel(\"Runtime (hours)\")\n",
    "ax.set_xlabel(\"Routing Step\")\n",
    "ax.set_title('Routing Runtime of Industrial Designs')\n",
    "ax.set_xticklabels(['Global Routing', 'Detailed Routing'], rotation=0) \n",
    "\n",
    "bar_index = 7\n",
    "p = ax.patches[bar_index]\n",
    "ax.annotate('Almost 22.4 times bigger GR max', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='bottom', fontsize=10, color='black', xytext=(0, 75),\n",
    "                textcoords='offset points')\n",
    "plt.show()\n",
    "# plt.savefig('RoutingRuntimeIndustrial.pdf')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0882510a-0d01-4c3e-83c7-5e0822d4e1c1",
   "metadata": {},
   "source": [
    "# Bar plot Routing Runtime Break Down (BACKUP)\n",
    "numCircuits = 10\n",
    "dfRuntime = df[0:numCircuits].copy()\n",
    "designs = dfRuntime['Design']\n",
    "idx = ['B'+str(x) for x in range(len(dfRuntime))]\n",
    "print(len(index), len(dfRuntime))\n",
    "dfRuntime['Design'] = idx\n",
    "# dfRuntime = dfRuntime.set_index(index)\n",
    "dfRuntime = dfRuntime.set_index('Design')\n",
    "numIDRShorts = dfRuntime['IDRVShort']\n",
    "dfRuntime = dfRuntime[['RuntimeGR', 'RuntimeIDR', 'RuntimeFDR']]\n",
    "dfRuntime.set_index('Design')\n",
    "ax = dfRuntime.plot.bar(stacked=False)\n",
    "\n",
    "# numBars = int(len(ax.patches)/3)\n",
    "# for rect, value in zip(ax.patches[numBars:numBars*2], numIDRShorts):\n",
    "#   h = rect.get_height() /2.\n",
    "#   w = rect.get_width() /2.\n",
    "#   x, y = rect.get_xy()\n",
    "#   ax.text(x+w, y+h, value, horizontalalignment='center',verticalalignment='center')\n",
    "\n",
    "ax.set_ylabel('Runtime in seconds')\n",
    "ax.set_xlabel('Designs (80% row utilization)')\n",
    "ax.legend([\"Global Routing\", \"Initial Detailed Routing\", \"Complete Detailed Routing\"])\n",
    "plt.title('Routing Runtime Break Down')\n",
    "# plt.savefig('routing_runtime.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b688e1-2d8b-4544-8d6c-1c3fbb8daa2a",
   "metadata": {},
   "source": [
    "# Other Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg IDR short (considering full routed circuits only)\n",
    "idrShort = []\n",
    "for x in range(70, 91):\n",
    "  tempDf = df.loc[df['Design'].str.contains(str(x))].copy()\n",
    "  tempDf = tempDf.loc[tempDf['FDRTotal'] == 0]\n",
    "  avgIDRShort = sum(tempDf['IDRShort']/len(tempDf))\n",
    "  idrShort.append(avgIDRShort)\n",
    "    \n",
    "plt.plot([y for y in range(70, 91)], idrShort, color = 'r')\n",
    "plt.xlabel(\"Design Density (Row Utilization %)\")\n",
    "plt.ylabel(\"Initial Detailed Routing Short Violations (IDRV)\")\n",
    "plt.title('Average IDR Short x Row Utilization (Only fully routable circuits)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0532a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot avg IDR short (considering full routed circuits only)\n",
    "fdrRuntime = []\n",
    "for x in range(70, 91):\n",
    "  tempDf = df.loc[df['Design'].str.contains(str(x))].copy()\n",
    "  tempDf = tempDf.loc[tempDf['FDRTotal'] == 0]\n",
    "  fdr = sum(tempDf['FDR']/len(tempDf))\n",
    "  fdrRuntime.append(fdr)\n",
    "\n",
    "plt.plot([y for y in range(70, 91)], fdrRuntime, color = 'r')\n",
    "plt.xlabel(\"Design Density (Row Utilization %)\")\n",
    "plt.ylabel(\"Runtime (seconds)\")\n",
    "plt.title('Average Runtime to complete routing (Only fully routable circuits)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b8cde-0278-4554-b8d8-d581b2ddfb88",
   "metadata": {},
   "source": [
    "# Related Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95767987-dde6-47cf-82d3-5d3ad327ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "works = {#DRC Violation Prediction with Pre-global-routing Features Through Convolutional Neural Network\n",
    "         '\\cite{tabrizi2019eh}':{'tp':4343, 'tn':225688, 'fp':10990, 'fn':102},# Eh? predictor: A deep learning framework to identify detailed routing short violations from a placed netlist\n",
    "        }\n",
    "\n",
    "# Transforming Global Routing Report into DRC Violation Map with Convolutional Neural Network\n",
    "# DRC Violation Prediction with Pre-global-routing Features Through Convolutional Neural Network\n",
    "# DRC Violation Prediction After Global Route Through Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851592f-9aba-478f-9fa0-af6386b5854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "work = works['\\cite{tabrizi2019eh}']\n",
    "print(getMetrics(work['tp'], work['tn'], work['fp'], work['fn']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
