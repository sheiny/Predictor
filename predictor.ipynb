{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb56f37",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021c96c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea80aed",
   "metadata": {},
   "source": [
    "# Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ac440",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalMetrics = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "               tf.keras.metrics.FalsePositives(name='fp'),\n",
    "               tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "               tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "               tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "               tf.keras.metrics.Precision(name='precision'),\n",
    "               tf.keras.metrics.Recall(name='recall'),\n",
    "               tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "# FCN Model\n",
    "def makeFCNModel():\n",
    "  print('Making FCN model.')\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (22, 33, 33)))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((3, 3)))\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "  model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "                loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics = evalMetrics)\n",
    "  return model\n",
    "\n",
    "# FCN Model\n",
    "# This model don't work\n",
    "# def makeNewFCNModel():\n",
    "#   print('Making New FCN model.')\n",
    "#   model = tf.keras.Sequential()\n",
    "#   model.add(tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (22, 33, 33)))\n",
    "#   model.add(tf.keras.layers.MaxPooling2D((3, 3)))\n",
    "#   model.add(tf.keras.layers.Flatten())\n",
    "#   model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "#   model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "#   model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "#                 loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "#                 metrics = evalMetrics)\n",
    "#   return model\n",
    "\n",
    "# CNN Model\n",
    "def makeCNNModel():\n",
    "  print('Making CNN model.')\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (22, 33, 33)))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((3, 3)))\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(128, activation = 'relu'))#Dense\n",
    "  model.add(tf.keras.layers.Dense(128, activation = 'relu'))#Dense\n",
    "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "  model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "                loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics = evalMetrics)\n",
    "  return model\n",
    "\n",
    "def create_fcn_model():\n",
    "  # Convolutional layers\n",
    "  print('Making a ChatGPT model.')\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(22, 33, 33)))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))\n",
    "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))\n",
    "  model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "  # Fully convolutional layers\n",
    "  model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "  model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "  # Output layer for binary classification\n",
    "  model.add(tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid'))\n",
    "\n",
    "  # Flatten the output\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics = evalMetrics)\n",
    "  return model\n",
    "\n",
    "def create_cnn_model():\n",
    "  # Create a Sequential model\n",
    "  print('Making a ChatGPT CNN model.')\n",
    "  model = tf.keras.Sequential()\n",
    "\n",
    "  # Add convolutional layers\n",
    "  model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(22, 33, 33)))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "  model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "  # Flatten the output and add dense layers for classification\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics = evalMetrics)\n",
    "\n",
    "  # Print model summary\n",
    "  # model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd640e7e-a6ad-4ec5-9d97-9ddd33c03649",
   "metadata": {},
   "source": [
    "# Test Model with Dummy Imput\n",
    "\n",
    "# Create the model\n",
    "# model = create_fcn_model()\n",
    "model = makeNewFCNModel()\n",
    "# model = makeFCNModel()\n",
    "# model = pickle.load(open('model_test.pkl', 'rb'))\n",
    "\n",
    "input_shape = (22, 33, 33)\n",
    "\n",
    "# # Print the model summary\n",
    "# model.summary()\n",
    "\n",
    "# # Dummy data for demonstration purposes\n",
    "X_train = np.random.rand(100, *input_shape)\n",
    "y_train = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "# Train the model\n",
    "\n",
    "\n",
    "weights = {0: 1, 1: 1}\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=8, class_weight=weights)\n",
    "\n",
    "# pickle.dump(model, open('model_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9cc971",
   "metadata": {},
   "source": [
    "# Select Training Data (requires benchmark.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath = '/home/sheiny/workspace/data/CSVS/'\n",
    "df = pd.read_pickle('benchmarkInfo/ufscbenchmark.pkl', compression='zip')\n",
    "kFold = 4\n",
    "testSize = 20\n",
    "\n",
    "if (testSize % kFold != 0):\n",
    "  print('Warning: testSize % kFold != 0')\n",
    "\n",
    "df = df.loc[df['FDRVTotal'] == 0]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "inexistingFiles = []\n",
    "for index in range(len(df)):\n",
    "  design = df['Design'][index]\n",
    "  density = df['Density'][index]\n",
    "  if os.path.exists(csvPath+design+'/cts_'+design+'_'+str(density)+'.pkl') == False:\n",
    "    inexistingFiles.append(index)\n",
    "df.drop(inexistingFiles, inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.sort_values('IDRVShort', ascending=False, inplace=True, ignore_index=True)\n",
    "totalSize = len(df)\n",
    "testCircuits = [(df['Design'][x], df['Density'][x]) for x in range(testSize)]\n",
    "if os.path.exists('testCircuits.pkl') == False:\n",
    "  print('creating test set')\n",
    "  random.shuffle(testCircuits)\n",
    "  pickle.dump(testCircuits, open('testCircuits.pkl', 'wb'))\n",
    "else:\n",
    "  print('found existing test set, loading')\n",
    "  testCircuits = pickle.load(open('testCircuits.pkl', 'rb'))\n",
    "testRuns = [testCircuits[x:x+kFold] for x in range(len(testCircuits)) if x % kFold == 0]\n",
    "trainingCircuits = [(df['Design'][x], df['Density'][x]) for x in range(testSize, len(df))]\n",
    "if totalSize != (len(testCircuits) + len(trainingCircuits)):\n",
    "  print('Error: len(df) should be equals len(trainingCircuits) + len(testCircuits)')\n",
    "trainingPkls = ['/home/sheiny/workspace/data/CSVS/'+x[0]+'/cts_'+x[0]+'_'+str(x[1])+'.pkl' for x in trainingCircuits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e103ee-a5a7-4d0c-9b46-7a3f5c5bd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont consider data augmentation\n",
    "trainingPklsNoAugmentation = [x for x in trainingPkls if '70' in x]\n",
    "trainingPkls = trainingPklsNoAugmentation\n",
    "trainingPkls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e54192",
   "metadata": {},
   "source": [
    "# Compute Class Weights"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85ae1fdb",
   "metadata": {},
   "source": [
    "# allPkls = ['/home/sheiny/workspace/data/CSVS/'+df['Design'][x]+'/cts_'+df['Design'][x]+'_'+str(df['Density'][x])+'.pkl' for x in range(len(df))]\n",
    "# Find weights for classes\n",
    "pos = 0\n",
    "neg = 0\n",
    "for pkl in allPkls:\n",
    "  print('reading', pkl)\n",
    "  trainDf = pd.read_pickle(pkl, compression='zip')\n",
    "  labels = trainDf.pop(trainDf.columns.values[-1])\n",
    "  totalViol = sum(labels)\n",
    "  pos += totalViol\n",
    "  neg += (len(labels) - totalViol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d222179",
   "metadata": {},
   "source": [
    "# Train for K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 7661\n",
    "neg = 1782339\n",
    "total = pos+neg\n",
    "w0 = total/(2*neg)\n",
    "w1 = total/(2*pos)\n",
    "weights = {0: w0, 1: w1}\n",
    "sizeBatch = 64\n",
    "\n",
    "def train(pklsForTraining, learningModel, modelPath, epochStart, epochEnd, trainResultDF = pd.DataFrame()):\n",
    "  pkls = pklsForTraining.copy()\n",
    "  for epoch in range(epochStart, epochEnd):\n",
    "    random.shuffle(pkls)\n",
    "    for pkl in pkls:\n",
    "      trainDf = pd.read_pickle(pkl, compression='zip')\n",
    "      trainDf.reset_index(inplace=True, drop=True)\n",
    "      valDf = trainDf.sample(frac=0.2)\n",
    "      trainDf = trainDf.drop(valDf.index)\n",
    "        \n",
    "      # labels = trainDf.pop(trainDf.columns.values[-1])\n",
    "      labels = trainDf.pop(trainDf.columns.values[-1]).to_numpy()\n",
    "      labels = labels.reshape((len(labels), 1))\n",
    "\n",
    "      # valLabels = valDf.pop(valDf.columns.values[-1])\n",
    "      valLabels = valDf.pop(valDf.columns.values[-1]).to_numpy()\n",
    "      valLabels = valLabels.reshape((len(valLabels), 1))\n",
    "\n",
    "      trainDf.pop(trainDf.columns.values[0])#drop first column which contains the nodeIds\n",
    "      valDf.pop(valDf.columns.values[0])#drop first column which contains the nodeIds\n",
    "      trainHyperImages = np.array(trainDf).reshape(len(trainDf),22,33,33)\n",
    "      valHyperImages = np.array(valDf).reshape(len(valDf),22,33,33)\n",
    "\n",
    "      print('Epoch: ',epoch,' Training with:', pkl)\n",
    "      train_history = learningModel.fit(x=trainHyperImages,\n",
    "                                       y=labels,\n",
    "                                       verbose=2, #0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "                                       batch_size=sizeBatch,\n",
    "                                       validation_data=(valHyperImages, valLabels),\n",
    "                                       class_weight=weights)\n",
    "      historyDf = pd.DataFrame(train_history.history)\n",
    "      historyDf['epoch'] = epoch\n",
    "      historyDf['design'] = pkl[pkl.rfind('/')+5:pkl.find('.')]\n",
    "      trainResultDF = pd.concat([trainResultDF, historyDf])\n",
    "    pickle.dump(learningModel, open(modelPath+'model_'+str(epoch)+'.pkl', 'wb'))\n",
    "    pickle.dump(trainResultDF, open(modelPath+'trainResultDF.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631d1aa-8814-4da6-9a27-89823ae80788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date and time as a string\n",
    "date_time_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'results/start_training_time_fcn4NoAugment.txt'\n",
    "\n",
    "# Write the date and time to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(date_time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4847a8-1d07-4089-8cb3-e82e1717ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 15\n",
    "numTestRuns = len(testRuns)\n",
    "useFCN = True\n",
    "# useFCN = False\n",
    "modelName = 'fcnNoAugment'\n",
    "# modelName = 'fcn'\n",
    "# modelName = 'cnn'\n",
    "\n",
    "for run in range(numTestRuns):\n",
    "  run = 4\n",
    "  modelPath = 'results/'+modelName+str(run)+'/'\n",
    "  if os.path.exists(modelPath) == False:\n",
    "    os.mkdir(modelPath)\n",
    "\n",
    "  models = [x for x in os.listdir(modelPath)]\n",
    "  lastRunEpoch = 0\n",
    "  learningModel = None\n",
    "  trainResultDF = pd.DataFrame()\n",
    "  if len(models) > 0:\n",
    "    if 'trainResultDF.pkl' in models:\n",
    "      models.remove('trainResultDF.pkl')\n",
    "    models.sort(key = lambda x : int(x[x.find('_')+1:x.find('.')]))\n",
    "    lastModel = models[-1]\n",
    "    lastRunEpoch = int(lastModel[lastModel.find('_')+1:lastModel.find('.')])\n",
    "    learningModel = pickle.load(open(modelPath+'model_'+str(lastRunEpoch)+'.pkl', 'rb'))\n",
    "    lastRunEpoch += 1\n",
    "    if lastRunEpoch == numEpochs:\n",
    "      continue\n",
    "    trainResultDF = pickle.load(open(modelPath+'trainResultDF.pkl', 'rb'))\n",
    "  else:\n",
    "    learningModel = makeFCNModel() if useFCN else makeCNNModel()\n",
    "    # learningModel = makeFCNModel() if useFCN else makeCNNModel()\n",
    "    # learningModel = create_fcn_model() if useFCN else makeCNNModel() #FCN chat gpt\n",
    "    \n",
    "    \n",
    "  allPkls = trainingPkls.copy()\n",
    "  # print('len(allPkls)', len(allPkls))\n",
    "  # allPkls += ['/home/sheiny/workspace/data/CSVS/'+y[0]+'/cts_'+y[0]+'_'+str(y[1])+'.pkl'\n",
    "  #             for x in range(numTestRuns) if x != run\n",
    "  #             for y in testRuns[x]]\n",
    "\n",
    "  train(allPkls, learningModel, modelPath, lastRunEpoch, numEpochs, trainResultDF)\n",
    "  break #Only run (run number 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d19d8-7daf-418d-bb78-aa126a5b0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date and time as a string\n",
    "date_time_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'results/end_training_time_fcn4NoAugment.txt'\n",
    "\n",
    "# Write the date and time to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(date_time_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493c48c",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, pkl):\n",
    "  testDf = pd.read_pickle(pkl, compression='zip')\n",
    "  labels = testDf.pop(testDf.columns.values[-1])\n",
    "  testDf.pop(testDf.columns.values[0])#drop first column which contains the nodeIds\n",
    "  testHyperImages = np.array(testDf).reshape(len(testDf),22,33,33)\n",
    "  result = model.evaluate(testHyperImages, labels)\n",
    "  resultDict = {m:r for (m, r) in zip(model.metrics_names, result)}\n",
    "  return resultDict\n",
    "\n",
    "def predictPkls(design, density, modelPath, pkls):\n",
    "  model = pickle.load(open(modelPath, 'rb'))\n",
    "  results = []\n",
    "  for pkl in pkls:\n",
    "    result = predict(model, pkl)\n",
    "    result['Design'] = design\n",
    "    result['Density'] = density\n",
    "    results.append(result)\n",
    "  return results\n",
    "\n",
    "def getCircuitPkls(path, design, density):\n",
    "  pkls = [path+design+'/'+x for x in os.listdir(path+design+'/') if '_'+str(density)+'_' in x]\n",
    "  pkls.sort(key = lambda x : int(x[x.rfind('_')+1:x.find('.')]))\n",
    "  return pkls"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7221950",
   "metadata": {},
   "source": [
    "testPath = '/home/sheiny/workspace/data/WholeCSV/'\n",
    "modelPaths = '/home/sheiny/workspace/Predictor/results/'\n",
    "\n",
    "for testRun in range(len(testRuns)):\n",
    "  modelPath = modelPaths+'fcn'+str(testRun)+'/model_14.pkl'\n",
    "  resultDf = pd.DataFrame()\n",
    "  for design, density in testRuns[testRun]:\n",
    "    pklsToTest = getCircuitPkls(testPath, design, density)\n",
    "    resultDicts = predictPkls(design, density, modelPath, pklsToTest)\n",
    "    df = pd.DataFrame.from_dict(resultDicts)\n",
    "    resultDf = pd.concat([resultDf, df], axis=0, ignore_index=True)\n",
    "  pickle.dump(resultDf, open('results/fcn'+str(testRun)+'_CV.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946fe830",
   "metadata": {},
   "source": [
    "# TrainingPerformace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10433b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 here means that this was using first CV run, test = [('bp_multi', 82), ('swerv', 84), ('bp_multi', 87), ('bp_multi', 86)]\n",
    "testPath = '/home/sheiny/workspace/data/WholeCSV/'\n",
    "modelPaths = '/home/sheiny/workspace/Predictor/results/fcn4/'\n",
    "outputPath = '/home/sheiny/workspace/Predictor/results/trainingPerformance_fcn.pkl'\n",
    "# modelPaths = '/home/sheiny/workspace/Predictor/results/cnn0/'\n",
    "# outputPath = '/home/sheiny/workspace/Predictor/results/trainingPerformance_cnn0.pkl'\n",
    "\n",
    "testRuns = [('bp_multi', 82), ('swerv', 84), ('bp_multi', 87), ('bp_multi', 86)]\n",
    "\n",
    "models = [x for x in os.listdir(modelPaths) if 'model' in x]\n",
    "models.sort(key=lambda x: int(x[x.rfind('_')+1:x.find('.')]))\n",
    "\n",
    "resultDf = None\n",
    "if os.path.exists(outputPath) == False:\n",
    "  print('creating resultDf')\n",
    "  resultDf = pd.DataFrame()\n",
    "else:\n",
    "  print('loading existent resultDf')\n",
    "  resultDf = pickle.load(open(outputPath, 'rb'))\n",
    "\n",
    "for model in models:\n",
    "  if len(resultDf) != 0 and model[:model.find('.')] in set(resultDf['Model']):\n",
    "    print('skipping already infered', model)\n",
    "    continue\n",
    "  modelPath = modelPaths + model\n",
    "  for design, density in testRuns:\n",
    "    pklsToTest = getCircuitPkls(testPath, design, density)\n",
    "    resultDicts = predictPkls(design, density, modelPath, pklsToTest)\n",
    "    for result in resultDicts:\n",
    "      result['Model'] = model[:model.find('.')]\n",
    "    df = pd.DataFrame.from_dict(resultDicts)\n",
    "    resultDf = pd.concat([resultDf, df], axis=0, ignore_index=True)\n",
    "  pickle.dump(resultDf, open(outputPath, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b8526",
   "metadata": {},
   "source": [
    "# Trained Model Experiment"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83ddf7db",
   "metadata": {},
   "source": [
    "testPath = '/home/sheiny/workspace/data/WholeCSV/'\n",
    "modelPath = 'results/fcn2/model_14.pkl'\n",
    "predictRuns = [y\n",
    "               for x in testRuns\n",
    "               for y in x]\n",
    "\n",
    "resultDf = pd.DataFrame()\n",
    "for design, density in predictRuns:\n",
    "  pklsToTest = [testPath+design+'/'+x\n",
    "                for x in os.listdir(testPath+design+'/') if '_'+str(density)+'_'\n",
    "                in x]\n",
    "  pklsToTest.sort(key = lambda x : int(x[x.rfind('_')+1:x.find('.')]))\n",
    "  resultDicts = predictPkls(design, density, modelPath, pklsToTest)\n",
    "  df = pd.DataFrame.from_dict(resultDicts)\n",
    "  resultDf = pd.concat([resultDf, df], axis=0, ignore_index=True)\n",
    "pickle.dump(resultDf, open('results/predictRuns.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b88282",
   "metadata": {},
   "source": [
    "# Benchmark Info (benchmark.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2b46a-a630-49b5-80b1-b3d964013c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numDRVs(file):\n",
    "  shortViol = 0\n",
    "  totalViol = 0\n",
    "  for line in open(file, 'r').readlines():\n",
    "    if 'Metal Short' in line:\n",
    "      shortViol += 1\n",
    "    if 'Total Violations' in line:\n",
    "      totalViol = int(line.split(' ')[5])\n",
    "  return [shortViol, totalViol]\n",
    "\n",
    "def componentCount(lines):\n",
    "  numComponents = 0\n",
    "  startCounting = False\n",
    "  for line in lines:\n",
    "    tokens = line.split(' ')\n",
    "    if startCounting and 'END' == tokens[0]:\n",
    "      startCounting = False\n",
    "    if startCounting and '-' == tokens[0] and 'FILL' not in tokens[2]:\n",
    "      numComponents += 1\n",
    "    if tokens[0] == 'COMPONENTS':\n",
    "      startCounting = True\n",
    "  return numComponents\n",
    "\n",
    "def getDefInfo(file):\n",
    "  info = {x:0 for x in ['NETS', 'SPECIALNETS', 'PINS', 'BLOCKAGES']}\n",
    "  lines = open(file, 'r').readlines()\n",
    "  for line in lines:\n",
    "    tokens = line.split(' ')\n",
    "    if tokens[0] in info:\n",
    "      info[tokens[0]] = int(tokens[1])\n",
    "  info['COMPONENTS'] = componentCount(lines)\n",
    "  return info\n",
    "\n",
    "def getRuntime(file):\n",
    "  runtime = {x:-1 for x in ['RuntimeGR', 'RuntimeIDR', 'RuntimeFDR']}\n",
    "  val = []\n",
    "  for line in open(file,\"r\"):\n",
    "      val.append(int(line.split(' ')[0]))\n",
    "  runtime['RuntimeGR'] = val[0]\n",
    "  runtime['RuntimeIDR'] = val[1]\n",
    "  runtime['RuntimeFDR'] = val[2]\n",
    "  return runtime\n",
    "\n",
    "def getTotalGridSize(circuit, density):\n",
    "  #TemplateRunner.tcl contains the library read part\n",
    "  shutil.copyfile('TemplateRunner.tcl', 'run.tcl')\n",
    "  f = open('run.tcl', 'a')\n",
    "  f.write('read_def -continue_on_errors /home/sheiny/workspace/Predictor/OpenCoresUFSC/'+circuit+'/base/cts_'+circuit+'_'+str(density)+'FirstDR.def\\n')\n",
    "  f.write('ftx::initGraphFromDef 16\\n')\n",
    "  f.write('ftx::readRPT /home/sheiny/workspace/Predictor/OpenCoresUFSC/'+circuit+'/base/cts_'+circuit+'_'+str(density)+'FirstDR.rpt\\n')\n",
    "  f.close()\n",
    "  f = open(\"log.txt\", \"w\")\n",
    "  subprocess.call(['OpenCoresUFSC/openroad',\n",
    "                   'run.tcl',\n",
    "                   '-exit'], stdout=f)\n",
    "  lastLine = None\n",
    "  with open('log.txt', 'r') as f:\n",
    "    lastLine = f.readlines()[-1]\n",
    "    f.close()\n",
    "  os.remove('run.tcl')\n",
    "  os.remove('log.txt')\n",
    "  totalSize = int(lastLine.split()[6])\n",
    "  positives = int(lastLine.split()[4])\n",
    "  return totalSize, positives\n",
    "\n",
    "includeGridSize = False\n",
    "df = pd.DataFrame()\n",
    "benchmarkPath = '/home/sheiny/workspace/Benchmarks/OpenCoresUFSC/'\n",
    "# benchmarkPath = '/home/sheiny/workspace/Benchmarks/CMC/'\n",
    "outFile = 'benchmarkInfo/NEWufscbenchmark.pkl'\n",
    "# outFile = 'benchmarkInfo/cmcbenchmark.pkl'\n",
    "allCircuitsPaths = [benchmarkPath+circuit+'/base/' for circuit in os.listdir(benchmarkPath) if os.path.exists(benchmarkPath+circuit+'/base/')]\n",
    "allCircuitsPaths.sort()\n",
    "\n",
    "\n",
    "\n",
    "for circuitPath in allCircuitsPaths:\n",
    "  files = [x[x.find('_')+1:x.find('FinalDR.rpt')] for x in os.listdir(circuitPath) if 'FinalDR.rpt' in x]\n",
    "  files.sort()\n",
    "\n",
    "  for file in files:\n",
    "    circuit = file[:file.rfind('_')]\n",
    "    density = int(file[file.rfind('_')+1:])\n",
    "    print('extracting: ', circuit, density)\n",
    "    IDRShort, IDRTotal = numDRVs(circuitPath+'cts_'+file+'FirstDR.rpt')\n",
    "    FDRShort, FDRTotal = numDRVs(circuitPath+'cts_'+file+'FinalDR.rpt')\n",
    "    # print('IDRShort:', IDRShort, ' IDRTotal:',IDRTotal)\n",
    "    # print('FDRShort:', FDRShort, ' FDRTotal:',FDRTotal)\n",
    "    info = getDefInfo(circuitPath+'cts_'+file+'FinalDR.def')\n",
    "\n",
    "    totalSize = None\n",
    "    positives = None\n",
    "    if (includeGridSize):\n",
    "      totalSize, positives = getTotalGridSize(circuit, density)\n",
    "\n",
    "    drvs = {'IDRVShort':IDRShort, 'IDRVTotal':IDRTotal, 'FDRVShort':FDRShort, 'FDRVTotal':FDRTotal}\n",
    "    runtime = getRuntime(circuitPath+'cts_'+file+'Runtime.out')\n",
    "\n",
    "    dfVals = {'Design':circuit, 'Density':density}\n",
    "    if (includeGridSize):\n",
    "      dfVals.update({'TotalSizeGrid':totalSize, 'Positives':positives})\n",
    "    dfVals.update(info)\n",
    "    dfVals.update(drvs)\n",
    "    dfVals.update(runtime)\n",
    "    dfTemp = pd.DataFrame(dfVals, index=[0])\n",
    "    df = pd.concat([df, dfTemp], ignore_index=True)\n",
    "df\n",
    "df.to_pickle(outFile, compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea59f68-5096-408c-8985-b3c9f9cf8653",
   "metadata": {},
   "source": [
    "# Save Unrouted Circuits (FDRVTotal != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aced78-a745-4dfc-b579-1b5b45242d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('benchmarkInfo/NEWufscbenchmark.pkl', compression='zip')\n",
    "df = df[df['FDRVTotal'] != 0]\n",
    "df.to_csv('UnroutedCircuits.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90a21643",
   "metadata": {},
   "source": [
    "def numDRVs(file):\n",
    "  shortViol = 0\n",
    "  totalViol = 0\n",
    "  for line in open(file, 'r').readlines():\n",
    "    if 'Metal Short' in line:\n",
    "      shortViol += 1\n",
    "    if 'Total Violations' in line:\n",
    "      totalViol = int(line.split(' ')[5])\n",
    "  return [shortViol, totalViol]\n",
    "\n",
    "def componentCount(lines):\n",
    "  numComponents = 0\n",
    "  startCounting = False\n",
    "  for line in lines:\n",
    "    tokens = line.split(' ')\n",
    "    if startCounting and 'END' == tokens[0]:\n",
    "      startCounting = False\n",
    "    if startCounting and '-' == tokens[0] and 'FILL' not in tokens[2]:\n",
    "      numComponents += 1\n",
    "    if tokens[0] == 'COMPONENTS':\n",
    "      startCounting = True\n",
    "  return numComponents\n",
    "\n",
    "def getDefInfo(file):\n",
    "  info = {x:0 for x in ['NETS', 'SPECIALNETS', 'PINS', 'BLOCKAGES']}\n",
    "  lines = open(file, 'r').readlines()\n",
    "  for line in lines:\n",
    "    tokens = line.split(' ')\n",
    "    if tokens[0] in info:\n",
    "      info[tokens[0]] = int(tokens[1])\n",
    "  info['COMPONENTS'] = componentCount(lines)\n",
    "  return info\n",
    "\n",
    "def getRuntime(file):\n",
    "  runtime = {x:-1 for x in ['RuntimeGR', 'RuntimeIDR', 'RuntimeFDR']}\n",
    "  val = []\n",
    "  for line in open(file,\"r\"):\n",
    "      val.append(int(line.split(' ')[0]))\n",
    "  runtime['RuntimeGR'] = val[0]\n",
    "  runtime['RuntimeIDR'] = val[1]\n",
    "  runtime['RuntimeFDR'] = val[2]\n",
    "  return runtime\n",
    "\n",
    "def getTotalGridSize(circuit, density):\n",
    "  #TemplateRunner.tcl contains the library read part\n",
    "  shutil.copyfile('TemplateRunner.tcl', 'run.tcl')\n",
    "  f = open('run.tcl', 'a')\n",
    "  f.write('read_def -continue_on_errors /home/sheiny/workspace/Predictor/OpenCoresUFSC/'+circuit+'/base/cts_'+circuit+'_'+str(density)+'FirstDR.def\\n')\n",
    "  f.write('ftx::initGraphFromDef 16\\n')\n",
    "  f.write('ftx::readRPT /home/sheiny/workspace/Predictor/OpenCoresUFSC/'+circuit+'/base/cts_'+circuit+'_'+str(density)+'FirstDR.rpt\\n')\n",
    "  f.close()\n",
    "  f = open(\"log.txt\", \"w\")\n",
    "  subprocess.call(['OpenCoresUFSC/openroad',\n",
    "                   'run.tcl',\n",
    "                   '-exit'], stdout=f)\n",
    "  lastLine = None\n",
    "  with open('log.txt', 'r') as f:\n",
    "    lastLine = f.readlines()[-1]\n",
    "    f.close()\n",
    "  os.remove('run.tcl')\n",
    "  os.remove('log.txt')\n",
    "  totalSize = int(lastLine.split()[6])\n",
    "  positives = int(lastLine.split()[4])\n",
    "  return totalSize, positives\n",
    "\n",
    "df = pd.DataFrame()\n",
    "benchmarkPath = '/home/sheiny/workspace/Benchmarks/OpenCoresUFSC/'\n",
    "# benchmarkPath = '/home/sheiny/workspace/Benchmarks/CMC/'\n",
    "outFile = 'benchmarkInfo/ufscbenchmark.pkl'\n",
    "# outFile = 'benchmarkInfo/cmcbenchmark.pkl'\n",
    "allCircuitsPaths = [benchmarkPath+circuit+'/base/' for circuit in os.listdir(benchmarkPath) if os.path.exists(benchmarkPath+circuit+'/base/')]\n",
    "allCircuitsPaths.sort()\n",
    "\n",
    "for circuitPath in allCircuitsPaths:\n",
    "  files = [x[x.find('_')+1:x.find('FinalDR.rpt')] for x in os.listdir(circuitPath) if 'FinalDR.rpt' in x]\n",
    "  files.sort()\n",
    "  for file in files:\n",
    "    circuit = file[:file.rfind('_')]\n",
    "    density = int(file[file.rfind('_')+1:])\n",
    "    print('extracting: ', circuit, density)\n",
    "    IDRShort, IDRTotal = numDRVs(circuitPath+'cts_'+file+'FirstDR.rpt')\n",
    "    FDRShort, FDRTotal = numDRVs(circuitPath+'cts_'+file+'FinalDR.rpt')\n",
    "\n",
    "    info = getDefInfo(circuitPath+'cts_'+file+'FinalDR.def')\n",
    "    \n",
    "    totalSize, positives = getTotalGridSize(circuit, density)\n",
    "\n",
    "    drvs = {'IDRVShort':IDRShort, 'IDRVTotal':IDRTotal, 'FDRVShort':FDRShort, 'FDRVTotal':FDRTotal}\n",
    "    runtime = getRuntime(circuitPath+'cts_'+file+'Runtime.out')\n",
    "\n",
    "    dfVals = {'Design':circuit, 'Density':density,\n",
    "              'TotalSizeGrid':totalSize, 'Positives':positives}\n",
    "    dfVals.update(info)\n",
    "    dfVals.update(drvs)\n",
    "    dfVals.update(runtime)\n",
    "    dfTemp = pd.DataFrame(dfVals, index=[0])\n",
    "    df = pd.concat([df, dfTemp], ignore_index=True)\n",
    "df\n",
    "df.to_pickle(outFile, compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d619a4",
   "metadata": {},
   "source": [
    "# Compress and merge CSVs into .pkls"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c5f60aa",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# MULTIPLE CSVs per circuit (Train)\n",
    "# dirCSVS = '/home/sheiny/workspace/data/CSVS/'\n",
    "dirCSVS = '/home/sheiny/workspace/data/CMC/'\n",
    "allCircuits = [dirCSVS+x+'/' for x in os.listdir(dirCSVS)]\n",
    "allCircuits.sort()\n",
    "for circuit in allCircuits:\n",
    "  files = os.listdir(circuit)\n",
    "  files.sort()\n",
    "  for csv in files:\n",
    "    if 'viol' not in csv:\n",
    "      continue\n",
    "    if csv[0:csv.find('_viol')]+'.pkl' in files: #Already compressed\n",
    "      continue\n",
    "    if csv[0:csv.find('viol')]+'nonViol.csv' not in files:\n",
    "      continue\n",
    "    \n",
    "    print('Compressing ', csv[0:csv.find('_viol')])\n",
    "    dfViol = pd.read_csv(circuit+csv, dtype=np.float32, header=None)\n",
    "    dfSurround = pd.read_csv(circuit+csv[0:csv.find('viol')]+'surround.csv', dtype=np.float32, header=None)\n",
    "    dfNonViol = pd.read_csv(circuit+csv[0:csv.find('viol')]+'nonViol.csv', dtype=np.float32, header=None)\n",
    "    df = pd.concat([dfViol, dfSurround, dfNonViol], ignore_index=True)\n",
    "    df.to_pickle(circuit+csv[0:csv.find('_viol')]+'.pkl', compression='zip')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd93fa7e",
   "metadata": {},
   "source": [
    "# A SINGLE CSV per circuit (TEST)\n",
    "path = '/home/sheiny/workspace/data/WholeCSV/'\n",
    "allCircuits = [path+x+'/' for x in os.listdir(path)]\n",
    "allCircuits.sort()\n",
    "for circuit in allCircuits:\n",
    "  files = os.listdir(circuit)\n",
    "  files.sort()\n",
    "  for csv in files:\n",
    "    if '.csv' not in csv:\n",
    "      continue\n",
    "\n",
    "    i = 0\n",
    "    print('Compressing ', csv[0:csv.find('.')])\n",
    "    for df in pd.read_csv(circuit+csv, dtype=np.float32, header=None, chunksize=10000):\n",
    "      df.to_pickle(circuit+csv[0:csv.find('.')]+'_'+str(i)+'.pkl', compression='zip')\n",
    "      i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
