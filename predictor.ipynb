{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb56f37",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021c96c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea80aed",
   "metadata": {},
   "source": [
    "# Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ac440",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalMetrics = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "               tf.keras.metrics.FalsePositives(name='fp'),\n",
    "               tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "               tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "               tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "               tf.keras.metrics.Precision(name='precision'),\n",
    "               tf.keras.metrics.Recall(name='recall'),\n",
    "               tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "# FCN Model\n",
    "def makeFCNModel():\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (22, 33, 33)))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((3, 3)))\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "  model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "                loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics = evalMetrics)\n",
    "  return model\n",
    "\n",
    "# CNN Model\n",
    "def makeCNNModel():\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (22, 33, 33)))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((3, 3)))\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(128, activation = 'relu'))#Dense\n",
    "  model.add(tf.keras.layers.Dense(128, activation = 'relu'))#Dense\n",
    "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "  model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "                loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics = evalMetrics)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9cc971",
   "metadata": {},
   "source": [
    "# Select Training Data (requires benchmark.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('benchmarkInfo/benchmark.pkl', compression='zip')\n",
    "df = df.loc[df['FDRTotal'] == 0]# Get ONLY drv clean routed circuits, that have DRVs in their first DR itr.\n",
    "\n",
    "circuitsToTrain = []\n",
    "for x in df['Design']:\n",
    "  density = int(x[x.rfind('_')+1:])\n",
    "#   if density != 80 and density > 74 and density < 86: # density 80 is only for testing!\n",
    "  if density == 80: # density 80 is only for testing!\n",
    "    circuitsToTrain.append(x)\n",
    "#   if density == 75: # test data augmentation strategy\n",
    "#     circuitsToTrain.append(x)\n",
    "\n",
    "allPkls = ['/data/CSV/' + x[:x.rfind('_')] + '/cts_' + x + '.pkl' for x in circuitsToTrain]\n",
    "allPkls = [x for x in allPkls if os.path.exists(x)] #few cases where we don't the pkl, because first itr waas already drv clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e54192",
   "metadata": {},
   "source": [
    "# Traning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05751f09",
   "metadata": {},
   "source": [
    "# Find weights for classes\n",
    "pos = 0\n",
    "neg = 0\n",
    "for pkl in allPkls:\n",
    "  trainDf = pd.read_pickle(pkl, compression='zip')\n",
    "  labels = trainDf.pop(trainDf.columns.values[-1])\n",
    "  totalViol = sum(labels)\n",
    "  pos += totalViol\n",
    "  neg += (len(labels) - totalViol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 3327\n",
    "neg = 876673\n",
    "total = pos+neg\n",
    "w0 = total/(2*neg)\n",
    "w1 = total/(2*pos)\n",
    "weights = {0: w0, 1: w1}\n",
    "sizeBatch = 64\n",
    "\n",
    "def train(pklsForTraining, learningModel, modelPath, epochStart, epochEnd, trainResultDF = pd.DataFrame()):\n",
    "  pkls = pklsForTraining.copy()\n",
    "  for epoch in range(epochStart, epochEnd):\n",
    "    random.shuffle(pkls)\n",
    "    for pkl in pkls:\n",
    "      trainDf = pd.read_pickle(pkl, compression='zip')\n",
    "      trainDf = trainDf.reset_index(drop=True)\n",
    "      valDf = trainDf.sample(frac=0.2)\n",
    "      trainDf = trainDf.drop(valDf.index)\n",
    "\n",
    "      labels = trainDf.pop(trainDf.columns.values[-1])\n",
    "      valLabels = valDf.pop(valDf.columns.values[-1])\n",
    "      trainDf.pop(trainDf.columns.values[0])#drop first column which contains the nodeIds\n",
    "      valDf.pop(valDf.columns.values[0])#drop first column which contains the nodeIds\n",
    "      trainHyperImages = np.array(trainDf).reshape(len(trainDf),22,33,33)\n",
    "      valHyperImages = np.array(valDf).reshape(len(valDf),22,33,33)\n",
    "      print('Epoch: ',epoch,' Training with:', pkl)\n",
    "      train_history = learningModel.fit(x=trainHyperImages,\n",
    "                                       y=labels,\n",
    "                                       verbose=2, #0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "                                       batch_size=sizeBatch,\n",
    "                                       validation_data=(valHyperImages, valLabels),\n",
    "                                       class_weight=weights)\n",
    "      historyDf = pd.DataFrame(train_history.history)\n",
    "      historyDf['epoch'] = epoch\n",
    "      historyDf['design'] = pkl[pkl.rfind('/')+5:pkl.find('.')]\n",
    "      trainResultDF = pd.concat([trainResultDF, historyDf])\n",
    "    pickle.dump(learningModel, open(modelPath+'model_'+str(epoch)+'.pkl', 'wb'))\n",
    "    pickle.dump(trainResultDF, open(modelPath+'trainResultDF.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad78d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = 'results/cnn/'\n",
    "useFCN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e109432",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = 'results/fcnNoOversampling/'\n",
    "useFCN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4764a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 50\n",
    "\n",
    "if os.path.exists(modelPath) == False:\n",
    "  os.mkdir(modelPath)\n",
    "\n",
    "models = [x for x in os.listdir(modelPath)]\n",
    "lastRunEpoch = 0\n",
    "learningModel = None\n",
    "trainResultDF = pd.DataFrame()\n",
    "if len(models) > 0:\n",
    "  if 'trainResultDF.pkl' in models:\n",
    "    models.remove('trainResultDF.pkl')\n",
    "  models.sort(key = lambda x : int(x[x.find('_')+1:x.find('.')]))\n",
    "  lastModel = models[-1]\n",
    "  lastRunEpoch = int(lastModel[lastModel.find('_')+1:lastModel.find('.')])\n",
    "  learningModel = pickle.load(open(modelPath+'model_'+str(lastRunEpoch)+'.pkl', 'rb'))\n",
    "  lastRunEpoch += 1\n",
    "  trainResultDF = pickle.load(open(modelPath+'trainResultDF.pkl', 'rb'))\n",
    "else:\n",
    "  learningModel = makeFCNModel() if useFCN else makeCNNModel()\n",
    "\n",
    "train(allPkls, learningModel, modelPath, lastRunEpoch, numEpochs, trainResultDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493c48c",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, pkl):\n",
    "  testDf = pd.read_pickle(pkl, compression='zip')\n",
    "  labels = testDf.pop(testDf.columns.values[-1])\n",
    "  testDf.pop(testDf.columns.values[0])#drop first column which contains the nodeIds\n",
    "  testHyperImages = np.array(testDf).reshape(len(testDf),22,33,33)\n",
    "  result = model.evaluate(testHyperImages, labels)\n",
    "  resultDict = {m:r for (m, r) in zip(model.metrics_names, result)}\n",
    "  return resultDict\n",
    "\n",
    "\n",
    "# modelsToCrossValidate = ['results/fcn/model_0.pkl', 'results/fcn/model_15.pkl', 'results/fcn/model_25.pkl']\n",
    "modelsToCrossValidate = ['results/fcnNoOversampling/model_18.pkl']\n",
    "circuitsToTest = ['/data/CSVWhole/'+x+'/' for x in os.listdir('/data/CSVWhole/')]\n",
    "\n",
    "for modelPath in modelsToCrossValidate:\n",
    "  model = pickle.load(open(modelPath, 'rb'))\n",
    "#   outputFile = 'results/cv/fcnCV'+modelPath[modelPath.find('_')+1:modelPath.find('.')]+'.pkl'\n",
    "  outputFile = 'results/cv/fcnNoOversampling.pkl'\n",
    "  resultDF = pd.DataFrame()\n",
    "  for circuit in circuitsToTest:\n",
    "    pkls = [x for x in os.listdir(circuit) if '.pkl' in x]\n",
    "    pkls.sort(key = lambda x : int(x[x.rfind('_')+1:x.find('.')]))\n",
    "    itrDf = pd.DataFrame()\n",
    "    for pkl in pkls:\n",
    "      resultDict = predict(model, circuit+pkl)\n",
    "      resultDf = pd.DataFrame(resultDict, index=[0])\n",
    "      itrDf = pd.concat([itrDf, resultDf], ignore_index=True)\n",
    "    itrDf = itrDf.sum(axis=0)\n",
    "    itrDf['Design'] = pkls[0][pkls[0].find('_')+1:pkls[0].rfind('_')]\n",
    "    itrDf = pd.DataFrame(itrDf).T\n",
    "    resultDF = pd.concat([resultDF, itrDf], ignore_index=True)\n",
    "  pickle.dump(resultDF, open(outputFile, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b88282",
   "metadata": {},
   "source": [
    "# Benchmark Info (benchmark.pkl)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60102dcb",
   "metadata": {},
   "source": [
    "def numDRVs(file):\n",
    "  shortViol = 0\n",
    "  totalViol = 0\n",
    "  for line in open(file, 'r').readlines():\n",
    "    if 'Metal Short' in line:\n",
    "      shortViol += 1\n",
    "    if 'Total Violations' in line:\n",
    "      totalViol = int(line.split(' ')[5])\n",
    "  return [shortViol, totalViol]\n",
    "\n",
    "def componentCount(lines):\n",
    "  numComponents = 0\n",
    "  startCounting = False\n",
    "  for line in lines:\n",
    "    tokens = line.split(' ')\n",
    "    if startCounting and 'END' == tokens[0]:\n",
    "      startCounting = False\n",
    "    if startCounting and '-' == tokens[0] and 'FILL' not in tokens[2]:\n",
    "      numComponents += 1\n",
    "    if tokens[0] == 'COMPONENTS':\n",
    "      startCounting = True\n",
    "  return numComponents\n",
    "\n",
    "def getDefInfo(file):\n",
    "  info = {x:0 for x in ['NETS', 'SPECIALNETS', 'PINS', 'BLOCKAGES']}\n",
    "  lines = open(file, 'r').readlines()\n",
    "  for line in lines:\n",
    "    tokens = line.split(' ')\n",
    "    if tokens[0] in info:\n",
    "      info[tokens[0]] = int(tokens[1])\n",
    "  info['COMPONENTS'] = componentCount(lines)\n",
    "  return info\n",
    "\n",
    "def getRuntime(file):\n",
    "  runtime = {x:-1 for x in ['RuntimeGR', 'RuntimeIDR', 'RuntimeFDR']}\n",
    "  val = []\n",
    "  for line in open(file,\"r\"):\n",
    "      val.append(int(line.split(' ')[0]))\n",
    "  runtime['RuntimeGR'] = val[0]\n",
    "  runtime['RuntimeIDR'] = val[1]\n",
    "  runtime['RuntimeFDR'] = val[2]\n",
    "  return runtime\n",
    "\n",
    "df = pd.DataFrame()\n",
    "# benchmarkPath = '/data/CMC/'\n",
    "benchmarkPath = '/data/RoutedOpenCores/'\n",
    "allCircuitsPaths = [benchmarkPath+circuit+'/base/' for circuit in os.listdir(benchmarkPath) if os.path.exists(benchmarkPath+circuit+'/base/')]\n",
    "allCircuitsPaths.sort()\n",
    "\n",
    "for circuitPath in allCircuitsPaths:\n",
    "  files = [x[x.find('_')+1:x.find('Runtime')] for x in os.listdir(circuitPath) if 'Runtime' in x]\n",
    "  files.sort()\n",
    "  for file in files:\n",
    "    circuit = file[:file.rfind('_')]\n",
    "    density = int(file[file.rfind('_')+1:])\n",
    "    print('extracting: ', circuit, density)\n",
    "    IDRShort, IDRTotal = numDRVs(circuitPath+'cts_'+file+'FirstDR.rpt')\n",
    "    FDRShort, FDRTotal = numDRVs(circuitPath+'cts_'+file+'FinalDR.rpt')\n",
    "\n",
    "    info = getDefInfo(circuitPath+'cts_'+file+'FinalDR.def')\n",
    "\n",
    "    drvs = {'IDRVShort':IDRShort, 'IDRVTotal':IDRTotal, 'FDRVShort':FDRShort, 'FDRVTotal':FDRTotal}\n",
    "    runtime = getRuntime(circuitPath+'cts_'+file+'Runtime.out')\n",
    "\n",
    "    dfVals = {'Design':circuit, 'Density':density}\n",
    "    dfVals.update(info)\n",
    "    dfVals.update(drvs)\n",
    "    dfVals.update(runtime)\n",
    "    dfTemp = pd.DataFrame(dfVals, index=[0])\n",
    "    df = pd.concat([df, dfTemp], ignore_index=True)\n",
    "df\n",
    "df.to_pickle('benchmarkInfo/CellFixbenchmark.pkl', compression='zip')\n",
    "\n",
    "# Save a CSV file for each circuit\n",
    "# designs = set(df['Design'])\n",
    "# for design in designs:\n",
    "#   df.loc[df['Design'] == design].to_csv('benchmarkInfo/circuits/'+design+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d619a4",
   "metadata": {},
   "source": [
    "# Compress and merge CSVs into .pkls"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d987fe0c",
   "metadata": {},
   "source": [
    "# MULTIPLE CSVs per circuit (Train)\n",
    "allCircuits = ['/data/CSV/'+x+'/' for x in os.listdir('/data/CSV/')]\n",
    "allCircuits.sort()\n",
    "for circuit in allCircuits:\n",
    "  files = os.listdir(circuit)\n",
    "  files.sort()\n",
    "  for csv in files:\n",
    "    if 'viol' not in csv:\n",
    "      continue\n",
    "    if csv[0:csv.find('_viol')]+'.pkl' in files: #Already compressed\n",
    "      continue\n",
    "    print('Compressing ', csv[0:csv.find('_viol')])\n",
    "    dfViol = pd.read_csv(circuit+csv, dtype=np.float32, header=None)\n",
    "    dfSurround = pd.read_csv(circuit+csv[0:csv.find('viol')]+'surround.csv', dtype=np.float32, header=None)\n",
    "    dfNonViol = pd.read_csv(circuit+csv[0:csv.find('viol')]+'nonViol.csv', dtype=np.float32, header=None)\n",
    "    df = pd.concat([dfViol, dfSurround, dfNonViol], ignore_index=True)\n",
    "    df.to_pickle(circuit+csv[0:csv.find('_viol')]+'.pkl', compression='zip')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ce26b2d",
   "metadata": {},
   "source": [
    "# A SINGLE CSV per circuit (TEST)\n",
    "allCircuits = ['/data/CSVWhole/'+x+'/' for x in os.listdir('/data/CSVWhole/')]\n",
    "allCircuits.sort()\n",
    "for circuit in allCircuits:\n",
    "  files = os.listdir(circuit)\n",
    "  files.sort()\n",
    "  for csv in files:\n",
    "    if '.csv' not in csv:\n",
    "      continue\n",
    "\n",
    "    i = 0\n",
    "    print('Compressing ', csv[0:csv.find('.')])\n",
    "    for df in pd.read_csv(circuit+csv, dtype=np.float32, header=None, chunksize=10000):\n",
    "      df.to_pickle(circuit+csv[0:csv.find('.')]+'_'+str(i)+'.pkl', compression='zip')\n",
    "      i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
