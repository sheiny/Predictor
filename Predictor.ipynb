{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import math\n",
    "import seaborn\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed.\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234) # Scikit Learn does not have its own global random state but uses the numpy random state instead.\n",
    "batch_size = 256 # is important to ensure that each batch has a decent chance of containing a few positive samples\n",
    "epochs = 10\n",
    "learning_rate = 0.005\n",
    "beta = 0.001 #regularization\n",
    "drop_out = 0.05\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = matplotlib.pyplot.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: If you want to deploy a model, it's critical that you preserve the preprocessing calculations.\n",
    "# The easiest way to implement them as layers, and attach them to your model before export.\n",
    "\n",
    "# Naming convention: Circuit Name + ABU + DR iteration + csv\n",
    "# Note: ABU zero means that the placement wasn't changed from contest benchmark.\n",
    "# csvs = [\"ispd19_test6.0.0.csv\", \"ispd19_test6.0.95.0.csv\", \"ispd19_test6.1.0.csv\", \"ispd19_test6.0.1.csv\", \"ispd19_test6.0.95.1.csv\", \"ispd19_test6.1.1.csv\"]\n",
    "csvs = [\"ispd19_test6.0.1.csv\", \"ispd19_test6.0.95.1.csv\", \"ispd19_test6.1.1.csv\"]\n",
    "path = \"data/ICCAD2019/\"\n",
    "\n",
    "dataframes = [pd.read_csv(path+file_name) for file_name in csvs]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('HasDetailedRoutingViolation'))\n",
    "val_labels = np.array(val_df.pop('HasDetailedRoutingViolation'))\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "\n",
    "# Scale\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "val_features = scaler.transform(val_features)\n",
    "train_features = np.clip(train_features, -5, 5)\n",
    "val_features = np.clip(val_features, -5, 5)\n",
    "\n",
    "# Claculate weight for classes\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "neg, pos = np.bincount(df['HasDetailedRoutingViolation'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "weight_for_0 = (1 / neg)*(total)/2.0\n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load EhPredictor's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ISDP14/EhPredictorISPD14.csv\")\n",
    "\n",
    "# drop l53 because is always zero\n",
    "df.pop('l53')\n",
    "df.pop('normal')\n",
    "\n",
    "# Instead of having the number of shorts, use them as a boolean\n",
    "df.loc[df['short'] > 0, 'short'] = 1\n",
    "\n",
    "# Convert to log-space. l9 l43 l45 l52 l51\n",
    "log_cols = ['l9', 'l43', 'l45', 'l52', 'l51']\n",
    "eps=0.001 # 0 => 0.1Â¢\n",
    "for col in log_cols:\n",
    "    df[col] = np.log(df[col] + eps)\n",
    "\n",
    "# CSV organization:\n",
    "# des_perf_1_dataset=all_dataset[0:5476,:]\n",
    "# des_perf_a_dataset=all_dataset[5476:16928,:]\n",
    "# des_perf_b_dataset=all_dataset[16928:26928,:]\n",
    "# fft_1_dataset=all_dataset[26928:28864,:]\n",
    "# fft_2_dataset=all_dataset[28864:32113,:]\n",
    "# fft_a_dataset=all_dataset[32113:38604,:]\n",
    "# fft_b_dataset=all_dataset[38604:44375,:]\n",
    "# matrix_mult_1_dataset=all_dataset[44375:52656,:]\n",
    "# matrix_mult_a_dataset=all_dataset[52656:69168,:]\n",
    "# matrix_mult_b_dataset=all_dataset[69168:90601,:]\n",
    "# pci_bridge32_a_dataset=all_dataset[90601:94170,:]\n",
    "# pci_bridge32_b_dataset=all_dataset[94170:103961,:]\n",
    "# superblue11_a_dataset=all_dataset[103961:175113,:]\n",
    "# superblue12_dataset=all_dataset[175113:241123,:]\n",
    "\n",
    "# Test circuits: mgc fft_2\n",
    "test_df = df.iloc[28864:32113]\n",
    "df2 = df[0:28864]\n",
    "df3 = df[32113:]\n",
    "df = pd.concat([df2, df3])\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('short'))\n",
    "val_labels = np.array(val_df.pop('short'))\n",
    "test_labels = np.array(test_df.pop('short'))\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)\n",
    "\n",
    "# Scale\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "train_features = np.clip(train_features, -5, 5)\n",
    "val_features = np.clip(val_features, -5, 5)\n",
    "test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "# Claculate weight for classes\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "neg, pos = np.bincount(df['short'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "           tf.keras.metrics.FalsePositives(name='fp'),\n",
    "           tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "           tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "           tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "           tf.keras.metrics.Precision(name='precision'),\n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "def make_model(metrics = METRICS, output_bias=None, lr=learning_rate):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(20,\n",
    "                                                       activation='relu', # Relu throw away negative values\n",
    "                                                       kernel_regularizer=tf.keras.regularizers.l2(beta)),\n",
    "                                 tf.keras.layers.Dropout(drop_out),\n",
    "                                 tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=metrics)\n",
    "    return model\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "train_dataset = dataset.shuffle(len(train_features)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_bias = np.log([pos/neg])\n",
    "# model = make_model(output_bias = initial_bias)\n",
    "# model.load_weights('model_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initial_bias = np.log([pos/neg])\n",
    "\n",
    "model = make_model(output_bias = initial_bias)\n",
    "train_history = model.fit(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(val_features, val_labels),\n",
    "                          class_weight=class_weight,\n",
    "                          epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('model_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = sklearn.metrics.confusion_matrix(labels, predictions > p)\n",
    "    matplotlib.pyplot.figure(figsize=(5,5))\n",
    "    seaborn.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    matplotlib.pyplot.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    matplotlib.pyplot.ylabel('Actual label')\n",
    "    matplotlib.pyplot.xlabel('Predicted label')\n",
    "    matplotlib.pyplot.show()\n",
    "    \n",
    "def calculate_metrics(model, results):\n",
    "    m = {}\n",
    "    for name, value in zip(model.metrics_names, results):\n",
    "        m[name] = value\n",
    "    if(m['precision'] + m['recall'] != 0):\n",
    "        f_score = (2 * m['precision'] * m['recall'])/(m['precision'] + m['recall'])\n",
    "        m['F-score'] = f_score\n",
    "    sqrt = math.sqrt((m['tp']+m['fp'])*(m['tp']+m['fn'])*(m['tn']+m['fp'])*(m['tn']+m['fn']))\n",
    "    if sqrt != 0:\n",
    "        mcc = (m['tp'] * m['tn'] - m['fp'] * m['fn'])/sqrt\n",
    "        m['MCC'] = mcc\n",
    "    return m\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    for x, y in metrics.items():\n",
    "        print(x,':', round(y, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(train_features, train_labels, batch_size=batch_size, verbose=0)\n",
    "metrics = calculate_metrics(model, baseline_results)\n",
    "print_metrics(metrics)\n",
    "train_predictions_baseline = model.predict(train_features, batch_size=batch_size)\n",
    "plot_cm(train_labels, train_predictions_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performace on ISPD14-mgc_fft_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(test_features, test_labels, batch_size=batch_size, verbose=0)\n",
    "metrics = calculate_metrics(model, baseline_results)\n",
    "print_metrics(metrics)\n",
    "test_predictions_baseline = model.predict(test_features, batch_size=batch_size)\n",
    "plot_cm(test_labels, test_predictions_baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
