{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Python\n",
    "    + Make sure to remove completely overlapped samples.\n",
    "    + Add padding in check_node_neighbors\n",
    "\n",
    "Warning:\n",
    "All area features were normalized, except AreaTile.\n",
    "\n",
    "tip: try to plot a histogram to check for the outliers before the normalization.\n",
    "\n",
    "* Readings\n",
    "    + Good reading about Standardization: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "    + Standardization vs normalization: https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf\n",
    "    + Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "    + https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "    + https://machinelearningmastery.com/evaluate-skill-deep-learning-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed.\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234) # Scikit Learn does not have its own global random state but uses the numpy random state instead.\n",
    "\n",
    "# CNN\n",
    "JSON_features = [\"NumCells\", \"NumCellPins\", \"NumMacros\", \"NumMacroPins\",\n",
    "                 \"NumPassingNets\", \"AreaCells\", \"AreaL1Pin\", \"AreaL2Pin\",\n",
    "                 \"AreaL1Blkg\", \"AreaL2Blkg\", \"AreaMacroPins\", \"AreaMacros\",\n",
    "                 \"AreaTile\"]\n",
    "#Window Width and Height will be equal to 2 * window_radius + 1\n",
    "#WARNING: The window_radius must be greater than zero\n",
    "#For instance if window_radius = 7 the window size will be 15x15\n",
    "window_radius = 7\n",
    "window_size = 2*window_radius+1\n",
    "# batch_size = 2048\n",
    "# epochs = 300\n",
    "# learning_rate = 0.0045 # decayed every two epochs using an exponential rate of 0.94;\n",
    "\n",
    "batch_size = 256 # is important to ensure that each batch has a decent chance of containing a few positive samples\n",
    "epochs = 10\n",
    "learning_rate = 0.005\n",
    "beta = 0.001 #regularization\n",
    "drop_out = 0.05\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = matplotlib.pyplot.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "METRICS = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "           tf.keras.metrics.FalsePositives(name='fp'),\n",
    "           tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "           tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "           tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "           tf.keras.metrics.Precision(name='precision'),\n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features will be rescaled so that they’ll have the properties of a standard normal distribution.\n",
    "# mean (μ) = 0\n",
    "# standard deviation (σ) = 1\n",
    "def standardize(train_array, val_array, test_array=None):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    train_array = scaler.fit_transform(train_array)\n",
    "    val_array = scaler.transform(val_array)\n",
    "    if test_array is not None:\n",
    "        test_array = scaler.transform(test_array)\n",
    "        return train_array, val_array, test_array\n",
    "    return train_array, val_array\n",
    "\n",
    "# Claculate weight for classes\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "def calculate_class_weights(df, classification_label):\n",
    "    neg, pos = np.bincount(df[classification_label])\n",
    "    total = neg + pos\n",
    "    print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    return class_weight, neg, pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph functions using NetworkX library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_graph_build(json_file_path):\n",
    "    graph = nx.Graph()\n",
    "    nodes_jason = open(json_file_path)\n",
    "    data = json.load(nodes_jason)\n",
    "    nodes_jason.close()\n",
    "    for j_node in data:\n",
    "        node_id = j_node[\"id\"]\n",
    "        graph.add_node(node_id)\n",
    "        node = graph.nodes[node_id]\n",
    "        for feature in JSON_features:\n",
    "            node[feature] = j_node[feature]\n",
    "            node[\"HasDetailedRoutingViolation\"] = j_node[\"HasDetailedRoutingViolation\"]\n",
    "    for j_node in data:\n",
    "        node_id = j_node[\"id\"]\n",
    "        node = graph.nodes[node_id]\n",
    "        for neighbor in [\"UpNode\", \"DownNode\", \"LeftNode\", \"RightNode\"]:\n",
    "            if j_node[neighbor] != -1:\n",
    "                graph.add_edge(node_id, j_node[neighbor])\n",
    "            node[neighbor] = j_node[neighbor] if j_node[neighbor] != -1 else -1\n",
    "    return graph\n",
    "\n",
    "# simple sanity check function for a grid graph structure\n",
    "def check_json_grid_graph(json_file_path):\n",
    "    graph = grid_graph_build(json_file_path)\n",
    "    nodes_jason = open(json_file_path)\n",
    "    data = json.load(nodes_jason)\n",
    "    nodes_jason.close()\n",
    "    corners = 0\n",
    "    for j_node in data:\n",
    "        node_id = j_node[\"id\"]\n",
    "        node = graph.nodes[node_id]\n",
    "        if graph.degree[node_id] == 0:\n",
    "            print(\"WARNING! disconnected node found!\")\n",
    "        elif graph.degree[node_id] == 2:\n",
    "            corners += 1\n",
    "        elif graph.degree[node_id] != 3 and graph.degree[node_id] != 4:\n",
    "            print(\"WARNING! strange node degree found: \", graph.degree[node_id])\n",
    "        for feature in JSON_features:\n",
    "            if node[feature] < 0:\n",
    "                print('WARNING! negative feature detected!')\n",
    "    if corners != 4:\n",
    "        print(\"WARNING! corners found: \", corners, \" it should be 4!\")\n",
    "\n",
    "def graph_to_df(graph):\n",
    "    return pd.DataFrame([graph.nodes[x] for x in graph.nodes])\n",
    "\n",
    "# Do not forget to skip completely overlaped nodes.\n",
    "def check_node_neighbors(graph, node, window_radius):\n",
    "    for neighbor in [\"UpNode\", \"DownNode\", \"LeftNode\", \"RightNode\"]:\n",
    "        current_node = node\n",
    "        for x in range(window_radius):\n",
    "            if current_node[neighbor] == -1:\n",
    "                return False\n",
    "            current_node = graph.nodes[current_node[neighbor]]\n",
    "    return True\n",
    "\n",
    "def get_window_elements(graph, node, window_radius):\n",
    "    max_neighbor_hops = window_size-1\n",
    "    window_elements = []\n",
    "    current_node = node\n",
    "    for y in range(window_radius):\n",
    "        current_node = graph.nodes[current_node[\"UpNode\"]]\n",
    "    for x in range(window_radius):\n",
    "        current_node = graph.nodes[current_node[\"LeftNode\"]]\n",
    "    leftmost_node = current_node\n",
    "    for y in range(window_size):\n",
    "        row = []\n",
    "        current_node = leftmost_node\n",
    "        for x in range(window_size):\n",
    "            row.append([current_node[feature] for feature in JSON_features])\n",
    "            if x != max_neighbor_hops:\n",
    "                current_node = graph.nodes[current_node[\"RightNode\"]]\n",
    "        window_elements.append(row)\n",
    "        if y != max_neighbor_hops:\n",
    "                leftmost_node = graph.nodes[leftmost_node[\"DownNode\"]]\n",
    "    return window_elements\n",
    "\n",
    "def graph_to_np_array(graph, window_radius):\n",
    "    train_samples = []\n",
    "    train_labels = []\n",
    "    if window_radius < 0:\n",
    "        print('ERROR: the window_radius must be greater than zero.')\n",
    "    for node_id in graph.nodes:\n",
    "        node = graph.nodes[node_id]\n",
    "        if check_node_neighbors(graph, node, window_radius):\n",
    "            train_samples.append(get_window_elements(graph, node, window_radius))\n",
    "            train_labels.append(node[\"HasDetailedRoutingViolation\"])\n",
    "    return np.array(train_samples), np.array(train_labels)\n",
    "\n",
    "def standardize_graph(graph):\n",
    "    all_features = {feature:[] for feature in JSON_features}\n",
    "    for node_id in graph.nodes:\n",
    "        node = graph.nodes[node_id]\n",
    "        for feature in JSON_features:\n",
    "            all_features[feature].append(node[feature])\n",
    "    all_features_np = {feature:np.array(all_features[feature]) for feature in JSON_features}\n",
    "    means = {feature:all_features_np[feature].mean() for feature in JSON_features}\n",
    "    stddevs = {feature:all_features_np[feature].std() for feature in JSON_features}\n",
    "    for node_id in graph.nodes:\n",
    "        node = graph.nodes[node_id]\n",
    "        for feature in JSON_features:\n",
    "            if stddevs[feature] == 0:\n",
    "                node[feature] = 0 #All values are equal, so let's ignore this feature by setting it to zero\n",
    "                continue\n",
    "            node[feature] = (node[feature] - means[feature]) / stddevs[feature]\n",
    "\n",
    "# Claculate weight for classes\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "def calculate_class_weights_graph(labels):\n",
    "    neg, pos = np.bincount(labels)\n",
    "    total = neg + pos\n",
    "    print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    return class_weight, neg, pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data from JSON Grid Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: If you want to deploy a model, it's critical that you preserve the preprocessing calculations.\n",
    "# The easiest way to implement them as layers, and attach them to your model before export.\n",
    "file_names = [\"ispd19_test6.0.0\"]#, \"ispd19_test6.0.95.0\", \"ispd19_test6.1.0\", \"ispd19_test6.0.1\", \"ispd19_test6.0.95.1\", \"ispd19_test6.1.1\"]\n",
    "# file_names = [\"ispd19_test6.0.1\", \"ispd19_test6.0.95.1\", \"ispd19_test6.1.1\"]\n",
    "graph_path = \"data/ICCAD2019/grid_graphs/\"\n",
    "\n",
    "graph = grid_graph_build(graph_path+file_names[0]+'.JSON')\n",
    "standardize_graph(graph)\n",
    "train_array, train_labels = graph_to_np_array(graph, window_radius)\n",
    "# Claculate weight for classes\n",
    "class_weight, neg, pos = calculate_class_weights_graph(train_labels)\n",
    "\n",
    "train_array, val_array, train_labels, val_labels = sklearn.model_selection.train_test_split(train_array, train_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data From ICCAD19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: If you want to deploy a model, it's critical that you preserve the preprocessing calculations.\n",
    "# The easiest way to implement them as layers, and attach them to your model before export.\n",
    "file_type_JSON = True\n",
    "\n",
    "file_names = [\"ispd19_test6.0.0\", \"ispd19_test6.0.95.0\", \"ispd19_test6.1.0\", \"ispd19_test6.0.1\", \"ispd19_test6.0.95.1\", \"ispd19_test6.1.1\"]\n",
    "# file_names = [\"ispd19_test6.0.1\", \"ispd19_test6.0.95.1\", \"ispd19_test6.1.1\"]\n",
    "graph_path = \"data/ICCAD2019/grid_graphs/\"\n",
    "csv_path = \"data/ICCAD2019/csv/\"\n",
    "\n",
    "dataframes = list\n",
    "if file_type_JSON:\n",
    "    dataframes = [graph_to_df(grid_graph_build(graph_path+file_name+'.JSON')) for file_name in file_names]\n",
    "else:\n",
    "    dataframes = [pd.read_csv(csv_path+file_name+'.csv') for file_name in file_names]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Remove graph attributes\n",
    "if file_type_JSON:\n",
    "    df.pop(\"UpNode\")\n",
    "    df.pop(\"DownNode\")\n",
    "    df.pop(\"LeftNode\")\n",
    "    df.pop(\"RightNode\")\n",
    "\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('HasDetailedRoutingViolation'))\n",
    "val_labels = np.array(val_df.pop('HasDetailedRoutingViolation'))\n",
    "\n",
    "train_array = np.array(train_df)\n",
    "val_array = np.array(val_df)\n",
    "\n",
    "# Scale\n",
    "train_array, val_array = standardize(train_array, val_array)\n",
    "\n",
    "# Claculate weight for classes\n",
    "class_weight, neg, pos = calculate_class_weights(df, 'HasDetailedRoutingViolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load EhPredictor's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ISDP14/EhPredictorISPD14.csv\")\n",
    "\n",
    "# drop l53 because is always zero\n",
    "df.pop('l53')\n",
    "df.pop('normal')\n",
    "\n",
    "# Instead of having the number of shorts, use them as a boolean\n",
    "df.loc[df['short'] > 0, 'short'] = 1\n",
    "\n",
    "# Convert to log-space. l9 l43 l45 l52 l51\n",
    "log_cols = ['l9', 'l43', 'l45', 'l52', 'l51']\n",
    "eps=0.001 # 0 => 0.1¢\n",
    "for col in log_cols:\n",
    "    df[col] = np.log(df[col] + eps)\n",
    "\n",
    "# CSV organization:\n",
    "# des_perf_1_dataset=all_dataset[0:5476,:]\n",
    "# des_perf_a_dataset=all_dataset[5476:16928,:]\n",
    "# des_perf_b_dataset=all_dataset[16928:26928,:]\n",
    "# fft_1_dataset=all_dataset[26928:28864,:]\n",
    "# fft_2_dataset=all_dataset[28864:32113,:]\n",
    "# fft_a_dataset=all_dataset[32113:38604,:]\n",
    "# fft_b_dataset=all_dataset[38604:44375,:]\n",
    "# matrix_mult_1_dataset=all_dataset[44375:52656,:]\n",
    "# matrix_mult_a_dataset=all_dataset[52656:69168,:]\n",
    "# matrix_mult_b_dataset=all_dataset[69168:90601,:]\n",
    "# pci_bridge32_a_dataset=all_dataset[90601:94170,:]\n",
    "# pci_bridge32_b_dataset=all_dataset[94170:103961,:]\n",
    "# superblue11_a_dataset=all_dataset[103961:175113,:]\n",
    "# superblue12_dataset=all_dataset[175113:241123,:]\n",
    "\n",
    "# Test circuits: mgc fft_2\n",
    "test_df = df.iloc[28864:32113]\n",
    "df2 = df[0:28864]\n",
    "df3 = df[32113:]\n",
    "df = pd.concat([df2, df3])\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('short'))\n",
    "val_labels = np.array(val_df.pop('short'))\n",
    "test_labels = np.array(test_df.pop('short'))\n",
    "\n",
    "train_array = np.array(train_df)\n",
    "val_array = np.array(val_df)\n",
    "test_array = np.array(test_df)\n",
    "\n",
    "# Scaling\n",
    "train_array, val_array, test_array = standardize(train_array, val_array, test_array)\n",
    "\n",
    "# Claculate weight for classes\n",
    "class_weight, neg, pos = calculate_class_weights(df, 'short')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(metrics = METRICS, output_bias=None, lr=learning_rate):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.Sequential([\n",
    "                                 tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(window_size, window_size, len(JSON_features)), kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "                                 tf.keras.layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "                                 tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "                                 tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "                                 tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "                                 tf.keras.layers.AveragePooling2D(3, 3),\n",
    "                                 tf.keras.layers.Flatten(),\n",
    "                                 tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.004)),\n",
    "                                 tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.004)),\n",
    "                                 tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(metrics = METRICS, output_bias=None, lr=learning_rate):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(20,\n",
    "                                                       activation='relu', # Relu throw away negative values\n",
    "                                                       kernel_regularizer=tf.keras.regularizers.l2(beta)),\n",
    "                                 tf.keras.layers.Dropout(drop_out),\n",
    "                                 tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_bias = np.log([pos/neg])\n",
    "# model = make_model(output_bias = initial_bias)\n",
    "# model.load_weights('model_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_array, train_labels))\n",
    "train_dataset = dataset.shuffle(len(train_array)).batch(batch_size)\n",
    "\n",
    "initial_bias = np.log([pos/neg])\n",
    "\n",
    "model = make_model(output_bias = initial_bias)\n",
    "train_history = model.fit(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(val_array, val_labels),\n",
    "                          class_weight=class_weight,\n",
    "                          epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('model_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = sklearn.metrics.confusion_matrix(labels, predictions > p)\n",
    "    matplotlib.pyplot.figure(figsize=(5,5))\n",
    "    seaborn.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    matplotlib.pyplot.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    matplotlib.pyplot.ylabel('Actual label')\n",
    "    matplotlib.pyplot.xlabel('Predicted label')\n",
    "    matplotlib.pyplot.show()\n",
    "    \n",
    "def calculate_metrics(model, results):\n",
    "    m = {}\n",
    "    for name, value in zip(model.metrics_names, results):\n",
    "        m[name] = value\n",
    "    if m['precision'] + m['recall'] != 0:\n",
    "        f_score = (2 * m['precision'] * m['recall'])/(m['precision'] + m['recall'])\n",
    "        m['F-score'] = f_score\n",
    "    sqrt = math.sqrt((m['tp']+m['fp'])*(m['tp']+m['fn'])*(m['tn']+m['fp'])*(m['tn']+m['fn']))\n",
    "    if sqrt != 0:\n",
    "        mcc = (m['tp'] * m['tn'] - m['fp'] * m['fn'])/sqrt\n",
    "        m['MCC'] = mcc\n",
    "    return m\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    for x, y in metrics.items():\n",
    "        print(x,':', round(y, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(train_array, train_labels, batch_size=batch_size, verbose=0)\n",
    "metrics = calculate_metrics(model, baseline_results)\n",
    "print_metrics(metrics)\n",
    "train_predictions_baseline = model.predict(train_array, batch_size=batch_size)\n",
    "plot_cm(train_labels, train_predictions_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performace on ISPD14 - mgc_fft_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(test_array, test_labels, batch_size=batch_size, verbose=0)\n",
    "metrics = calculate_metrics(model, baseline_results)\n",
    "print_metrics(metrics)\n",
    "test_predictions_baseline = model.predict(test_array, batch_size=batch_size)\n",
    "plot_cm(test_labels, test_predictions_baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
