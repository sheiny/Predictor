{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "* Good Readings\n",
    "    + Re-sampling (over/under): https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-\n",
    "    + Good reading about Standardization: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "    + Standardization vs normalization: https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf\n",
    "    + Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "    + https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "    + How to evaluate the model: https://machinelearningmastery.com/evaluate-skill-deep-learning-models/\n",
    "    + https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/\n",
    "    + https://towardsdatascience.com/how-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55\n",
    "    + Batch size: https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/\n",
    "    + Model doesn't fit in memory: https://gdmarmerola.github.io/big-data-ml-training/\n",
    "    + More aboute standardization of test set: https://stats.stackexchange.com/questions/202287/why-standardization-of-the-testing-set-has-to-be-performed-with-the-mean-and-sd\n",
    "    + How to standardize: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run: packages to install\n",
    "\n",
    "    0- conda install nb_conda\n",
    "    1- conda install matplotlib \n",
    "    2- conda install tensorflow\n",
    "    3- conda install scikit-learn \n",
    "    4- conda install seaborn\n",
    "    5- pip install imbalanced-learn\n",
    "        \n",
    "    Run all initialization cells (View -> Cell Toolbar -> Initialization Cell).\n",
    "    After that select a dataset to load a learning model.\n",
    "    Finally train and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and extract dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://drive.google.com/file/d/1yPDkDKCKScE9KiLxZ8rc7Zn8cHWgQIua/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from keras.callbacks import CSVLogger\n",
    "import os\n",
    "from enum import Enum\n",
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class BalanceStrategy(Enum):\n",
    "    NONE = 0\n",
    "    WEIGHTS = 1\n",
    "    OVERSAMPLE = 2\n",
    "    UNDERSAMPLE = 3\n",
    "\n",
    "balance_strategy = BalanceStrategy.WEIGHTS #Strategy to handle the imbalanced dataset\n",
    "\n",
    "# Neural networks are stochastic by design and that the source of randomness can be fixed to make results reproducible.\n",
    "# Therefore, the most robust way to report results and compare models is to repeat your experiment many times (30+) and use summary statistics.\n",
    "# Source: https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
    "# Fix random seed.\n",
    "#tf.random.set_seed(1234)\n",
    "#np.random.seed(1234) # Scikit Learn does not have its own global random state but uses the numpy random state instead.\n",
    "\n",
    "model_name = \"Test\" #it will create a directory with this name to save the model's weights and training history\n",
    "\n",
    "batch_size = 32 # is important to ensure that each batch has a decent chance of containing a few positive samples\n",
    "epochs = 20\n",
    "learning_rate = 0.001 #Eh?Predictor=0.05, default=0.001\n",
    "drop_out = 0.05 ##Eh?Predictor=0.05\n",
    "\n",
    "METRICS = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "           tf.keras.metrics.FalsePositives(name='fp'),\n",
    "           tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "           tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "           tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "           tf.keras.metrics.Precision(name='precision'),\n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(len(selectedFeatures), activation='relu'),\n",
    "                                 tf.keras.layers.Dropout(drop_out),\n",
    "                                 tf.keras.layers.Dense(50, activation='relu'),\n",
    "                                 tf.keras.layers.Dropout(drop_out),\n",
    "                                 tf.keras.layers.Dense(50, activation='relu'),\n",
    "                                 tf.keras.layers.Dropout(drop_out),\n",
    "                                 tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Features and DRV types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "placementFeatures = [\"#Cells\", \"#CellPins\", \"#Macros\", \"#MacroPins\", \"HorizontalOverflow\",\n",
    "                     \"VerticalOverflow\",\n",
    "                     \"TileArea\", \"CellDensity\", \"MacroDensity\", \"MacroPinDensity\",\n",
    "                     \"Layer1BlkgDensity\", \"Layer2BlkgDensity\", \"Layer1PinDensity\", \"Layer2PinDensity\"]\n",
    "\n",
    "placementNeighborFeatures = [\"NeighborTileArea\", \"NeighborCellArea\", \"NeighborL1PinArea\", \"NeighborL2PinArea\",\n",
    "                             \"NeighborL1BlkArea\", \"NeighborL2BlkArea\", \"NeighborMacroArea\",\n",
    "                             \"NeighborMacroPinArea\", \"#NeighborCells\", \"#NeighborCellPins\", \"#NeighborMacros\",\n",
    "                             \"#NeighborMacroPins\", \"#NeighborPassingNets\"]\n",
    "\n",
    "GRFeatures = [\"#VerticalOverflow\", \"#VerticalRemain\", \"#VerticalTracks\",\n",
    "              \"#HorizontalOverflow\", \"#HorizontalRemain\", \"#HorizontalTracks\"]\n",
    "\n",
    "GRNeighborFeatures = [\"#NeighborVerticalOverflow\", \"#NeighborVerticalRemain\", \"#NeighborVerticalTracks\",\n",
    "                      \"#NeighborHorizontalOverflow\", \"#NeighborHorizontalRemain\", \"#NeighborHorizontalTracks\"]\n",
    "selectedFeatures = set()\n",
    "selectedFeatures.update(placementFeatures)\n",
    "#selectedFeatures.update(placementNeighborFeatures)\n",
    "#selectedFeatures.update(GRFeatures)\n",
    "#selectedFeatures.update(GRNeighborFeatures)\n",
    "\n",
    "AllDRVTypes = [\"AdjacentCutSpacing\", \"SameLayerCutSpacing\", \"EndOfLine\", \"FloatingPatch\", \"MinArea\", \"MinWidth\",\n",
    "  \"NonSuficientMetalOverlap\", \"CutShort\", \"MetalShort\", \"OutOfDieShort\", \"CornerSpacing\", \"ParallelRunLength\"]\n",
    "\n",
    "SelectedDRVTypes = [\"CutShort\", \"MetalShort\"]\n",
    "\n",
    "label_name = \"HasDetailedRoutingViolation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Benchmarks\n",
    "# ispd18 = [\"ispd18_test\"+str(x) for x in range(1, 11)]\n",
    "# ispd18.extend([\"ispd18_test5_metal5\", \"ispd18_test8_metal5\"]) #Include Hidden cases (benchmarks with less layers)\n",
    "\n",
    "ispd19 = [\"ispd19_test\"+str(x) for x in range(1, 11)]\n",
    "# ispd19.extend([\"ispd19_test7_metal5\", \"ispd19_test8_metal5\", \"ispd19_test9_metal5\"]) #Include Hidden cases (benchmarks with less layers)\n",
    "\n",
    "circuits = ispd19\n",
    "test_circuit = \"ispd19_test10\"\n",
    "# test_circuit = 'swerv'\n",
    "if test_circuit in circuits:\n",
    "    circuits.remove(test_circuit)\n",
    "circuits.remove(\"ispd19_test4\")#low density benchmark\n",
    "circuits.remove(\"ispd19_test5\")#low density benchmark\n",
    "circuits.remove(\"ispd19_test9\")#Simillar from test10\n",
    "circuits.extend(['aes_cipher_top', 'black_parrot', 'gcd', 'ibex', 'jpeg', 'dynamic_node', 'TinyRocket'])\n",
    "\n",
    "# CSV Paths\n",
    "csv_path = \"/home/sheiny/workspace/Data/CSV/steiner_metal3/\"\n",
    "# csv_path = \"./data/SecondIterationGCellCoords/\"\n",
    "# csv_path = \"./data/FirstIterationFixedBin/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "ATTENTION: If you want to deploy a model, it's critical that you preserve the preprocessing calculations.\n",
    "The easiest way to implement them as layers, and attach them to your model before export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# The features will be rescaled so that they’ll have the properties of a standard normal distribution.\n",
    "# mean (μ) = 0\n",
    "# standard deviation (σ) = 1\n",
    "def standardize(train_array, val_array, test_array=None):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    train_array = scaler.fit_transform(train_array)\n",
    "    val_array = scaler.transform(val_array)\n",
    "    if test_array is not None:\n",
    "        test_array = scaler.transform(test_array)\n",
    "        return train_array, val_array, test_array\n",
    "    return train_array, val_array\n",
    "\n",
    "def print_positive_ratio(train_labels):\n",
    "    neg, pos = np.bincount(train_labels)\n",
    "    total = neg + pos\n",
    "    print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "\n",
    "# Claculate weight for classes\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "def calculate_class_weights(df, train_labels):\n",
    "    neg, pos = np.bincount(train_labels)\n",
    "    total = neg + pos\n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    print_positive_ratio(train_labels)\n",
    "    return class_weight, neg, pos\n",
    "\n",
    "def oversample(train_array, train_labels):\n",
    "    oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "    train_array, train_labels = oversample.fit_resample(train_array, train_labels)\n",
    "    print_positive_ratio(train_labels)\n",
    "    return train_array, train_labels\n",
    "\n",
    "def undersample(train_array, train_labels):\n",
    "    undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "    train_array, train_labels = undersample.fit_resample(train_array, train_labels)\n",
    "    print_positive_ratio(train_labels)\n",
    "    return train_array, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#matplotlib.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = matplotlib.pyplot.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "def plotDRVFrequency():\n",
    "    benchmarks = [\"ispd19_test\"+str(x) for x in range(1, 11)]\n",
    "    #Remove some non important DRVs\n",
    "    drvs = AllDRVTypes.copy()\n",
    "    drvs.remove(\"FloatingPatch\")\n",
    "    drvs.remove(\"MinWidth\")\n",
    "    drvs.remove(\"NonSuficientMetalOverlap\")\n",
    "    drvs.remove(\"CutShort\")\n",
    "    drvs.remove(\"OutOfDieShort\")\n",
    "    drv_dict = {drv:[] for drv in drvs}\n",
    "\n",
    "    for circuit in benchmarks:\n",
    "        df = pd.read_csv(\"./data/FirstIterationGCellCoords/\"+circuit+\".csv\", dtype=np.float32)\n",
    "        df2 = pd.read_csv(\"./data/SecondIterationGCellCoords/\"+circuit+\".csv\", dtype=np.float32)\n",
    "        for drv in drvs:\n",
    "            drv_dict[drv].append((int(sum(df[drv])), int(sum(df2[drv]))))\n",
    "    drv_df = pd.DataFrame(drv_dict)\n",
    "    drv_df.index = benchmarks\n",
    "    drv_df\n",
    "\n",
    "def computeFScoreAndMCC(df):\n",
    "    df['F-score'] = (2 * df['precision'] * df['recall'])/(df['precision'] + df['recall'])\n",
    "    sqrt = np.sqrt((df['tp']+df['fp'])*(df['tp']+df['fn'])*(df['tn']+df['fp'])*(df['tn']+df['fn']))\n",
    "    df['MCC'] = (df['tp'] * df['tn'] - df['fp'] * df['fn'])/sqrt\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "def plot_df(history_df, metric, size=None):\n",
    "    if size == None:\n",
    "        size = history_df.shape[0]\n",
    "    matplotlib.pyplot.style.use(\"ggplot\")\n",
    "    matplotlib.pyplot.figure()\n",
    "    matplotlib.pyplot.plot(np.arange(0, size), history_df[metric][0:size], label=metric)\n",
    "    matplotlib.pyplot.title(\"Training performace: \"+metric)\n",
    "    matplotlib.pyplot.xlabel(\"Epoch #\")\n",
    "    matplotlib.pyplot.ylabel(metric)\n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "def plot_cm(labels, predictions, title=None, output_path=None, p=0.5):\n",
    "    cm = sklearn.metrics.confusion_matrix(labels, predictions > p)\n",
    "    matplotlib.pyplot.figure(figsize=(5,5))\n",
    "    seaborn.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    if title == None:\n",
    "        matplotlib.pyplot.title('Confusion matrix')\n",
    "    else:\n",
    "        matplotlib.pyplot.title(title)\n",
    "    matplotlib.pyplot.ylabel('Actual label')\n",
    "    matplotlib.pyplot.xlabel('Predicted label')\n",
    "    if output_path != None:\n",
    "        matplotlib.pyplot.savefig(output_path)\n",
    "    else:\n",
    "        matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data From ICCAD19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "\n",
    "dataframes = []\n",
    "dataframes = [pd.read_csv(csv_path+circuit+\".csv\", dtype=np.float32) for circuit in circuits]\n",
    "test_df = pd.read_csv(csv_path+test_circuit+\".csv\", dtype=np.float32)\n",
    "    \n",
    "#merge all DataFrames into a single one\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "#save some memory\n",
    "dataframes.clear()\n",
    "\n",
    "# Remove NodeIDs (debug info)\n",
    "df = df.drop(columns=[\"NodeID\"])\n",
    "test_df_NodeID = test_df[\"NodeID\"] #backup this for DRV draw\n",
    "test_df = test_df.drop(columns=[\"NodeID\"])\n",
    "\n",
    "# Make sure to clear all DRV columns\n",
    "df['HasDetailedRoutingViolation'] = False\n",
    "test_df['HasDetailedRoutingViolation'] = False\n",
    "# Apply filter for selected DRVs\n",
    "for drv in SelectedDRVTypes:\n",
    "    df['HasDetailedRoutingViolation'] = df['HasDetailedRoutingViolation'] | df[drv]\n",
    "    test_df['HasDetailedRoutingViolation'] = test_df['HasDetailedRoutingViolation'] | test_df[drv]\n",
    "\n",
    "# Drop all columns that are not in {selectedFeatures + label_name}\n",
    "dontDropColumns = selectedFeatures | {label_name}\n",
    "dropColumns = set(df.columns) - dontDropColumns\n",
    "df = df.drop(columns=dropColumns)\n",
    "test_df = test_df.drop(columns=dropColumns)\n",
    "\n",
    "# Split 80/20 (train 80% test 20%)\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Build np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('HasDetailedRoutingViolation'))\n",
    "val_labels = np.array(val_df.pop('HasDetailedRoutingViolation'))\n",
    "test_labels = np.array(test_df.pop('HasDetailedRoutingViolation'))\n",
    "train_array = np.array(train_df)\n",
    "val_array = np.array(val_df)\n",
    "test_array = np.array(test_df)\n",
    "\n",
    "# Apply the selected strategy to handle umbalanced data.\n",
    "class_weight = None\n",
    "if balance_strategy == BalanceStrategy.OVERSAMPLE:\n",
    "    train_array, train_labels = oversample(train_array, train_labels)\n",
    "elif balance_strategy == BalanceStrategy.UNDERSAMPLE:\n",
    "    train_array, train_labels = undersample(train_array, train_labels)\n",
    "elif balance_strategy == BalanceStrategy.WEIGHTS:\n",
    "    class_weight = calculate_class_weights(df, train_labels)\n",
    "    class_weight = class_weight[0]\n",
    "\n",
    "# Scale\n",
    "train_array, val_array, test_array = standardize(train_array, val_array, test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load EhPredictor's dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"./data/ISPD14/EhPredictorISPD14.csv\")\n",
    "\n",
    "# drop l53 because is always zero\n",
    "df.pop('l53')\n",
    "df.pop('normal')\n",
    "\n",
    "# Instead of having the number of shorts, use them as a boolean\n",
    "df.loc[df['short'] > 0, 'short'] = 1\n",
    "\n",
    "# Convert to log-space. l9 l43 l45 l52 l51\n",
    "log_cols = ['l9', 'l43', 'l45', 'l52', 'l51']\n",
    "eps=0.001 # 0 => 0.1¢\n",
    "for col in log_cols:\n",
    "    df[col] = np.log(df[col] + eps)\n",
    "\n",
    "# CSV organization:\n",
    "# des_perf_1_dataset=all_dataset[0:5476,:]\n",
    "# des_perf_a_dataset=all_dataset[5476:16928,:]\n",
    "# des_perf_b_dataset=all_dataset[16928:26928,:]\n",
    "# fft_1_dataset=all_dataset[26928:28864,:]\n",
    "# fft_2_dataset=all_dataset[28864:32113,:]\n",
    "# fft_a_dataset=all_dataset[32113:38604,:]\n",
    "# fft_b_dataset=all_dataset[38604:44375,:]\n",
    "# matrix_mult_1_dataset=all_dataset[44375:52656,:]\n",
    "# matrix_mult_a_dataset=all_dataset[52656:69168,:]\n",
    "# matrix_mult_b_dataset=all_dataset[69168:90601,:]\n",
    "# pci_bridge32_a_dataset=all_dataset[90601:94170,:]\n",
    "# pci_bridge32_b_dataset=all_dataset[94170:103961,:]\n",
    "# superblue11_a_dataset=all_dataset[103961:175113,:]\n",
    "# superblue12_dataset=all_dataset[175113:241123,:]\n",
    "\n",
    "# Test circuits: mgc fft_2\n",
    "test_df = df.iloc[28864:32113]\n",
    "df2 = df[0:28864]\n",
    "df3 = df[32113:]\n",
    "df = pd.concat([df2, df3])\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('short'))\n",
    "val_labels = np.array(val_df.pop('short'))\n",
    "test_labels = np.array(test_df.pop('short'))\n",
    "\n",
    "train_array = np.array(train_df)\n",
    "val_array = np.array(val_df)\n",
    "test_array = np.array(test_df)\n",
    "\n",
    "# Scaling\n",
    "train_array, val_array, test_array = standardize(train_array, val_array, test_array)\n",
    "\n",
    "# Claculate weight for classes\n",
    "class_weight, neg, pos = calculate_class_weights(df, 'short')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./results/OverSampleSingleHiddenLayer50/cp.ckpt\" #path to cp.ckpt\n",
    "model = make_model()\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(model_name)\n",
    "checkpoint_path = model_name+\"/cp.ckpt\"\n",
    "\n",
    "# Create a callback that saves the model's weights at the end of each epoch\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True)\n",
    "# Create a callback that saves model history at the end of each epoch\n",
    "csv_logger = CSVLogger(model_name+\"/model_history_log.csv\", append=True)\n",
    "\n",
    "#initialize learning model\n",
    "neg, pos = np.bincount(train_labels)\n",
    "initial_bias = np.log([pos/neg])\n",
    "model = make_model(output_bias = initial_bias)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_array, train_labels))\n",
    "train_dataset = dataset.shuffle(len(train_array)).batch(batch_size)\n",
    "\n",
    "train_history = model.fit(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(val_array, val_labels),\n",
    "                          class_weight=class_weight, #default class_weight = None\n",
    "                          epochs=epochs,\n",
    "                          callbacks=[cp_callback, csv_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.read_csv(model_name+\"/model_history_log.csv\") #Path to \"model_history_log.csv\"\n",
    "computeFScoreAndMCC(history_df)\n",
    "metrics_to_draw = ['loss', 'F-score', 'MCC', 'precision', 'recall']\n",
    "max_epochs = 20 #Use None to draw the entire history\n",
    "for metric in metrics_to_draw:\n",
    "    plot_df(history_df, metric) #plot_df(history_df, metric, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_metrics(model, results):\n",
    "    m = {}\n",
    "    for name, value in zip(model.metrics_names, results):\n",
    "        m[name] = value\n",
    "    if m['precision'] + m['recall'] != 0:\n",
    "        f_score = (2 * m['precision'] * m['recall'])/(m['precision'] + m['recall'])\n",
    "        m['F-score'] = f_score\n",
    "    sqrt = math.sqrt((m['tp']+m['fp'])*(m['tp']+m['fn'])*(m['tn']+m['fp'])*(m['tn']+m['fn']))\n",
    "    if sqrt != 0:\n",
    "        mcc = (m['tp'] * m['tn'] - m['fp'] * m['fn'])/sqrt\n",
    "        m['MCC'] = mcc\n",
    "    return m\n",
    "\n",
    "baseline_results = model.evaluate(test_array, test_labels, batch_size=batch_size, verbose=0)\n",
    "metrics = calculate_test_metrics(model, baseline_results)\n",
    "print(metrics)\n",
    "test_predictions_baseline = model.predict(test_array, batch_size=batch_size)\n",
    "plot_cm(test_labels, test_predictions_baseline, \"Test Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Predicted Node IDs to draw in OpenROAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all predited node ids\n",
    "# This can be used to plot the violating bins inside C++ OpenROAD\n",
    "violations = test_predictions_baseline > 0.5\n",
    "violating_ids = []\n",
    "i = 0\n",
    "for x in zip (violations, test_df_NodeID):\n",
    "    if x[0]:\n",
    "        violating_ids.append(int(x[1]))\n",
    "with open(test_circuit+'violating_nodes.txt', 'w') as f:\n",
    "    for item in violating_ids:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
