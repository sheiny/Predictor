{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Python\n",
    "    + Make sure to remove completely overlapped samples.\n",
    "\n",
    "Warning:\n",
    "All area features were normalized, except AreaTile.\n",
    "\n",
    "tip: try to plot a histogram to check for the outliers before the normalization.\n",
    "\n",
    "* Readings\n",
    "    + Standardization vs normalization: https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf\n",
    "    + Scaling: https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed.\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234) # Scikit Learn does not have its own global random state but uses the numpy random state instead.\n",
    "batch_size = 256 # is important to ensure that each batch has a decent chance of containing a few positive samples\n",
    "epochs = 5\n",
    "learning_rate = 0.005\n",
    "beta = 0.001 #regularization\n",
    "drop_out = 0.05\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = matplotlib.pyplot.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(train_array, val_array, test_array=None):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    train_array = scaler.fit_transform(train_array)\n",
    "    val_array = scaler.transform(val_array)\n",
    "    if test_array is not None:\n",
    "        test_array = scaler.transform(test_array)\n",
    "        return train_array, val_array, test_array\n",
    "    return train_array, val_array\n",
    "\n",
    "# Claculate weight for classes\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "def calculate_class_weights(df, classification_label):\n",
    "    neg, pos = np.bincount(df[classification_label])\n",
    "    total = neg + pos\n",
    "    print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    return class_weight, neg, pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph functions using NetworkX library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_graph_build(json_file_path):\n",
    "    graph = nx.Graph()\n",
    "    nodes_jason = open(json_file_path)\n",
    "    data = json.load(nodes_jason)\n",
    "    nodes_jason.close()\n",
    "    for j_node in data:\n",
    "        node_id = j_node[\"id\"]\n",
    "        graph.add_node(node_id)\n",
    "        node = graph.nodes[node_id]\n",
    "        node[\"NumCells\"] = j_node[\"NumCells\"]\n",
    "        node[\"NumCellPins\"] = j_node[\"NumCellPins\"]\n",
    "        node[\"NumMacros\"] = j_node[\"NumMacros\"]\n",
    "        node[\"NumMacroPins\"] = j_node[\"NumMacroPins\"]\n",
    "        node[\"NumPassingNets\"] = j_node[\"NumPassingNets\"]\n",
    "        node[\"AreaCells\"] = j_node[\"AreaCells\"]\n",
    "        node[\"AreaL1Pin\"] = j_node[\"AreaL1Pin\"]\n",
    "        node[\"AreaL2Pin\"] = j_node[\"AreaL2Pin\"]\n",
    "        node[\"AreaL1Blkg\"] = j_node[\"AreaL1Blkg\"]\n",
    "        node[\"AreaL2Blkg\"] = j_node[\"AreaL2Blkg\"]\n",
    "        node[\"AreaMacroPins\"] = j_node[\"AreaMacroPins\"]\n",
    "        node[\"AreaMacros\"] = j_node[\"AreaMacros\"]\n",
    "        node[\"AreaTile\"] = j_node[\"AreaTile\"]\n",
    "        node[\"HasDetailedRoutingViolation\"] = j_node[\"HasDetailedRoutingViolation\"]\n",
    "    for j_node in data:\n",
    "        node_id = j_node[\"id\"]\n",
    "        for neighbor in [\"UpNode\", \"DownNode\", \"LeftNode\", \"RightNode\"]:\n",
    "            if j_node[neighbor] != -1:\n",
    "                graph.add_edge(node_id, j_node[neighbor])\n",
    "            node[neighbor] = j_node[neighbor] if j_node[neighbor] != -1 else -1\n",
    "    return graph\n",
    "\n",
    "# simple sanity check function for a grid graph structure\n",
    "def check_json_grid_graph(json_file_path):\n",
    "    graph = grid_graph(json_file_path)\n",
    "    nodes_jason = open(json_file_path)\n",
    "    data = json.load(nodes_jason)\n",
    "    nodes_jason.close()\n",
    "    corners = 0\n",
    "    for j_node in data:\n",
    "        node_id = j_node[\"id\"]\n",
    "        if graph.degree[node_id] == 2 and corners < 4:\n",
    "            corners += 1\n",
    "            continue\n",
    "        if graph.degree[node_id] != 3 and graph.degree[node_id] != 4:\n",
    "            print(\"WARNING! strange node degree found: \", graph.degree[node_id])\n",
    "\n",
    "def graph_to_df(graph):\n",
    "    return pd.DataFrame([graph.nodes[x] for x in graph.nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data From A JSON Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: If you want to deploy a model, it's critical that you preserve the preprocessing calculations.\n",
    "# The easiest way to implement them as layers, and attach them to your model before export.\n",
    "graph = grid_graph_build('data/ICCAD2019/grid_graphs/ispd19_test6.0.0.JSON')\n",
    "df = graph_to_df(graph)\n",
    "\n",
    "# Remove graph attributes\n",
    "df.pop(\"UpNode\")\n",
    "df.pop(\"DownNode\")\n",
    "df.pop(\"LeftNode\")\n",
    "df.pop(\"RightNode\")\n",
    "\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('HasDetailedRoutingViolation'))\n",
    "val_labels = np.array(val_df.pop('HasDetailedRoutingViolation'))\n",
    "\n",
    "train_array = np.array(train_df)\n",
    "val_array = np.array(val_df)\n",
    "\n",
    "# Scale\n",
    "train_array, val_array = scaling(train_array, val_array)\n",
    "\n",
    "# Claculate weight for classes\n",
    "class_weight, neg, pos = calculate_class_weights(df, 'HasDetailedRoutingViolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data From A CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: If you want to deploy a model, it's critical that you preserve the preprocessing calculations.\n",
    "# The easiest way to implement them as layers, and attach them to your model before export.\n",
    "\n",
    "# Naming convention: Circuit Name + ABU + DR iteration + csv\n",
    "# Note: ABU zero means that the placement wasn't changed from contest benchmark.\n",
    "csvs = [\"ispd19_test6.0.0.csv\", \"ispd19_test6.0.95.0.csv\", \"ispd19_test6.1.0.csv\", \"ispd19_test6.0.1.csv\", \"ispd19_test6.0.95.1.csv\", \"ispd19_test6.1.1.csv\"]\n",
    "# csvs = [\"ispd19_test6.0.1.csv\", \"ispd19_test6.0.95.1.csv\", \"ispd19_test6.1.1.csv\"]\n",
    "path = \"data/ICCAD2019/csv/\"\n",
    "\n",
    "dataframes = [pd.read_csv(path+file_name) for file_name in csvs]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('HasDetailedRoutingViolation'))\n",
    "val_labels = np.array(val_df.pop('HasDetailedRoutingViolation'))\n",
    "\n",
    "train_array = np.array(train_df)\n",
    "val_array = np.array(val_df)\n",
    "\n",
    "# Scale\n",
    "train_array, val_array = scaling(train_array, val_array)\n",
    "\n",
    "# Claculate weight for classes\n",
    "class_weight, neg, pos = calculate_class_weights(df, 'HasDetailedRoutingViolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load EhPredictor's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ISDP14/EhPredictorISPD14.csv\")\n",
    "\n",
    "# drop l53 because is always zero\n",
    "df.pop('l53')\n",
    "df.pop('normal')\n",
    "\n",
    "# Instead of having the number of shorts, use them as a boolean\n",
    "df.loc[df['short'] > 0, 'short'] = 1\n",
    "\n",
    "# Convert to log-space. l9 l43 l45 l52 l51\n",
    "log_cols = ['l9', 'l43', 'l45', 'l52', 'l51']\n",
    "eps=0.001 # 0 => 0.1Â¢\n",
    "for col in log_cols:\n",
    "    df[col] = np.log(df[col] + eps)\n",
    "\n",
    "# CSV organization:\n",
    "# des_perf_1_dataset=all_dataset[0:5476,:]\n",
    "# des_perf_a_dataset=all_dataset[5476:16928,:]\n",
    "# des_perf_b_dataset=all_dataset[16928:26928,:]\n",
    "# fft_1_dataset=all_dataset[26928:28864,:]\n",
    "# fft_2_dataset=all_dataset[28864:32113,:]\n",
    "# fft_a_dataset=all_dataset[32113:38604,:]\n",
    "# fft_b_dataset=all_dataset[38604:44375,:]\n",
    "# matrix_mult_1_dataset=all_dataset[44375:52656,:]\n",
    "# matrix_mult_a_dataset=all_dataset[52656:69168,:]\n",
    "# matrix_mult_b_dataset=all_dataset[69168:90601,:]\n",
    "# pci_bridge32_a_dataset=all_dataset[90601:94170,:]\n",
    "# pci_bridge32_b_dataset=all_dataset[94170:103961,:]\n",
    "# superblue11_a_dataset=all_dataset[103961:175113,:]\n",
    "# superblue12_dataset=all_dataset[175113:241123,:]\n",
    "\n",
    "# Test circuits: mgc fft_2\n",
    "test_df = df.iloc[28864:32113]\n",
    "df2 = df[0:28864]\n",
    "df3 = df[32113:]\n",
    "df = pd.concat([df2, df3])\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('short'))\n",
    "val_labels = np.array(val_df.pop('short'))\n",
    "test_labels = np.array(test_df.pop('short'))\n",
    "\n",
    "train_array = np.array(train_df)\n",
    "val_array = np.array(val_df)\n",
    "test_array = np.array(test_df)\n",
    "\n",
    "# Scaling\n",
    "train_array, val_array, test_array = scaling(train_array, val_array, test_array)\n",
    "\n",
    "# Claculate weight for classes\n",
    "class_weight, neg, pos = calculate_class_weights(df, 'short')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "           tf.keras.metrics.FalsePositives(name='fp'),\n",
    "           tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "           tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "           tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "           tf.keras.metrics.Precision(name='precision'),\n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "def make_model(metrics = METRICS, output_bias=None, lr=learning_rate):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(20,\n",
    "                                                       activation='relu', # Relu throw away negative values\n",
    "                                                       kernel_regularizer=tf.keras.regularizers.l2(beta)),\n",
    "                                 tf.keras.layers.Dropout(drop_out),\n",
    "                                 tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=metrics)\n",
    "    return model\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_array, train_labels))\n",
    "train_dataset = dataset.shuffle(len(train_array)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_bias = np.log([pos/neg])\n",
    "# model = make_model(output_bias = initial_bias)\n",
    "# model.load_weights('model_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initial_bias = np.log([pos/neg])\n",
    "\n",
    "model = make_model(output_bias = initial_bias)\n",
    "train_history = model.fit(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(val_array, val_labels),\n",
    "                          class_weight=class_weight,\n",
    "                          epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('model_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = sklearn.metrics.confusion_matrix(labels, predictions > p)\n",
    "    matplotlib.pyplot.figure(figsize=(5,5))\n",
    "    seaborn.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    matplotlib.pyplot.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    matplotlib.pyplot.ylabel('Actual label')\n",
    "    matplotlib.pyplot.xlabel('Predicted label')\n",
    "    matplotlib.pyplot.show()\n",
    "    \n",
    "def calculate_metrics(model, results):\n",
    "    m = {}\n",
    "    for name, value in zip(model.metrics_names, results):\n",
    "        m[name] = value\n",
    "    if m['precision'] + m['recall'] != 0:\n",
    "        f_score = (2 * m['precision'] * m['recall'])/(m['precision'] + m['recall'])\n",
    "        m['F-score'] = f_score\n",
    "    sqrt = math.sqrt((m['tp']+m['fp'])*(m['tp']+m['fn'])*(m['tn']+m['fp'])*(m['tn']+m['fn']))\n",
    "    if sqrt != 0:\n",
    "        mcc = (m['tp'] * m['tn'] - m['fp'] * m['fn'])/sqrt\n",
    "        m['MCC'] = mcc\n",
    "    return m\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    for x, y in metrics.items():\n",
    "        print(x,':', round(y, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(train_array, train_labels, batch_size=batch_size, verbose=0)\n",
    "metrics = calculate_metrics(model, baseline_results)\n",
    "print_metrics(metrics)\n",
    "train_predictions_baseline = model.predict(train_array, batch_size=batch_size)\n",
    "plot_cm(train_labels, train_predictions_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performace on ISPD14 - mgc_fft_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(test_array, test_labels, batch_size=batch_size, verbose=0)\n",
    "metrics = calculate_metrics(model, baseline_results)\n",
    "print_metrics(metrics)\n",
    "test_predictions_baseline = model.predict(test_array, batch_size=batch_size)\n",
    "plot_cm(test_labels, test_predictions_baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
