{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Important\n",
    "    + A weight function that receives a list of graphs\n",
    "    + optimize the code to consume less memory\n",
    "    + Double-check if the normalization should be done for each benchmark or using all dataset\n",
    "      I believe that only the weights should consider all benchmarks (excluding test).\n",
    "    + Double-check if we are doing the incremental DR correctly, because the number of DRVs keeps unchanged across the iterations (check the complete routeDesign result).\n",
    "    + Improve NN achitecture (parameters)\n",
    "    + Include more DRVs\n",
    "    + Make sure to remove completely overlapped samples.\n",
    "    + Add padding in check_node_neighbors\n",
    "    + Grid drawn\n",
    "\n",
    "* Readings\n",
    "    + Fit Generator hands-on: https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/\n",
    "    + Good reading about Standardization: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "    + Standardization vs normalization: https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf\n",
    "    + Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "    + https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "    + https://machinelearningmastery.com/evaluate-skill-deep-learning-models/\n",
    "    + https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/\n",
    "    + https://towardsdatascience.com/how-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55\n",
    "    + Email: How to run GPU job on HPC cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Fix random seed.\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234) # Scikit Learn does not have its own global random state but uses the numpy random state instead.\n",
    "\n",
    "placementFeatures = [\"NumCells\", \"NumCellPins\", \"NumMacros\", \"NumMacroPins\", \"NumPassingNets\",\n",
    "                     \"AreaTile\", \"AreaCells\", \"AreaMacros\", \"AreaMacroPins\",\n",
    "                     \"AreaL1Blkg\", \"AreaL2Blkg\", \"AreaL1Pin\", \"AreaL2Pin\"]\n",
    "GRFeatures = [\"NumVerticalOverflow\", \"NumVerticalRemain\", \"NumVerticalTracks\",\n",
    "              \"NumHorizontalOverflow\", \"NumHorizontalRemain\", \"NumHorizontalTracks\"]\n",
    "features = placementFeatures\n",
    "features.extend(GRFeatures)\n",
    "\n",
    "label_name = \"HasDetailedRoutingViolation\"\n",
    "\n",
    "# Benchmarks\n",
    "# ispd18 = [\"ispd18_test\"+str(x) for x in range(1, 11)]\n",
    "# ispd18.extend([\"ispd18_test5_metal5\", \"ispd18_test8_metal5\"])\n",
    "\n",
    "ispd19 = [\"ispd19_test\"+str(x) for x in range(1, 11)]\n",
    "ispd19.extend([\"ispd19_test7_metal5\", \"ispd19_test8_metal5\", \"ispd19_test9_metal5\"])\n",
    "\n",
    "circuits = ispd19\n",
    "test_circuit = \"ispd19_test10\"\n",
    "if test_circuit in circuits:\n",
    "    circuits.remove(test_circuit)\n",
    "\n",
    "# Paths\n",
    "graph_path = \"/home/sheiny/workspace/results/gcells/graphs/\"\n",
    "csv_path = \"/home/sheiny/workspace/results/gcells/csvs/\"\n",
    "\n",
    "#Window Width and Height will be equal to 2 * window_radius + 1\n",
    "#WARNING: The window_radius must be greater than zero\n",
    "#For instance if window_radius = 7 the window size will be 15x15\n",
    "window_radius = 7\n",
    "window_size = 2*window_radius+1\n",
    "# batch_size = 2048\n",
    "# epochs = 300\n",
    "# learning_rate = 0.0045 # decayed every two epochs using an exponential rate of 0.94;\n",
    "\n",
    "batch_size = 1024 # is important to ensure that each batch has a decent chance of containing a few positive samples\n",
    "epochs = 10\n",
    "learning_rate = 0.005\n",
    "beta = 0.001 #regularization\n",
    "drop_out = 0.05\n",
    "\n",
    "METRICS = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "           tf.keras.metrics.FalsePositives(name='fp'),\n",
    "           tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "           tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "           tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "           tf.keras.metrics.Precision(name='precision'),\n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = matplotlib.pyplot.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and graph functions using NetworkX library\n",
    "\n",
    "ATTENTION: If you want to deploy a model, it's critical that you preserve the preprocessing calculations.\n",
    "The easiest way to implement them as layers, and attach them to your model before export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# The features will be rescaled so that they’ll have the properties of a standard normal distribution.\n",
    "# mean (μ) = 0\n",
    "# standard deviation (σ) = 1\n",
    "def standardize(train_array, val_array, test_array=None):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    train_array = scaler.fit_transform(train_array)\n",
    "    val_array = scaler.transform(val_array)\n",
    "    if test_array is not None:\n",
    "        test_array = scaler.transform(test_array)\n",
    "        return train_array, val_array, test_array\n",
    "    return train_array, val_array\n",
    "\n",
    "def standardize_graph(graph):\n",
    "    all_features = {feature:[] for feature in features}\n",
    "    for node_id in graph.nodes:\n",
    "        node = graph.nodes[node_id]\n",
    "        for feature in features:\n",
    "            all_features[feature].append(node[feature])\n",
    "    all_features_np = {feature:np.array(all_features[feature]) for feature in features}\n",
    "    means = {feature:all_features_np[feature].mean() for feature in features}\n",
    "    stddevs = {feature:all_features_np[feature].std() for feature in features}\n",
    "    del all_features_np\n",
    "    for node_id in graph.nodes:\n",
    "        node = graph.nodes[node_id]\n",
    "        for feature in features:\n",
    "            node[feature] = (node[feature]-means[feature])/stddevs[feature] if stddevs[feature] != 0 else 0\n",
    "\n",
    "# Claculate weight for classes\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "def calculate_class_weights(df, label_name):\n",
    "    neg, pos = np.bincount(df[label_name])\n",
    "    total = neg + pos\n",
    "    print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    return class_weight, neg, pos\n",
    "\n",
    "def calculate_class_weights_from_graphs(graphs):\n",
    "    neg = 0\n",
    "    pos = 0\n",
    "    for graph in graphs:\n",
    "        for node_id in graph.nodes:\n",
    "            node = graph.nodes[node_id]\n",
    "            if node[label_name]:\n",
    "                pos += 1\n",
    "            else:\n",
    "                neg += 1\n",
    "    total = neg + pos\n",
    "    print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    return class_weight, neg, pos\n",
    "\n",
    "def grid_graph_build(json_file_path):\n",
    "    graph = nx.Graph()\n",
    "    nodes_jason = open(json_file_path)\n",
    "    data = json.load(nodes_jason)\n",
    "    nodes_jason.close()\n",
    "    for j_node in data:\n",
    "        node_id = j_node[\"id\"]\n",
    "        graph.add_node(node_id)\n",
    "        node = graph.nodes[node_id]\n",
    "        for feature in features:\n",
    "            node[feature] = j_node[feature]\n",
    "            node[label_name] = j_node[label_name]\n",
    "    for j_node in data:\n",
    "        node_id = j_node[\"id\"]\n",
    "        node = graph.nodes[node_id]\n",
    "        for neighbor in [\"UpNode\", \"DownNode\", \"LeftNode\", \"RightNode\"]:\n",
    "            if j_node[neighbor] != -1:\n",
    "                graph.add_edge(node_id, j_node[neighbor])\n",
    "            node[neighbor] = j_node[neighbor] if j_node[neighbor] != -1 else -1\n",
    "    return graph\n",
    "\n",
    "# simple sanity check function for a grid graph structure\n",
    "def check_json_grid_graph(json_file_path):\n",
    "    graph = grid_graph_build(json_file_path)\n",
    "    nodes_jason = open(json_file_path)\n",
    "    data = json.load(nodes_jason)\n",
    "    nodes_jason.close()\n",
    "    corners = 0\n",
    "    for j_node in data:\n",
    "        node_id = j_node[\"id\"]\n",
    "        node = graph.nodes[node_id]\n",
    "        if graph.degree[node_id] == 0:\n",
    "            print(\"WARNING! disconnected node found!\")\n",
    "        elif graph.degree[node_id] == 2:\n",
    "            corners += 1\n",
    "        elif graph.degree[node_id] != 3 and graph.degree[node_id] != 4:\n",
    "            print(\"WARNING! strange node degree found: \", graph.degree[node_id])\n",
    "        for feature in features:\n",
    "            if node[feature] < 0 and feature not in [\"NumVerticalOverflow\", \"NumVerticalRemain\", \"NumVerticalTracks\",\n",
    "                                                     \"NumHorizontalOverflow\", \"NumHorizontalRemain\", \"NumHorizontalTracks\"]:\n",
    "                print('WARNING! negative feature detected!',feature)\n",
    "    if corners != 4:\n",
    "        print(\"WARNING! corners found: \", corners, \" it should be 4!\")\n",
    "\n",
    "def get_valid_id_nodes(graph):\n",
    "    valid_node_ids = []\n",
    "    for node_id in graph.nodes:\n",
    "        node = graph.nodes[node_id]\n",
    "        if check_node_neighbors(graph, node, window_radius):\n",
    "            valid_node_ids.append(node_id)\n",
    "    return valid_node_ids\n",
    "\n",
    "def graph_to_df(graph):\n",
    "    return pd.DataFrame([graph.nodes[x] for x in graph.nodes])\n",
    "\n",
    "def check_node_neighbors(graph, node, window_radius):\n",
    "    for direction in [\"UpNode\", \"DownNode\", \"LeftNode\", \"RightNode\"]:\n",
    "        current_node = node\n",
    "        for x in range(window_radius):\n",
    "            if current_node[direction] == -1:\n",
    "                return False\n",
    "            current_node = graph.nodes[current_node[direction]]\n",
    "    return True\n",
    "\n",
    "def get_window_elements(graph, node, window_radius):\n",
    "    max_neighbor_hops = window_size-1\n",
    "    window_elements = []\n",
    "    current_node = node\n",
    "    for y in range(window_radius):\n",
    "        current_node = graph.nodes[current_node[\"UpNode\"]]\n",
    "    for x in range(window_radius):\n",
    "        current_node = graph.nodes[current_node[\"LeftNode\"]]\n",
    "    leftmost_node = current_node\n",
    "    for y in range(window_size):\n",
    "        row = []\n",
    "        current_node = leftmost_node\n",
    "        for x in range(window_size):\n",
    "            row.append([current_node[feature] for feature in features])\n",
    "            if x != max_neighbor_hops:\n",
    "                current_node = graph.nodes[current_node[\"RightNode\"]]\n",
    "        window_elements.append(row)\n",
    "        if y != max_neighbor_hops:\n",
    "                leftmost_node = graph.nodes[leftmost_node[\"DownNode\"]]\n",
    "    return window_elements\n",
    "\n",
    "def graph_to_np_array(graph, window_radius):\n",
    "    train_samples = []\n",
    "    train_labels = []\n",
    "    if window_radius < 0:\n",
    "        print('ERROR: the window_radius must be greater than zero.')\n",
    "    valid_node_ids = get_valid_id_nodes(graph)\n",
    "    for node_id in valid_node_ids:\n",
    "        node = graph.nodes[node_id]\n",
    "        train_samples.append(get_window_elements(graph, node, window_radius))\n",
    "        train_labels.append(node[label_name])\n",
    "    return np.array(train_samples), np.array(train_labels)\n",
    "\n",
    "def write_csv_from_graph(graph, output_path):\n",
    "    with open(output_path, 'w') as file:\n",
    "        header = ','.join([feature for feature in features])\n",
    "        file.write(header+\",\"+label_name+\"\\n\")\n",
    "        valid_node_ids = get_valid_id_nodes(graph)\n",
    "        for node_id in valid_node_ids:\n",
    "            node = graph.nodes[node_id]\n",
    "            np_image = np.array(get_window_elements(graph, node, window_radius))\n",
    "            np_image = np_image.reshape(-1)\n",
    "            str_image = ','.join([str(n) for n in np_image])\n",
    "            label = node[label_name]\n",
    "            line = str_image+\",\"+str(int(label))+\"\\n\"\n",
    "            file.write(line)\n",
    "        file.close()\n",
    "\n",
    "def image_generator_from_graph(mapping, graphs, batch_size, mode=\"train\"):\n",
    "    while True:\n",
    "        images = []\n",
    "        labels = []\n",
    "        random_sequence = random.sample(range(len(mapping)), len(mapping))\n",
    "        last_id = random_sequence[len(random_sequence)-1]\n",
    "        for random_id in random_sequence:\n",
    "            node_id = mapping[random_id][0]\n",
    "            graph_id = mapping[random_id][1]\n",
    "            graph = graphs[graph_id]\n",
    "            node = graph.nodes[node_id]\n",
    "            images.append(get_window_elements(graph, node, window_radius))\n",
    "            labels.append(int(node[label_name]))\n",
    "            if len(images) == batch_size:\n",
    "                yield(np.array(images), np.array(labels))\n",
    "                images = []\n",
    "                labels = []\n",
    "        if mode == \"test\":\n",
    "            break\n",
    "\n",
    "def image_generator_from_csv(inputPath, mode=\"train\", aug=None):\n",
    "    # open the CSV file for reading\n",
    "    f = open(inputPath, \"r\")\n",
    "    line = f.readline() #Skip Header\n",
    "    # loop indefinitely\n",
    "    while True:\n",
    "        # initialize our batches of images and labels\n",
    "        images = []\n",
    "        labels = []\n",
    "        # keep looping until we reach our batch size\n",
    "        while len(images) < batch_size:\n",
    "            # attempt to read the next line of the CSV file\n",
    "            line = f.readline()\n",
    "            # check to see if the line is empty, indicating we have\n",
    "            # reached the end of the file\n",
    "            if line == \"\":\n",
    "                # reset the file pointer to the beginning of the file\n",
    "                # and re-read the line\n",
    "                f.seek(0)\n",
    "                line = f.readline() #Skip Header\n",
    "                line = f.readline()\n",
    "                # if we are evaluating we should now break from our\n",
    "                # loop to ensure we don't continue to fill up the\n",
    "                # batch from samples at the beginning of the file\n",
    "                if mode == \"test\":\n",
    "                    break\n",
    "            # extract the label and construct the image\n",
    "            np_image = np.fromstring(line, sep=',')\n",
    "            label = int(np_image[len(np_image)-1])\n",
    "            np_image = np.arange(len(np_image)-1)\n",
    "            np_image = np_image.reshape(window_size, window_size, len(features))\n",
    "            # update our corresponding batches lists\n",
    "            images.append(np_image)\n",
    "            labels.append(label)\n",
    "        # yield the batch to the calling function\n",
    "        yield (np.array(images), np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data from JSON Grid Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "for circuit in circuits:\n",
    "    graph = grid_graph_build(graph_path+circuit+\"_0.JSON\")\n",
    "    standardize_graph(graph)\n",
    "    graphs.append(graph)\n",
    "\n",
    "node_mapping = {}\n",
    "idx = 0\n",
    "for graph_index, graph in enumerate(graphs):\n",
    "    valid_nodes = get_valid_id_nodes(graph)\n",
    "    for node_id in valid_nodes:\n",
    "        node_mapping[idx] = (node_id, graph_index)\n",
    "        idx += 1\n",
    "\n",
    "# Claculate weight for classes\n",
    "class_weight, neg, pos = calculate_class_weights_from_graphs(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data From ICCAD19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type_JSON = False\n",
    "\n",
    "test_df = pd.DataFrame() \n",
    "dataframes = []\n",
    "if file_type_JSON:\n",
    "    dataframes = [graph_to_df(grid_graph_build(graph_path+circuit+'.JSON')) for circuit in circuits]\n",
    "else:\n",
    "    dataframes = [pd.read_csv(csv_path+circuit+'_0.csv') for circuit in circuits]\n",
    "#     dataframes = [pd.read_csv(csv_path+circuit+'_1.csv') for circuit in circuits]\n",
    "#     dataframes = [pd.read_csv(csv_path+circuit+'_2.csv') for circuit in circuits]\n",
    "#     dataframes = [pd.read_csv(csv_path+circuit+'_3.csv') for circuit in circuits]\n",
    "#     dataframes = [pd.read_csv(csv_path+circuit+'_4.csv') for circuit in circuits]\n",
    "    test_df = pd.read_csv(csv_path+test_circuit+'_0.csv')\n",
    "\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Remove graph attributes\n",
    "if file_type_JSON:\n",
    "    df = df.drop(columns=[\"UpNode\", \"DownNode\", \"LeftNode\", \"RightNode\"])\n",
    "    test_df = test_df.drop(columns=[\"UpNode\", \"DownNode\", \"LeftNode\", \"RightNode\"])\n",
    "\n",
    "\n",
    "# df = df.drop(columns=[\"#VerticalOverflow\", \"#VerticalRemain\", \"#VerticalTracks\", \"#HorizontalOverflow\", \"#HorizontalRemain\", \"#HorizontalTracks\"])\n",
    "# test_df = test_df.drop(columns=[\"#VerticalOverflow\", \"#VerticalRemain\", \"#VerticalTracks\", \"#HorizontalOverflow\", \"#HorizontalRemain\", \"#HorizontalTracks\"])\n",
    "\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop(label_name))\n",
    "val_labels = np.array(val_df.pop(label_name))\n",
    "test_labels = np.array(test_df.pop(label_name))\n",
    "\n",
    "train_array = np.array(train_df)\n",
    "val_array = np.array(val_df)\n",
    "test_array = np.array(test_df)\n",
    "\n",
    "# Scale\n",
    "train_array, val_array, test_array = standardize(train_array, val_array, test_array)\n",
    "\n",
    "# Claculate weight for classes\n",
    "class_weight, neg, pos = calculate_class_weights(df, label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load EhPredictor's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ISDP14/EhPredictorISPD14.csv\")\n",
    "\n",
    "# drop l53 because is always zero\n",
    "df.pop('l53')\n",
    "df.pop('normal')\n",
    "\n",
    "# Instead of having the number of shorts, use them as a boolean\n",
    "df.loc[df['short'] > 0, 'short'] = 1\n",
    "\n",
    "# Convert to log-space. l9 l43 l45 l52 l51\n",
    "log_cols = ['l9', 'l43', 'l45', 'l52', 'l51']\n",
    "eps=0.001 # 0 => 0.1¢\n",
    "for col in log_cols:\n",
    "    df[col] = np.log(df[col] + eps)\n",
    "\n",
    "# CSV organization:\n",
    "# des_perf_1_dataset=all_dataset[0:5476,:]\n",
    "# des_perf_a_dataset=all_dataset[5476:16928,:]\n",
    "# des_perf_b_dataset=all_dataset[16928:26928,:]\n",
    "# fft_1_dataset=all_dataset[26928:28864,:]\n",
    "# fft_2_dataset=all_dataset[28864:32113,:]\n",
    "# fft_a_dataset=all_dataset[32113:38604,:]\n",
    "# fft_b_dataset=all_dataset[38604:44375,:]\n",
    "# matrix_mult_1_dataset=all_dataset[44375:52656,:]\n",
    "# matrix_mult_a_dataset=all_dataset[52656:69168,:]\n",
    "# matrix_mult_b_dataset=all_dataset[69168:90601,:]\n",
    "# pci_bridge32_a_dataset=all_dataset[90601:94170,:]\n",
    "# pci_bridge32_b_dataset=all_dataset[94170:103961,:]\n",
    "# superblue11_a_dataset=all_dataset[103961:175113,:]\n",
    "# superblue12_dataset=all_dataset[175113:241123,:]\n",
    "\n",
    "# Test circuits: mgc fft_2\n",
    "test_df = df.iloc[28864:32113]\n",
    "df2 = df[0:28864]\n",
    "df3 = df[32113:]\n",
    "df = pd.concat([df2, df3])\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('short'))\n",
    "val_labels = np.array(val_df.pop('short'))\n",
    "test_labels = np.array(test_df.pop('short'))\n",
    "\n",
    "train_array = np.array(train_df)\n",
    "val_array = np.array(val_df)\n",
    "test_array = np.array(test_df)\n",
    "\n",
    "# Scaling\n",
    "train_array, val_array, test_array = standardize(train_array, val_array, test_array)\n",
    "\n",
    "# Claculate weight for classes\n",
    "class_weight, neg, pos = calculate_class_weights(df, 'short')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(metrics = METRICS, output_bias=None, lr=learning_rate):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.Sequential([\n",
    "                                 tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(window_size, window_size, len(features)), kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "                                 tf.keras.layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "                                 tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "                                 tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "                                 tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "                                 tf.keras.layers.AveragePooling2D(3, 3),\n",
    "                                 tf.keras.layers.Flatten(),\n",
    "                                 tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.004)),\n",
    "                                 tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.004)),\n",
    "                                 tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(metrics = METRICS, output_bias=None, lr=learning_rate):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(20,\n",
    "                                                       activation='relu', # Relu throw away negative values\n",
    "                                                       kernel_regularizer=tf.keras.regularizers.l2(beta)),\n",
    "                                 tf.keras.layers.Dropout(drop_out),\n",
    "                                 tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_bias = np.log([pos/neg])\n",
    "# model = make_model(output_bias = initial_bias)\n",
    "# model.load_weights('./CNN_weights')\n",
    "\n",
    "# model.save_weights('./CNN_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with Image Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = image_generator_from_graph(node_mapping, graphs, batch_size, mode=\"train\")\n",
    "num_train_images = len(node_mapping)\n",
    "\n",
    "initial_bias = np.log([pos/neg])\n",
    "\n",
    "model = make_model(output_bias = initial_bias)\n",
    "H = model.fit(x=trainGen,\n",
    "              steps_per_epoch=num_train_images // batch_size,\n",
    "              class_weight=class_weight,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Image Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_circuit = \"ispd19_test10\"\n",
    "test_graph = grid_graph_build(graph_path+test_circuit+\"_0.JSON\")\n",
    "standardize_graph(test_graph)\n",
    "graphs.append(test_graph)\n",
    "test_graph_idx = len(graphs)-1\n",
    "\n",
    "test_node_mapping = {}\n",
    "valid_nodes = get_valid_id_nodes(test_graph)\n",
    "idx = 0\n",
    "for node_id in valid_nodes:\n",
    "    test_node_mapping[idx] = (node_id, test_graph_idx)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testGen = image_generator_from_graph(test_node_mapping, graphs, batch_size, mode=\"test\")\n",
    "baseline_results = model.evaluate(testGen, batch_size=batch_size)\n",
    "metrics = calculate_metrics(model, baseline_results)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train without image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_array, train_labels))\n",
    "train_dataset = dataset.shuffle(len(train_array)).batch(batch_size)\n",
    "\n",
    "initial_bias = np.log([pos/neg])\n",
    "\n",
    "model = make_model(output_bias = initial_bias)\n",
    "train_history = model.fit(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(val_array, val_labels),\n",
    "                          class_weight=class_weight,\n",
    "                          epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./CNN_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = sklearn.metrics.confusion_matrix(labels, predictions > p)\n",
    "    matplotlib.pyplot.figure(figsize=(5,5))\n",
    "    seaborn.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    matplotlib.pyplot.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    matplotlib.pyplot.ylabel('Actual label')\n",
    "    matplotlib.pyplot.xlabel('Predicted label')\n",
    "    matplotlib.pyplot.show()\n",
    "    \n",
    "def calculate_metrics(model, results):\n",
    "    m = {}\n",
    "    for name, value in zip(model.metrics_names, results):\n",
    "        m[name] = value\n",
    "    if m['precision'] + m['recall'] != 0:\n",
    "        f_score = (2 * m['precision'] * m['recall'])/(m['precision'] + m['recall'])\n",
    "        m['F-score'] = f_score\n",
    "    sqrt = math.sqrt((m['tp']+m['fp'])*(m['tp']+m['fn'])*(m['tn']+m['fp'])*(m['tn']+m['fn']))\n",
    "    if sqrt != 0:\n",
    "        mcc = (m['tp'] * m['tn'] - m['fp'] * m['fn'])/sqrt\n",
    "        m['MCC'] = mcc\n",
    "    return m\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    for x, y in metrics.items():\n",
    "        print(x,':', round(y, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "N = epochs\n",
    "matplotlib.pyplot.style.use(\"ggplot\")\n",
    "matplotlib.pyplot.figure()\n",
    "matplotlib.pyplot.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "# matplotlib.pyplot.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "matplotlib.pyplot.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "# matplotlib.pyplot.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "matplotlib.pyplot.title(\"Training Loss and Accuracy on Dataset\")\n",
    "matplotlib.pyplot.xlabel(\"Epoch #\")\n",
    "matplotlib.pyplot.ylabel(\"Loss/Accuracy\")\n",
    "matplotlib.pyplot.legend(loc=\"lower left\")\n",
    "matplotlib.pyplot.show()\n",
    "# plt.savefig(\"plot.png\")\n",
    "\n",
    "print(\"loss:\",H.history[\"loss\"])\n",
    "print(\"accuracy:\",H.history[\"accuracy\"])\n",
    "print(\"tp:\",H.history[\"tp\"])\n",
    "print(\"fp:\",H.history[\"fp\"])\n",
    "print(\"tn:\",H.history[\"tn\"])\n",
    "print(\"fn:\",H.history[\"fn\"])\n",
    "print(\"precision:\",H.history[\"precision\"])\n",
    "print(\"recall\",H.history[\"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(train_array, train_labels, batch_size=batch_size, verbose=0)\n",
    "metrics = calculate_metrics(model, baseline_results)\n",
    "print_metrics(metrics)\n",
    "train_predictions_baseline = model.predict(train_array, batch_size=batch_size)\n",
    "plot_cm(train_labels, train_predictions_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performace Test10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(test_array, test_labels, batch_size=batch_size, verbose=0)\n",
    "metrics = calculate_metrics(model, baseline_results)\n",
    "print_metrics(metrics)\n",
    "test_predictions_baseline = model.predict(test_array, batch_size=batch_size)\n",
    "plot_cm(test_labels, test_predictions_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performace on ISPD14 - mgc_fft_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(test_array, test_labels, batch_size=batch_size, verbose=0)\n",
    "metrics = calculate_metrics(model, baseline_results)\n",
    "print_metrics(metrics)\n",
    "test_predictions_baseline = model.predict(test_array, batch_size=batch_size)\n",
    "plot_cm(test_labels, test_predictions_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "path = \"/home/sheiny/workspace/results/routed_circuits\"\n",
    "\n",
    "circuit_violations = {}\n",
    "for circuit in circuits:\n",
    "    circuit_violations[circuit] = []\n",
    "    for itr in range(5):\n",
    "        \n",
    "        file = path+circuit+\"/drc_rpt/\"+circuit+\"_\"+str(itr)+\".rpt\"\n",
    "        with open(file, \"r\") as file:\n",
    "            first_line = file.readline()\n",
    "            for last_line in file:\n",
    "                pass\n",
    "        drv_number = last_line.split(' ')[5]\n",
    "        file.close()\n",
    "        \n",
    "        file = path+circuit+\"/routeReport/\"+circuit+\"_\"+str(itr)+\".report\"\n",
    "        with open(file, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                if re.search('Total net length = *', line):\n",
    "                    wirelength = line.split(' ')[4]\n",
    "                    circuit_violations[circuit].append((drv_number, wirelength))\n",
    "                    break\n",
    "        file.close()\n",
    "\n",
    "dataframe = pd.DataFrame(circuit_violations)\n",
    "new_collumns = [\"test\"+str(x) for x in range(1, 11)]\n",
    "new_collumns.extend([\"test5_metal5\", \"test8_metal5\"])\n",
    "dataframe.columns = new_collumns\n",
    "dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
